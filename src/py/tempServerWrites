Skip to content
Enterprise
Search or jump toâ¦

Pull requests
Issues
Explore
 
@dnagre 
Learn Git and GitHub without any code!
Using the Hello World guide, youâll start a branch, write comments, and open a pull request.


SICE-Networks
/
Net-Fall20
6
0
10
 Code
 Issues 0
 Pull requests 0
 Projects 0
 Wiki
 Insights
Net-Fall20/04_rudp/README.org
@jemusser jemusser Update README.org
33a13f4 9 hours ago
132 lines (99 sloc)  8.5 KB
  
Reliable UDP (Part 1)
This is part 1 of 2. Part 2 is here.

Instructions
The purpose of this assignment is to extend your netster code so that it supports reliable network communication over UDP - Reliable UDP (RUDP)! You already implemented an application protocol over TCP and UDP and you will now have an opportunity to test your existing code over an unreliable network. Once the limitations of UDP over an unreliable channel become clear you should feel motivated to solve the issue at hand with your own RUDP implementation.

This assignment requires both careful planning and likely several iterations of your design before it meets the specifications put forth in the tasks below. You will have more time to complete this assignment, but be sure to start early and begin experimenting with a lossy network to understand how things work. Good luck!

Emulating your local coffee shopâs free public Wi-Fi
On a Linux machine where you have sudo privileges, you can emulate a lossy network using tc (traffic control) and netem (network emulator).

tc allows a user to simulate network properties on linux machines; properties such as latency and packet loss rates. These properties can be restricted to specific TCP/UDP ports. This is the approach you will use in this assignment to test your rudp implementation.

# emulate latency and loss on certain port ranges
declare -x LAT="25ms"
declare -x iface="lo"
declare -x DMASK=0xff00

declare -x ID=2
declare -x LOSS=0
declare -x DPORT=2048
sudo tc qdisc add dev $iface root handle 1: htb
sudo tc class add dev $iface parent 1: classid 1:1 htb rate 10000Mbps
sudo tc class add dev $iface parent 1:1 classid 1:${ID} htb rate 10000Mbps
sudo tc qdisc add dev $iface parent 1:${ID} handle ${ID}0: netem delay $LAT loss $LOSS limit 10000
sudo tc filter add dev $iface parent 1:0 protocol ip u32 match ip dport $DPORT $DMASK flowid 1:${ID}

declare -x ID=3
declare -x LOSS=1
declare -x DPORT=4096
sudo tc class add dev $iface parent 1:1 classid 1:${ID} htb rate 10000Mbps
sudo tc qdisc add dev $iface parent 1:${ID} handle ${ID}0: netem delay $LAT loss $LOSS limit 10000
sudo tc filter add dev $iface parent 1:0 protocol ip u32 match ip dport $DPORT $DMASK flowid 1:${ID}

declare -x ID=4
declare -x LOSS=30
declare -x DPORT=8192
sudo tc class add dev $iface parent 1:1 classid 1:${ID} htb rate 10000Mbps
sudo tc qdisc add dev $iface parent 1:${ID} handle ${ID}0: netem delay $LAT loss $LOSS limit 10000
sudo tc filter add dev $iface parent 1:0 protocol ip u32 match ip dport $DPORT $DMASK flowid 1:${ID}
Using these recipes, your development environment will now have special port ranges with bad performance:

dst port range	latency	loss
2048-2304	25ms	0%
4096-4352	25ms	1%
8192-8448	25ms	30%
To remove these settings, use the following:

declare -x iface="lo"
sudo tc qdisc del dev $iface root
How to develop and test with packet loss and delay
Other ports (not within the ranges described in the table above) are normal, with no added latency or loss behavior.
The test ports add 25ms latency in both directions, or 50ms round-trip-time (RTT).
You can observe the impact of 30% loss across the network with two terminals.

Server terminal:

server$ ./netster -u -p 8199
<...missing lots of input from the client...>
Client terminal:

client$ ./netster -u -p 8199 localhost
Hello, I am a client
send a message: <...type lots of input...>
<...missing lots of responses from the server...>
Task 1 - Extend netster to transfer file data
You should extend netster so that you can transfer file data from client to server. In addition to infrequent request/response messages from the previous assignment, you will now need to send lots of data at once and stress the underlying network. Reading from a file and transfering its contents over the network meets that criteria. Having the netster server save the file also lets you check if your RUDP design is working correctly. Obviously, the received and saved file should be a bit-wise duplicate of the source file sent by the netster client when transfered with a working RUDP design.

The netster skeleton code has a command line argument (-f) to specify a filename, and should read the file if started as a client, or write the file if started as a server.

With an open file handle available when the -f option if given, you will use the file I/O methods in your given programming language to read and write file data. For example, fread() and fwrite() in C, or f.read() and f.write() in Python3. It is your job to move file data between the file handle and the network socket using an intermediate buffer. Performance is not a primary concern here, so copying in and out of a user space buffer is perfectly fine. When segmenting the file into UDP frames, restrict each packet to less than a single MTU (1500 bytes for the purposes of this assignment). In practical terms, this means limiting the send size to approx. 1024 bytes.

Task 2 - Implement alternating bit, stop-and-wait protocol
You will implement a stop-and-wait protocol (i.e. alternating bit protocol), called rdt3.0 in the book and slides.

Since you implemented the UDP client/server in the previous assignment, you already have experience with a similar unreliable channel interface (i.e. udt_send() and udt_recv() from rdt3.0). You must now implement the rdt_send() and rdt_rcv() interface that your client and server will use when running in RUDP mode. In other words, your rdt_ methods should provide the reliable service for the calling application and use the underlying UDP sendto() and recvfrom() methods to access the unreliable channel.

There are a number of ways to implement this task correctly; however, there are a few key features and assumptions you can count on:

You must introduce a new RUDP header that encapsulates the application data sent in via rdt_send(). This should include fields to support sequence numbers, message type (ACK, NACK, etc.), and potentially other fields like length.
You will need a countdown timer.
You may assume unidirectional data transfer (e.g. client -> server). You can make changes in your netster application code to ensure unidirectional flow (e.g. file transfer) and also to pace how often data is sent into your RUDP interface for debugging purposes.
You may use both ACK and NACK control messages instead of duplicate ACKs.
You may assume only a single client RUDP session at a time.
You may assume that UDP handles the checksum error detection for you. Your implementation should treat any potentially corrupt packets the same as if they were simply lost.
Your strategy for state management and handling control messages is up to you. The rdt3.0 state machines from the book and slides may be guides but the expectation is that many different implementations should arise from this assignment.

TO SUBMIT
Write well-documented and well-formatted code in your a3 and netster files for Tasks 1 and 2.
Commit and push your changes to your Net-Fall20 github.iu.edu repository.
Submit to Canvas a URL for your repository containing a commit hash. To find this url, go to your Net-Fall20 repositoryâs commits endpoint, and choose one specific commit for your teachers to grade.
./images/out.gif

Your URL should look similar to this:

https://github.iu.edu/username/Net-Fall20/commit/00a3e8fc1e0f2bad388a9f138be1e909eef25eb4
Replace username with your username, and replace 00a3e8fc1e0f2bad388a9f138be1e909eef25eb4 with the commit hash of your chosen commit.

You can find this same commit hash by running the command git log from your terminal, after changing directory to your repository folder.

Â© 2020 GitHub, Inc.
Help
Support
API
Training
Blog
About
GitHub Enterprise Server 2.21.6



Origins
The earliest suggestion of unusual disappearances in the Bermuda area appeared in a September 17, 1950, article published in The Miami Herald (Associated Press)[4] by Edward Van Winkle Jones.[5] Two years later, Fate magazine published "Sea Mystery at Our Back Door",[6][7] a short article by George Sand covering the loss of several planes and ships, including the loss of Flight 19, a group of five US Navy Grumman TBM Avenger torpedo bombers on a training mission. Sand's article was the first to lay out the now-familiar triangular area where the losses took place, as well as the first to suggest a supernatural element to the Flight 19 incident. Flight 19 alone would be covered again in the April 1962 issue of American Legion magazine.[8] In it, author Allan W. Eckert wrote that the flight leader had been heard saying, "We are entering white water, nothing seems right. We don't know where we are, the water is green, no white." He also wrote that officials at the Navy board of inquiry stated that the planes "flew off to Mars."[9]

In February 1964, Vincent Gaddis wrote an article called "The Deadly Bermuda Triangle" in the pulp magazine Argosy saying Flight 19 and other disappearances were part of a pattern of strange events in the region.[10] The next year, Gaddis expanded this article into a book, Invisible Horizons.[11]

Other writers elaborated on Gaddis' ideas: John Wallace Spencer (Limbo of the Lost, 1969, repr. 1973);[12] Charles Berlitz (The Bermuda Triangle, 1974);[13] Richard Winer (The Devil's Triangle, 1974),[14] and many others, all keeping to some of the same supernatural elements outlined by Eckert.[15]

Triangle area
The Gaddis Argosy article delineated the boundaries of the triangle,[10] giving its vertices as Miami; San Juan, Puerto Rico; and Bermuda. Subsequent writers did not necessarily follow this definition.[16] Some writers gave different boundaries and vertices to the triangle, with the total area varying from 1,300,000 to 3,900,000 km2 (500,000 to 1,510,000 sq mi).[16] "Indeed, some writers even stretch it as far as the Irish coast."[2] Consequently, the determination of which accidents occurred inside the triangle depends on which writer reported them.[16]

Criticism of the concept
Larry Kusche
Larry Kusche, author of The Bermuda Triangle Mystery: Solved (1975)[1] argued that many claims of Gaddis and subsequent writers were exaggerated, dubious or unverifiable. Kusche's research revealed a number of inaccuracies and inconsistencies between Berlitz's accounts and statements from eyewitnesses, participants, and others involved in the initial incidents. Kusche noted cases where pertinent information went unreported, such as the disappearance of round-the-world yachtsman Donald Crowhurst, which Berlitz had presented as a mystery, despite clear evidence to the contrary. Another example was the ore-carrier recounted by Berlitz as lost without trace three days out of an Atlantic port when it had been lost three days out of a port with the same name in the Pacific Ocean. Kusche also argued that a large percentage of the incidents that sparked allegations of the Triangle's mysterious influence actually occurred well outside it. Often his research was simple: he would review period newspapers of the dates of reported incidents and find reports on possibly relevant events like unusual weather, that were never mentioned in the disappearance stories.

Kusche concluded that:

The number of ships and aircraft reported missing in the area was not significantly greater, proportionally speaking, than in any other part of the ocean.
In an area frequented by tropical cyclones, the number of disappearances that did occur were, for the most part, neither disproportionate, unlikely, nor mysterious.
Furthermore, Berlitz and other writers would often fail to mention such storms or even represent the disappearance as having happened in calm conditions when meteorological records clearly contradict this.
The numbers themselves had been exaggerated by sloppy research. A boat's disappearance, for example, would be reported, but its eventual (if belated) return to port may not have been.
Some disappearances had, in fact, never happened. One plane crash was said to have taken place in 1937, off Daytona Beach, Florida, in front of hundreds of witnesses; a check of the local papers revealed nothing.[citation needed]
The legend of the Bermuda Triangle is a manufactured mystery, perpetuated by writers who either purposely or unknowingly made use of misconceptions, faulty reasoning, and sensationalism.[1]
In a 2013 study, the World Wide Fund for Nature identified the world's 10 most dangerous waters for shipping, but the Bermuda Triangle was not among them.[17][18]

Further responses
When the UK Channel 4 television program The Bermuda Triangle (1992)[19] was being produced by John Simmons of Geofilms for the Equinox series, the marine insurance market Lloyd's of London was asked if an unusually large number of ships had sunk in the Bermuda Triangle area. Lloyd's determined that large numbers of ships had not sunk there.[3] Lloyd's does not charge higher rates for passing through this area. United States Coast Guard records confirm their conclusion. In fact, the number of supposed disappearances is relatively insignificant considering the number of ships and aircraft that pass through on a regular basis.[1]

The Coast Guard is also officially skeptical of the Triangle, noting that they collect and publish, through their inquiries, much documentation contradicting many of the incidents written about by the Triangle authors. In one such incident involving the 1972 explosion and sinking of the tanker V. A. Fogg, the Coast Guard photographed the wreck and recovered several bodies,[20] in contrast with one Triangle author's claim that all the bodies had vanished, with the exception of the captain, who was found sitting in his cabin at his desk, clutching a coffee cup.[12] In addition, V. A. Fogg sank off the coast of Texas, nowhere near the commonly accepted boundaries of the Triangle.

The Nova/Horizon episode The Case of the Bermuda Triangle, aired on June 27, 1976, was highly critical, stating that "When we've gone back to the original sources or the people involved, the mystery evaporates. Science does not have to answer questions about the Triangle because those questions are not valid in the first place ... Ships and planes behave in the Triangle the same way they behave everywhere else in the world."[2]

Skeptical researchers, such as Ernest Taves[21] and Barry Singer,[22] have noted how mysteries and the paranormal are very popular and profitable. This has led to the production of vast amounts of material on topics such as the Bermuda Triangle. They were able to show that some of the pro-paranormal material is often misleading or inaccurate, but its producers continue to market it. Accordingly, they have claimed that the market is biased in favor of books, TV specials, and other media that support the Triangle mystery, and against well-researched material if it espouses a skeptical viewpoint.

Benjamin Radford, an author and scientific paranormal investigator, noted in an interview on the Bermuda Triangle that it could be very difficult locating an aircraft lost at sea due to the vast search area, and although the disappearance might be mysterious, that did not make it paranormal or unexplainable. Radford further noted the importance of double-checking information as the mystery surrounding the Bermuda Triangle had been created by people who had neglected to do so.[23]

Hypothetical explanation attempts
Persons accepting the Bermuda Triangle as a real phenomenon have offered a number of explanatory approaches.

Paranormal explanations
Triangle writers have used a number of supernatural concepts to explain the events. One explanation pins the blame on leftover technology from the mythical lost continent of Atlantis. Sometimes connected to the Atlantis story is the submerged rock formation known as the Bimini Road off the island of Bimini in the Bahamas, which is in the Triangle by some definitions. Followers of the purported psychic Edgar Cayce take his prediction that evidence of Atlantis would be found in 1968, as referring to the discovery of the Bimini Road. Believers describe the formation as a road, wall, or other structure, but the Bimini Road is of natural origin.[24]

Other writers attribute the events to UFOs.[25][26] Charles Berlitz, author of various books on anomalous phenomena, lists several theories attributing the losses in the Triangle to anomalous or unexplained forces.[13]

Natural explanations
Compass variations
Compass problems are one of the cited phrases in many Triangle incidents. While some have theorized that unusual local magnetic anomalies may exist in the area,[27] such anomalies have not been found. Compasses have natural magnetic variations in relation to the magnetic poles, a fact which navigators have known for centuries. Magnetic (compass) north and geographic (true) north are exactly the same only for a small number of places â for example, as of 2000, in the United States, only those places on a line running from Wisconsin to the Gulf of Mexico.[28] But the public may not be as informed, and think there is something mysterious about a compass "changing" across an area as large as the Triangle, which it naturally will.[1]


False-color image of the Gulf Stream flowing north through the western Atlantic Ocean. (NASA)
Gulf Stream
The Gulf Stream is a major surface current, primarily driven by thermohaline circulation that originates in the Gulf of Mexico and then flows through the Straits of Florida into the North Atlantic. In essence, it is a river within an ocean, and, like a river, it can and does carry floating objects. It has a maximum surface velocity of about 2 m/s (6.6 ft/s).[29] A small plane making a water landing or a boat having engine trouble can be carried away from its reported position by the current.

Human error
One of the most cited explanations in official inquiries as to the loss of any aircraft or vessel is human error.[30] Human stubbornness may have caused businessman Harvey Conover to lose his sailing yacht, Revonoc, as he sailed into the teeth of a storm south of Florida on January 1, 1958.[31]

Violent weather
Hurricanes are powerful storms that form in tropical waters and have historically cost thousands of lives and caused billions of dollars in damage. The sinking of Francisco de Bobadilla's Spanish fleet in 1502 was the first recorded instance of a destructive hurricane. These storms have in the past caused a number of incidents related to the Triangle.

A powerful downdraft of cold air was suspected to be a cause in the sinking of Pride of Baltimore on May 14, 1986. The crew of the sunken vessel noted the wind suddenly shifted and increased velocity from 32 km/h (20 mph) to 97â145 km/h (60â90 mph). A National Hurricane Center satellite specialist, James Lushine, stated "during very unstable weather conditions the downburst of cold air from aloft can hit the surface like a bomb, exploding outward like a giant squall line of wind and water."[32] A similar event occurred to Concordia in 2010, off the coast of Brazil. Scientists are currently investigating whether "hexagonal" clouds may be the source of these up-to-170 mph (270 km/h) "air bombs".[33]

Methane hydrates
Main article: Methane clathrate

Worldwide distribution of confirmed or inferred offshore gas hydrate-bearing sediments, 1996.
Source: United States Geological Survey
An explanation for some of the disappearances has focused on the presence of large fields of methane hydrates (a form of natural gas) on the continental shelves.[34] Laboratory experiments carried out in Australia have proven that bubbles can, indeed, sink a scale model ship by decreasing the density of the water;[35][36][37] any wreckage consequently rising to the surface would be rapidly dispersed by the Gulf Stream. It has been hypothesized that periodic methane eruptions (sometimes called "mud volcanoes") may produce regions of frothy water that are no longer capable of providing adequate buoyancy for ships. If this were the case, such an area forming around a ship could cause it to sink very rapidly and without warning.

Publications by the USGS describe large stores of undersea hydrates worldwide, including the Blake Ridge area, off the coast of the southeastern United States.[38] However, according to the USGS, no large releases of gas hydrates are believed to have occurred in the Bermuda Triangle for the past 15,000 years.[3]

Notable incidents
Main article: List of Bermuda Triangle incidents
USS Cyclops
Main article: USS Cyclops (AC-4)
The incident resulting in the single largest loss of life in the history of the US Navy not related to combat occurred when the collier Cyclops, carrying a full load of manganese ore and with one engine out of action, went missing without a trace with a crew of 309 sometime after March 4, 1918, after departing the island of Barbados. Although there is no strong evidence for any single theory, many independent theories exist, some blaming storms, some capsizing, and some suggesting that wartime enemy activity was to blame for the loss.[39][40] In addition, two of Cyclops's sister ships, Proteus and Nereus were subsequently lost in the North Atlantic during World War II. Both ships were transporting heavy loads of metallic ore similar to that which was loaded on Cyclops during her fatal voyage. In all three cases structural failure due to overloading with a much denser cargo than designed is considered the most likely cause of sinking.

Carroll A. Deering
Main article: Carroll A. Deering

Schooner Carroll A. Deering, as seen from the Cape Lookout lightvessel on January 29, 1921, two days before she was found deserted in North Carolina. (US Coast Guard)
A five-masted schooner built in 1919, Carroll A. Deering was found hard aground and abandoned at Diamond Shoals, near Cape Hatteras, North Carolina, on January 31, 1921. Rumors and more at the time indicated Deering was a victim of piracy, possibly connected with the illegal rum-running trade during Prohibition, and possibly involving another ship, Hewitt, which disappeared at roughly the same time. Just hours later, an unknown steamer sailed near the lightship along the track of Deering, and ignored all signals from the lightship. It is speculated that Hewitt may have been this mystery ship, and possibly involved in Deering's crew disappearance.[41]

Flight 19
Main article: Flight 19

US Navy Avengers, similar to those of Flight 19
Flight 19 was a training flight of five TBM Avenger torpedo bombers that disappeared on December 5, 1945, while over the Atlantic. The squadron's flight plan was scheduled to take them due east from Fort Lauderdale for 141 mi (227 km), north for 73 mi (117 km), and then back over a final 140-mile (230-kilometre) leg to complete the exercise. The flight never returned to base. The disappearance was attributed by Navy investigators to navigational error leading to the aircraft running out of fuel.

One of the search and rescue aircraft deployed to look for them, a PBM Mariner with a 13-man crew, also disappeared. A tanker off the coast of Florida reported seeing an explosion[42] and observing a widespread oil slick when fruitlessly searching for survivors. The weather was becoming stormy by the end of the incident.[43] According to contemporaneous sources the Mariner had a history of explosions due to vapour leaks when heavily loaded with fuel, as it might have been for a potentially long search-and-rescue operation.

Star Tiger and Star Ariel
Main articles: BSAA Star Tiger disappearance and BSAA Star Ariel disappearance
G-AHNP Star Tiger disappeared on January 30, 1948, on a flight from the Azores to Bermuda; G-AGRE Star Ariel disappeared on January 17, 1949, on a flight from Bermuda to Kingston, Jamaica. Both were Avro Tudor IV passenger aircraft operated by British South American Airways.[44] Both planes were operating at the very limits of their range and the slightest error or fault in the equipment could keep them from reaching the small island.[1]

Douglas DC-3
Main article: 1948 Airborne Transport DC-3 (DST) disappearance
On December 28, 1948, a Douglas DC-3 aircraft, number NC16002, disappeared while on a flight from San Juan, Puerto Rico, to Miami. No trace of the aircraft, or the 32 people on board, was ever found. A Civil Aeronautics Board investigation found there was insufficient information available on which to determine probable cause of the disappearance.[45]

Connemara IV
A pleasure yacht was found adrift in the Atlantic south of Bermuda on September 26, 1955; it is usually stated in the stories (Berlitz, Winer)[13][14] that the crew vanished while the yacht survived being at sea during three hurricanes. The 1955 Atlantic hurricane season shows Hurricane Ione passing nearby between 14 and 18 September, with Bermuda being affected by winds of almost gale force.[1] In his second book on the Bermuda Triangle, Winer quoted from a letter he had received from Mr J.E. Challenor of Barbados:[46]

On the morning of September 22, Connemara IV was lying to a heavy mooring in the open roadstead of Carlisle Bay. Because of the approaching hurricane, the owner strengthened the mooring ropes and put out two additional anchors. There was little else he could do, as the exposed mooring was the only available anchorage. ... In Carlisle Bay, the sea in the wake of Hurricane Janet was awe-inspiring and dangerous. The owner of Connemara IV observed that she had disappeared. An investigation revealed that she had dragged her moorings and gone to sea.

KC-135 Stratotankers
On August 28, 1963, a pair of US Air Force KC-135 Stratotanker aircraft collided and crashed into the Atlantic 300 miles west of Bermuda.[47][48] Some writers [10][13][14] say that while the two aircraft did collide there were two distinct crash sites, separated by over 160 miles (260 km) of water. However, Kusche's research showed that the unclassified version of the Air Force investigation report revealed that the debris field defining the second "crash site" was examined by a search and rescue ship, and found to be a mass of seaweed and driftwood tangled in an old buoy.[1]

See also
icon	Oceans portal
icon	Weather portal
List of Bermuda Triangle incidents
List of topics characterized as pseudoscience
Nevada Triangle
Devil's Sea (or Dragon's Triangle)
Sargasso Sea
SS Cotopaxi
Vile vortex
Hurricane Alley
References
Notes

 Kusche, 1975.
 "The Case of the Bermuda Triangle". NOVA / Horizon. 1976-06-27. PBS.
 "Bermuda Triangle". Gas Hydrates at the USGS. Woods Hole. Archived from the original on 23 October 2012.
 "E. V. W. Jones AP article". Retrieved 1 October 2014.
 E.V.W. Jones (September 16, 1950). "Same Big World, Sea's Puzzles Still Baffle Men In Pushbutton Age". Associated Press.
 "Has the 'Mystery' of the Bermuda Triangle Finally Been Solved?". 2016-10-24. Retrieved 24 October 2016.
 George X. San (October 1952). "Sea Mystery at Our Back Door". Fate.
 Allen W. Eckert (April 1962). "The Mystery of The Lost Patrol". American Legion Magazine. Cited in James R. Lewis (editor), Satanism Today: An Encyclopedia of Religion, Folklore, and Popular Culture, page 72, segment by Jerome Clark (ABC-CLIO, Inc., 2001). ISBN 1-57607-292-4
 Diana Formisano Willett, Paranormal Fright, p. 9 (AuthorHouse, 2013), ISBN 978-1-4817-3268-0
 Gaddis, Vincent (1964), "The Deadly Bermuda Triangle", Argosy
 Vincent Gaddis (1965). Invisible Horizons.
 Spencer, 1969.
 Berlitz, 1974.
 Winer 1974
 "Strange fish: the scientifiction of Charles F. Berlitz, 1913â2003". Skeptic. Altadena, CA. March 2004.
 "Frequently Asked Questions: Bermuda Triangle Fact Sheet" (PDF). US Department of Defense. 1998. Archived from the original (PDF) on 2016-11-21.
 "Study finds shipwrecks threaten precious seas". BBC News/science. 7 June 2013. Retrieved 7 June 2013.
 "Bermuda Triangle doesn't make the cut on list of world's most dangerous oceans". The Christian Science Monitor. 2013-06-10. Retrieved 22 March 2016.
 "Equinox: The Bermuda Triangle". Archived from the original on 2009-05-27. Retrieved 2012-12-06.
 "V A Fogg" (PDF). USCG. Retrieved 2012-12-06.
 Taves, Ernest H. (1978). The Skeptical Inquirer. 111 (1): 75â76. Missing or empty |title= (help)
 Singer, Barry (1979). The Humanist. XXXIX (3): 44â45. Missing or empty |title= (help)
 Radford, Benjamin. "Lessons From A Middle School Bermuda Triangle Q&A". Center for Inquiry. Archived from the original on 21 November 2019. Retrieved 21 November 2019.
 Shinn, Eugene A. (January 2004). "A Geologist's Adventures with Bimini Beachrock and Atlantis True Believers". Skeptical Inquirer. Amherst, New York: Committee for Skeptical Inquiry. Archived from the original on April 6, 2007.
 "UFO over Bermuda Triangle". Ufos.about.com. June 29, 2008. Retrieved June 1, 2009.
 Cochran-Smith, Marilyn (2003). "Bermuda Triangle: dichotomy, mythology, and amnesia". Journal of Teacher Education. Thousand Oaks, California: SAGE Publications. 54 (4): 275. doi:10.1177/0022487103256793.
 "Bermuda Triangle". US Navy. Retrieved 2009-05-26.
 "National Geomagnetism Program | Charts | North America | Declination" (PDF). United States Geological Survey. Archived from the original (PDF) on 2010-05-27. Retrieved 2010-02-28.
 Phillips, Pamela. "The Gulf Stream". USNA/Johns Hopkins. Retrieved August 2, 2007.
 Mayell, Hillary (15 December 2003). "Bermuda Triangle: Behind the Intrigue". National Geographic News. National Geographic Society. National Geographic Partners, LLC. Retrieved 26 May 2009.
 Scott, Captain Thomas A. (1994). Histories & Mysteries: The Shipwrecks of Key Largo (1st ed.). Best Publishing Company. p. 124. ISBN 0941332330.
 "Downdraft likely sank clipper, The Miami News, May 23, 1986, p. 6A". Retrieved 1 October 2014.
 Kenny Walter (24 October 2016). "Bermuda Triangle Mystery Explained". RandD Magazine. Retrieved 2016-10-24.
 Gruy, H. J. (March 1998). "Office of Scientific & Technical Information, OSTI, U.S. Department of Energy, DOE". Petroleum Engineer International. OTSI. 71 (3). OSTI 616279.
 "Could methane bubbles sink ships?". Monash Univ.
 Jason Dowling (2003-10-23). "Bermuda Triangle mystery solved? It's a load of gas". The Age.
 Terrence Aym (2010-08-06). "How Brilliant Computer Scientists Solved the Bermuda Triangle Mystery". Salem-News.com.
 Paull, C.K.; W.P., D. (1981). "Appearance and distribution of the gas hydrate reflection in the Blake Ridge region, offshore southeastern United States". Gas Hydrates at the USGS. Woods Hole. MF-1252. Archived from the original on 2012-02-18.
 "Bermuda Triangle". D Merrill. Archived from the original on 2002-11-24.
 "Myths and Folklore of Bermuda". Bermuda Cruises. Archived from the original on 2009-06-10.
 "Carroll A Deering". Graveyard of the Atlantic. Archived from the original on 2005-08-28.
 "The Loss of Flight 19". history.navy.mil.
 "The Disappearance of Flight 19". Bermuda-Triangle.Org. Archived from the original on 22 July 2012. Retrieved 26 June 2018.
 "The Tudors". Bermuda-Triangle.Org. Archived from the original on 23 July 2012. Retrieved 26 June 2018.
 "Airborne Transport, Miami, December 1948" (PDF). Aviation Safety. Archived from the original (PDF) on 2007-01-03. Retrieved 2015-10-05.
 Winer 1975, pp. 95â96
 Accident description for 61-0322 at the Aviation Safety Network. Retrieved on 2 February 2013.
 Accident description for 61-0319 at the Aviation Safety Network. Retrieved on 2 February 2013.
Bibliography
The incidents cited above, apart from the official documentation, come from the following works. Some incidents mentioned as having taken place within the Triangle are found only in these sources:

Berg, Daniel (2000). Bermuda Shipwrecks. East Rockaway, NY: Aqua Explorers. ISBN 0-9616167-4-1.
Berlitz, Charles (1974). The Bermuda Triangle (1st ed.). Doubleday. ISBN 0-385-04114-4.
Group, David (1984). The Evidence for the Bermuda Triangle. Wellingborough, Northamptonshire: Aquarian Press. ISBN 0-85030-413-X.
Jeffrey, Adi-Kent Thomas (1975). The Bermuda Triangle. ISBN 0-446-59961-1.
Kusche, Lawrence David (1975). The Bermuda Triangle Mystery Solved. Buffalo, NY: Prometheus Books. ISBN 0-87975-971-2.
Quasar, Gian J. (2003). Into the Bermuda Triangle: Pursuing the Truth Behind the World's Greatest Mystery. International Marine / Ragged Mountain Press. ISBN 0-07-142640-X. Reprinted in paperback in 2005; ISBN 0-07-145217-6.
Spencer, John Wallace (1969). Limbo Of The Lost. ISBN 0-686-10658-X.
Winer, Richard (1974). The Devil's Triangle. ISBN 0-553-10688-0.
Winer, Richard (1975). The Devil's Triangle 2. ISBN 0-553-02464-7.
Further reading

Newspaper articles
ProQuest has newspaper source material for many incidents, archived in Portable Document Format (PDF). The newspapers include The New York Times, The Washington Post, and The Atlanta Constitution. To access this website, registration is required, usually through a library connected to a college or university.

Flight 19

"Great Hunt On For 27 Navy Fliers Missing In Five Planes Off Florida", The New York Times, December 7, 1945.
"Wide Hunt For 27 Men In Six Navy Planes", The Washington Post, December 7, 1945.
"Fire Signals Seen In Area Of Lost Men", The Washington Post, December 9, 1945.
SS Cotopaxi

"Lloyd's posts Cotopaxi As 'Missing'", The New York Times, January 7, 1926.
"Efforts To Locate Missing Ship Fail", The Washington Post, December 6, 1925.
"Lighthouse Keepers Seek Missing Ship", The Washington Post, December 7, 1925.
"53 On Missing Craft Are Reported Saved", The Washington Post, December 13, 1925.
USS Cyclops (AC-4)

"Cold High Winds Do $25,000 Damage", The Washington Post, March 11, 1918.
"Collier Overdue A Month", The New York Times, April 15, 1918.
"More Ships Hunt For Missing Cyclops", The New York Times, April 16, 1918.
"Haven't Given Up Hope For Cyclops", The New York Times, April 17, 1918.
"Collier Cyclops Is Lost; 293 Persons On Board; Enemy Blow Suspected", The Washington Post, April 15, 1918.
"U.S. Consul Gottschalk Coming To Enter The War", The Washington Post, April 15, 1918.
"Cyclops Skipper Teuton, 'Tis Said", The Washington Post, April 16, 1918.
"Fate Of Ship Baffles", The Washington Post, April 16, 1918.
"Steamer Met Gale On Cyclops' Course", The Washington Post, April 19, 1918.
Carroll A. Deering

"Piracy Suspected In Disappearance Of 3 American Ships", The New York Times, June 21, 1921.
"Bath Owners Skeptical", The New York Times, June 22, 1921. piera antonella
"Deering Skipper's Wife Caused Investigation", The New York Times, June 22, 1921.
"More Ships Added To Mystery List", The New York Times, June 22, 1921.
"Hunt On For Pirates", The Washington Post, June 21, 1921
"Comb Seas For Ships", The Washington Post, June 22, 1921.
"Port Of Missing Ships Claims 3000 Yearly", The Washington Post, July 10, 1921.
Wreckers

"'Wreckreation' Was The Name Of The Game That Flourished 100 Years Ago", The New York Times, March 30, 1969.
S.S. Suduffco

"To Search For Missing Freighter", The New York Times, April 11, 1926.
"Abandon Hope For Ship", The New York Times, April 28, 1926.
Star Tiger and Star Ariel

"Hope Wanes in Sea Search For 28 Aboard Lost Airliner", The New York Times, January 31, 1948.
"72 Planes Search Sea For Airliner", The New York Times, January 19, 1949.
DC-3 Airliner NC16002 disappearance

"30-Passenger Airliner Disappears In Flight From San Juan To Miami", The New York Times, December 29, 1948.
"Check Cuba Report Of Missing Airliner", The New York Times, December 30, 1948.
"Airliner Hunt Extended", The New York Times, December 31, 1948.
Harvey Conover and Revonoc

"Search Continuing For Conover Yawl", The New York Times, January 8, 1958.
"Yacht Search Goes On", The New York Times, January 9, 1958.
"Yacht Search Pressed", The New York Times, January 10, 1958.
"Conover Search Called Off", The New York Times, January 15, 1958.
KC-135 Stratotankers

"Second Area Of Debris Found In Hunt For Jets", The New York Times, August 31, 1963.
"Hunt For Tanker Jets Halted", The New York Times, September 3, 1963.
"Planes Debris Found In Jet Tanker Hunt", The Washington Post, August 30, 1963.
B-52 Bomber (Pogo 22)

"U.S.-Canada Test Of Air Defence A Success", The New York Times, October 16, 1961.
"Hunt For Lost B-52 Bomber Pushed In New Area", The New York Times, October 17, 1961.
"Bomber Hunt Pressed", The New York Times, October 18, 1961.
"Bomber Search Continuing", The New York Times, October 19, 1961.
"Hunt For Bomber Ends", The New York Times, October 20, 1961.
Charter vessel Sno'Boy

"Plane Hunting Boat Sights Body In Sea", The New York Times, July 7, 1963.
"Search Abandoned For 40 On Vessel Lost In Caribbean", The New York Times, July 11, 1963.
"Search Continues For Vessel With 55 Aboard In Caribbean", The Washington Post, July 6, 1963.
"Body Found In Search For Fishing Boat", The Washington Post, July 7, 1963.
SS Marine Sulphur Queen

"Tanker Lost In Atlantic; 39 Aboard", The Washington Post, February 9, 1963.
"Debris Sighted In Plane Search For Tanker Missing Off Florida", The New York Times, February 11, 1963.
"2.5 Million Is Asked In Sea Disaster", The Washington Post, February 19, 1963.
"Vanishing Of Ship Ruled A Mystery", The New York Times, April 14, 1964.
"Families Of 39 Lost At Sea Begin $20-Million Suit Here", The New York Times, June 4, 1969.
"10-Year Rift Over Lost Ship Near End", The New York Times, February 4, 1973.
SS Sylvia L. Ossa

"Ship And 37 Vanish In Bermuda Triangle On Voyage To U.S.", The New York Times, October 18, 1976.
"Ship Missing In Bermuda Triangle Now Presumed To Be Lost At Sea", The New York Times, October 19, 1976.
"Distress Signal Heard From American Sailor Missing For 17 Days", The New York Times, October 31, 1976.
Website links
The following websites have either online material that supports the popular version of the Bermuda Triangle, or documents published from official sources as part of hearings or inquiries, such as those conducted by the United States Navy or United States Coast Guard. Copies of some inquiries are not online and may have to be ordered; for example, the losses of Flight 19 or USS Cyclops can be ordered direct from the United States Naval Historical Center.

Text of Feb, 1964 Argosy Magazine article by Vincent Gaddis
United States Coast Guard database of selected reports and inquiries
U.S. Navy Historical Center Bermuda Triangle FAQ
U.S. Navy Historical C/ The Bermuda Triangle: Startling New Secrets, Sci Fi Channel documentary (November 2005)
Navy Historical Center: The Loss Of Flight 19
on losses of heavy ships at sea
Bermuda Shipwrecks
Association of Underwater Explorers shipwreck listings page
Dictionary of American Naval Fighting Ships
"Summary of Missing Planes". Bermuda-Triangle.Org. Archived from the original on 3 June 2004. Retrieved 30 December 2007.
Books
Most of the works listed here are largely out of print. Copies may be obtained at your local library, or purchased used at bookstores, or through eBay or Amazon.com. These books are often the only source material for some of the incidents that have taken place within the Triangle.

Into the Bermuda Triangle: Pursuing the Truth Behind the World's Greatest Mystery by Gian J. Quasar, International Marine/Ragged Mountain Press (2003) ISBN 0-07-142640-X; contains list of missing craft as researched in official records. (Reprinted in paperback (2005) ISBN 0-07-145217-6).
The Bermuda Triangle, Charles Berlitz (ISBN 0-385-04114-4): Out of print.
The Bermuda Triangle Mystery Solved (1975). Lawrence David Kusche (ISBN 0-87975-971-2)
Limbo Of The Lost, John Wallace Spencer (ISBN 0-686-10658-X)
The Evidence for the Bermuda Triangle (1984), David Group (ISBN 0-85030-413-X)
The Final Flight (2006), Tony Blackman (ISBN 0-9553856-0-1). This book is a work of fiction.
Bermuda Shipwrecks (2000), Daniel Berg(ISBN 0-9616167-4-1)
The Devil's Triangle (1974), Richard Winer (ISBN 0-553-10688-0); this book sold well over a million copies by the end of its first year; to date there have been at least 17 printings.
The Devil's Triangle 2 (1975), Richard Winer (ISBN 0-553-02464-7)
From the Devil's Triangle to the Devil's Jaw (1977), Richard Winer (ISBN 0-553-10860-3)
Ghost Ships: True Stories of Nautical Nightmares, Hauntings, and Disasters (2000), Richard Winer (ISBN 0-425-17548-0)
The Bermuda Triangle (1975) by Adi-Kent Thomas Jeffrey (ISBN 0-446-59961-1)
Bara, Mike (2019). The Triangle: The truth behind the world's most enduring mystery. Kempton, IL: Adventures Unlimited. p. 191. ASIN B07SVG79C5.
External links
	Wikimedia Commons has media related to Bermuda Triangle.
	Look up Bermuda Triangle in Wiktionary, the free dictionary.
"Database of selected reports and inquiries". United States Coast Guard.
Quasar, Gian. "Bermuda Triangle Mystery". Bermuda-Triangle.Org. Archived from the original on 20 July 2012.
Quasar, Gian. "Gian Quasar's Bermuda Triangle". â updated version of Quasar's Bermuda Triangle information.
"Bermuda Triangle FAQ". US Navy Historical Center.
"Selective Bibliography". US Navy Historical Center. Archived from the original on 2006-07-09.
"The Loss Of Flight 19". US Navy Historical Center.
"On losses of heavy ships at sea".
"Bermuda Shipwrecks".
Barnette, Michael C. "Shipwreck listings page". Association of Underwater Explorers.
SigmaDocumentaries. "The Mystery of the Bermuda Triangle". Sigma Documentaries.
Dunning, Brian (20 November 2012). "Skeptoid #337: The Bermuda Triangle and the Devil's Sea". Skeptoid. Retrieved 15 June 2017.
vte
List of Bermuda Triangle incidents



Etymology
The Mariana Trench is named after the nearby Mariana Islands, which are named Las Marianas in honour of Spanish Queen Mariana of Austria, widow of Philip IV of Spain. The islands are part of the island arc that is formed on an over-riding plate, called the Mariana Plate (also named for the islands), on the western side of the trench.

Geology

The Pacific plate is subducted beneath the Mariana Plate, creating the Mariana trench, and (further on) the arc of the Mariana Islands, as water trapped in the plate is released and explodes upward to form island volcanoes and earthquakes .
The Mariana Trench is part of the Izu-Bonin-Mariana subduction system that forms the boundary between two tectonic plates. In this system, the western edge of one plate, the Pacific Plate, is subducted (i.e., thrust) beneath the smaller Mariana Plate that lies to the west. Crustal material at the western edge of the Pacific Plate is some of the oldest oceanic crust on earth (up to 170 million years old), and is, therefore, cooler and denser; hence its great height difference relative to the higher-riding (and younger) Mariana Plate. The deepest area at the plate boundary is the Mariana Trench proper.

The movement of the Pacific and Mariana plates is also indirectly responsible for the formation of the Mariana Islands. These volcanic islands are caused by flux melting of the upper mantle due to the release of water that is trapped in minerals of the subducted portion of the Pacific Plate.

Research history

Ocean trenches in the western Pacific
See also: Challenger Deep
The trench was first sounded during the Challenger expedition in 1875, using a weighted rope, which recorded a depth of 4,475 fathoms (8,184 metres; 26,850 feet).[11] In 1877, a map was published called Tiefenkarte des Grossen Ozeans ("Depth map of the Great Ocean") by Petermann, which showed a Challenger Tief ("Challenger deep") at the location of that sounding. In 1899, USS Nero, a converted collier, recorded a depth of 5,269 fathoms (9,636 metres; 31,614 feet).[12]

In 1951, Challenger II surveyed the trench using echo sounding, a much more precise and vastly easier way to measure depth than the sounding equipment and drag lines used in the original expedition. During this survey, the deepest part of the trench was recorded when the Challenger II measured a depth of 5,960 fathoms (10,900 metres; 35,760 feet) at 11Â°19â²N 142Â°15â²E, known as the Challenger Deep.[13]

In 1957, the Soviet vessel Vityaz reported a depth of 11,034 metres (36,201 ft) at a location dubbed the Mariana Hollow.[3]

In 1962, the surface ship M.V. Spencer F. Baird recorded a maximum depth of 10,915 metres (35,810 ft) using precision depth gauges.

In 1984, the Japanese survey vessel TakuyÅ (ææ´) collected data from the Mariana Trench using a narrow, multi-beam echo sounder; it reported a maximum depth of 10,924 metres (35,840 ft), also reported as 10,920 metres (35,830 ft) Â±10 m (33 ft).[14] Remotely Operated Vehicle KAIKO reached the deepest area of the Mariana Trench and made the deepest diving record of 10,911 metres (35,797 ft) on 24 March 1995.[15]

During surveys carried out between 1997 and 2001, a spot was found along the Mariana Trench that had depth similar to that of the Challenger Deep, possibly even deeper. It was discovered while scientists from the Hawaii Institute of Geophysics and Planetology were completing a survey around Guam; they used a sonar mapping system towed behind the research ship to conduct the survey. This new spot was named the HMRG (Hawaii Mapping Research Group) Deep, after the group of scientists who discovered it.[16]

On 1 June 2009, mapping aboard the RV Kilo Moana (mothership of the Nereus vehicle), indicated a spot with a depth of 10,971 metres (35,994 ft). The sonar mapping of the Challenger Deep was possible by its Simrad EM120 sonar multibeam bathymetry system for deep water. The sonar system uses phase and amplitude bottom detection, with an accuracy of better than 0.2% of water depth across the entire swath (implying that the depth figure is accurate to Â± 22 metres (72 ft)).[17][18]

In 2011, it was announced at the American Geophysical Union Fall Meeting that a US Navy hydrographic ship equipped with a multibeam echosounder conducted a survey which mapped the entire trench to 100 metres (330 ft) resolution.[4] The mapping revealed the existence of four rocky outcrops thought to be former seamounts.[19]

The Mariana Trench is a site chosen by researchers at Washington University and the Woods Hole Oceanographic Institution in 2012 for a seismic survey to investigate the subsurface water cycle. Using both ocean-bottom seismometers and hydrophones the scientists are able to map structures as deep as 97 kilometres (60 mi) beneath the surface.[20]

Descents
Ambox current red.svg
This section needs to be updated. Please update this article to reflect recent events or newly available information. (June 2020)

The bathyscaphe Trieste (designed by Auguste Piccard), the first manned vehicle to reach the bottom of the Mariana Trench.[21]
Four manned descents and three unmanned descents have been achieved. The first was the manned descent by Swiss-designed, Italian-built, United States Navy-owned bathyscaphe Trieste which reached the bottom at 1:06 pm on 23 January 1960, with Don Walsh and Jacques Piccard on board.[13][22] Iron shot was used for ballast, with gasoline for buoyancy.[13] The onboard systems indicated a depth of 11,521 m (37,799 ft), but this was later revised to 10,916 m (35,814 ft).[23] The depth was estimated from a conversion of pressure measured and calculations based on the water density from sea surface to seabed.[22]

This was followed by the unmanned ROVs KaikÅ in 1996 and Nereus in 2009. The first three expeditions directly measured very similar depths of 10,902 to 10,916 m (35,768 to 35,814 ft). The fourth was made by Canadian film director James Cameron in 2012. On 26 March, he reached the bottom of the Mariana Trench in the submersible vessel Deepsea Challenger, diving to a depth of 10,908 m (35,787 ft).[24][25][26][27]

In July 2015, members of the National Oceanic and Atmospheric Administration, Oregon State University, and the Coast Guard submerged a hydrophone into the deepest part of the Mariana Trench, the Challenger Deep, never having deployed one past a mile. The titanium-shelled hydrophone was designed to withstand the immense pressure 7 miles under.[28] Although researchers were unable to retrieve the hydrophone until November, the data capacity was full within the first 23 days. After months of analyzing the sounds, the experts were surprised to pick up natural sounds like earthquakes, a typhoon and baleen whales along with man-made sounds such as boats.[29] Due to the mission's success, the researchers announced plans to deploy a second hydrophone in 2017 for an extended period of time.

Victor Vescovo achieved a new record descent to 10,927 metres (35,853 ft.), using the DSV Limiting Factor, a Triton 36000/2 model manufactured by Florida-based Triton Submarines. He dived again in May 2019 and became the first person to dive the Challenger Deep twice.[30][31]

In May 2020, a joint project between the Russian shipbuilders, scientific teams of the Russian Academy of Sciences with the support of Russian Foundation for Advanced Research Projects and the Pacific Fleet submerged an autonomous underwater vehicle "Vityaz" to the bottom of the Mariana Trench at a depth of 10,028 metres. Vityaz is the first underwater vehicle (AUV) to operate autonomously at the extreme depths of the Mariana Trench. The duration of the mission, excluding diving and surfacing, was more than 3 hours. [32]

Planned descents
Ambox current red.svg
This section needs to be updated. Please update this article to reflect recent events or newly available information. (December 2019)
In April 2011, Richard Branson unveiled a new single-person submarine to go to the bottom of the Mariana Trench in the next two years.[33]

As of February 2012, at least one other team was planning a piloted submarine to reach the bottom of the Mariana Trench.[34]

Life
The expedition conducted in 1960 claimed to have observed, with great surprise because of the high pressure, large creatures living at the bottom, such as a flatfish about 30 cm (12 in) long,[23] and shrimp.[35] According to Piccard, "The bottom appeared light and clear, a waste of firm diatomaceous ooze".[23] Many marine biologists are now skeptical of the supposed sighting of the flatfish, and it is suggested that the creature may instead have been a sea cucumber.[36][37] During the second expedition, the unmanned vehicle KaikÅ collected mud samples from the seabed.[38] Tiny organisms were found to be living in those samples.

In July 2011, a research expedition deployed untethered landers, called dropcams, equipped with digital video cameras and lights to explore this region of the deep sea.

Among many other living organisms, some gigantic single-celled amoebas with a size of more than 10 cm (4 in), belonging to the class of monothalamea were observed.[39] Monothalamea are noteworthy for their size, their extreme abundance on the seafloor and their role as hosts for a variety of organisms.

In December 2014, a new species of snailfish was discovered at a depth of 8,145 m (26,722 ft), breaking the previous record for the deepest living fish seen on video.[40]

During the 2014 expedition, several new species were filmed including huge amphipods known as supergiants. Deep-sea gigantism is the process where species grow larger than their shallow water relatives.[40]

In May 2017, an unidentified type of snailfish was filmed at a depth of 8,178 metres (26,800 ft).[41]

Pollution
In 2016, a research expedition looked at the chemical makeup of crustacean scavengers collected from the range of 7,841â10,250 metres within the trench. Within these organisms, the researchers found extremely elevated concentrations of PCBs, a chemical toxin banned for its environmental harm in the 1970s, concentrated at all depths within the sediment of the trench.[42] Further research has found that amphipods also ingest microplastics, with 100% of amphipods having at least one piece of synthetic material in their stomachs.[43][44]

In 2019, Victor Vescovo reported finding a plastic bag and candy wrappers at the bottom of the trench.[30] That year, Scientific American also reported that carbon-14 from nuclear bomb testing has been found in the bodies of aquatic animals found in the trench.[45]

Possible nuclear waste disposal site
Like other oceanic trenches, the Mariana Trench has been proposed as a site for nuclear waste disposal in 1972,[46][47] in the hope that tectonic plate subduction occurring at the site might eventually push the nuclear waste deep into the Earth's mantle, the second layer of the Earth. However, ocean dumping of nuclear waste is prohibited by international law.[46][47][48] Furthermore, plate subduction zones are associated with very large megathrust earthquakes, the effects of which are unpredictable for the safety of long-term disposal of nuclear wastes within the hadopelagic ecosystem.[47]

See also
icon	Oceania portal
Marianas Trench Marine National Monument, United States national monument at the trench. This National Monument protects 246,610 square kilometres (95,216 sq mi) of submerged lands and waters of the Mariana Archipelago. It includes some of the Mariana Trench, but not the deepest part, the Challenger Deep, which lies just outside the monument area.
Notes
 Mariana Trench is 10,994 m deep,[4] while Mount Everest is 8,848 m tall.[5] The difference is 2,146 m, or at least no less than 2,104 m, accounting for the combined 42 m uncertainty in the measurements.
References
 "NGA GeoNames Search". National Geospatial Agency. Retrieved 29 February 2016.
 "So, How Deep Is the Mariana Trench?" (PDF). Center for Coastal & Ocean Mapping-Joint Hydrographic Center (CCOM/JHC), Chase Ocean Engineering Laboratory of the University of New Hampshire. 5 March 2014. Retrieved 20 May 2014.
 "Mariana Trench". EncyclopÃ¦dia Britannica. EncyclopÃ¦dia Britannica.
 "Scientists map Mariana Trench, deepest known section of ocean in the world". The Telegraph. Telegraph Media Group. 7 December 2011. Retrieved 23 June 2018.
 "Official height for Everest set". BBC News. 8 April 2010. Retrieved 24 June 2018.
 infoplease.com â The Temperature in the Mariana Trench, read 13 May 2012
 "About the Monument â Mariana Trench". U.S. Fish and Wildlife Service.
 "Giant amoeba found in Mariana Trench â 6.6 miles beneath the sea". Los Angeles Times. 26 October 2011. Retrieved 23 March 2012.
 Choi, Charles Q. (17 March 2013). "Microbes Thrive in Deepest Spot on Earth". LiveScience. Retrieved 17 March 2013.
 Glud, Ronnie; WenzhÃ¶fer, Frank; Middleboe, Mathias; Oguri, Kazumasa; Turnewitsch, Robert; Canfield, Donald E.; Kitazato, Hiroshi (17 March 2013). "High rates of microbial carbon turnover in sediments in the deepest oceanic trench on Earth". Nature Geoscience. 6 (4): 284â288. Bibcode:2013NatGe...6..284G. doi:10.1038/ngeo1773.
 "About the Mariana Trench â DEEPSEA CHALLENGE Expedition". Deepseachallenge.com. 26 March 2012. Archived from the original on 28 June 2013. Retrieved 8 July 2013.
 Theberge, A. (24 March 2009). "Thirty Years of Discovering the Mariana Trench". Hydro International. Retrieved 31 July 2010.
 "The Mariana Trench â Exploration". marianatrench.com.
 Tani, S. "Continental shelf survey of Japan" (PDF). Archived from the original (PDF) on 9 March 2011. Retrieved 24 December 2010.
 Development and Construction of Launcher System of 10000mâClass Remotely Operated Vehicle KAIKO Mitsubishi Heavy Industry
 Whitehouse, David (16 July 2003). "Sea floor survey reveals deep hole". BBC News. Retrieved 17 December 2011.
 "Daily Reports for R/V KILO MOANA June and July 2009". University of Hawaii Marine Center. Archived from the original on 24 May 2012.
 "Inventory of Scientific Equipment aboard the R/V KILO MOANA". University of Hawaii Marine Center. Archived from the original on 13 June 2010.
 Duncan Geere (7 February 2012). "Four 'bridges' span the Mariana Trench". Wired. CondÃ© Nast Digital. Archived from the original on 11 March 2012. Retrieved 23 March 2012.
 "Seismic Survey at the Mariana Trench Will Follow Water Dragged Down into the Earth's Mantle". ScienceDaily. 22 March 2012. Retrieved 23 March 2012.
 Strickland, Eliza (29 February 2012). "Don Walsh Describes the Trip to the Bottom of the Mariana Trench â IEEE Spectrum". Spectrum.ieee.org. Retrieved 8 July 2013.
 "Mariana Trench". Earthquake Hazards Program. U.S. Geological Survey. 21 October 2009. Archived from the original on 18 March 2012. Retrieved 23 March 2012.
 "NOAA Ocean Explorer: History: Quotations: Soundings, Sea-Bottom, and Geophysics". oceanexplorer.noaa.gov.
 "A man took a submarine to the deepest place on Earth â and found trash".
 AP Staff (25 March 2012). "James Cameron has reached deepest spot on Earth". NBC News. Retrieved 25 March 2012.
 Broad, William J. (25 March 2012). "Filmmaker in Submarine Voyages to Bottom of Sea". New York Times. Retrieved 25 March 2012.
 Than, Ker (25 March 2012). "James Cameron Completes Record-Breaking Mariana Trench Dive". National Geographic Society. Retrieved 25 March 2012.
 Schneider, Kate (2016), "Eerie sounds from the bottom of the Earth", News.com.au.
 Chappell, Bill (4 March 2016). "Deep-Sea Audio Recordings Reveal A Noisy Mariana Trench, Surprising Scientists". The Two-Way: Breaking News from NPR. National Public Radio. Retrieved 1 May 2016.
 Street, Francesca (13 May 2019). "Deepest ocean dive recorded: How Victor Vescovo did it". CNN Travel. CNN. Retrieved 13 May 2019.
 "Deepest Submarine Dive in History, Five Deeps Expedition Conquers Challenger Deep" (PDF). Retrieved 2 December 2019.
 "Russian Submarine "Vityaz" Reached The Bottom Of The Mariana Trench". Russian Geographical Society. 13 May 2020. Retrieved 6 June 2020.
 "Richard Branson plans deep-ocean submarine dives". Santa Rosa Press Democrat. 5 April 2011. Retrieved 7 August 2020.
 "Deep Search". DOER Marine. 16 March 2011. Retrieved 2 December 2019.
 "Bathyscaphe Trieste | Mariana Trench | Challenger Deep". Geology.com. Retrieved 1 March 2012.
 "James Cameron dives deep for Avatar", Guardian, 18 January 2011
 "James Cameron heads into the abyss", Nature, 19 March 2012
 Woods, Michael; Mary B. Woods (2009). Seven Natural Wonders of the Arctic, Antarctica, and the Oceans. Twenty-First Century Books. p. 13. ISBN 978-0-8225-9075-0. Retrieved 23 March 2012.
 "Giant amoebas discovered in the deepest ocean trench". Retrieved 26 March 2012.
 Morelle, Rebecca (9 December 2014). "New record for deepest fish". BBC News. Retrieved 26 August 2017.
 "Ghostly fish in Mariana Trench in the Pacific is deepest ever recorded". CBC News. 25 August 2017. Retrieved 26 August 2017.
 Jamieson, Alan J.; Malkocs, Tamas; Piertney, Stuart B.; Fujii, Toyonobu; Zhang, Zulin (13 February 2017). "Bioaccumulation of persistent organic pollutants in the deepest ocean fauna". Nature Ecology & Evolution. 1 (3). doi:10.1038/s41559-016-0051. hdl:2164/9142. ISSN 2397-334X. PMID 28812719.
 Jamieson, A. J.; Brooks, L. S. R.; Reid, W. D. K.; Piertney, S. B.; Narayanaswamy, B. E.; Linley, T. D. (28 February 2019). "Microplastics and synthetic particles ingested by deep-sea amphipods in six of the deepest marine ecosystems on Earth". Royal Society Open Science. 6 (2): 180667. Bibcode:2019RSOS....680667J. doi:10.1098/rsos.180667. ISSN 2054-5703. PMC 6408374. PMID 30891254.
 Robbins, Gary (5 September 2019). "UCSD discovers surge in plastics pollution off Santa Barbara". Los Angeles Times. Retrieved 5 September 2019.
 Levy, Adam,âBomb Carbonâ Has Been Found in Deep-Ocean Creatures, Scientific American, 15 May 2019
 Hafemeister, David W. (2007). Physics of societal issues: calculations on national security, environment, and energy. Berlin: Springer. p. 187. ISBN 978-0-387-95560-5.
 Kingsley, Marvin G.; Rogers, Kenneth H. (2007). Calculated risks: highly radioactive waste and homeland security. Aldershot, Hants, England: Ashgate. pp. 75â76. ISBN 978-0-7546-7133-6.
 "Dumping and Loss overview". Oceans in the Nuclear Age. Archived from the original on 5 June 2011. Retrieved 18 September 2010.
External links


The Dead Sea (Hebrew: ×Ö¸× ×Ö·×Ö¶Ö¼×Ö·×â About this soundYam ha-Melah lit. Sea of Salt; Arabic: Ø§ÙØ¨Ø­Ø± Ø§ÙÙÙØªâ About this soundAl-Bahr al-Mayyit[5] or Buhayrat,[6][7] Bahret or Birket Lut,[6] lit. "Lake/Sea of Lot") is a salt lake bordered by Jordan to the east and Israel and the West Bank to the west. It lies in the Jordan Rift Valley, and its main tributary is the Jordan River.

Its surface and shores are 430.5 metres (1,412 ft) below sea level,[4][8] Earth's lowest elevation on land. It is 304 m (997 ft) deep, the deepest hypersaline lake in the world. With a salinity of 342 g/kg, or 34.2% (in 2011), it is one of the world's saltiest bodies of water[9] â 9.6 times as salty as the ocean â and has a density of 1.24 kg/litre, which makes swimming similar to floating.[10][11] This salinity makes for a harsh environment in which plants and animals cannot flourish, hence its name. The Dead Sea's main, northern basin is 50 kilometres (31 mi) long and 15 kilometres (9 mi) wide at its widest point.[1]

The Dead Sea has attracted visitors from around the Mediterranean Basin for thousands of years. It was one of the world's first health resorts (for Herod the Great), and it has been the supplier of a wide variety of products, from asphalt for Egyptian mummification to potash for fertilisers.

The Dead Sea is receding at a swift rate; its surface area today is 605 km2 (234 sq mi), having been 1,050 km2 (410 sq mi) in 1930. The recession of the Dead Sea has begun causing problems, and multiple canal and pipeline proposals have been made to reduce its recession. One of these proposals is the Red SeaâDead Sea Water Conveyance project, carried out by Jordan, which will provide water to neighbouring countries, while the brine will be carried to the Dead Sea to help stabilise its water level. The first phase of the project is scheduled to begin in 2018 and be completed in 2021.[12]


Contents
1	Etymology and toponymy
2	Geography
3	Geology
3.1	Formation theories
3.2	Sedom Lagoon
3.3	Salt deposits
3.4	Lake formation
3.5	Lake salinity
3.6	Salt mounts formation
4	Climate
5	Chemistry
6	Putative therapies
7	Fauna and flora
8	Human settlement
9	Human history
9.1	Biblical period
9.2	Greek and Roman period
9.3	Byzantine period
9.4	Modern times
10	Tourism and leisure
10.1	British Mandate period
10.2	Israel
10.3	Jordan
10.4	West Bank
11	Chemical industry
11.1	British Mandate period
11.2	Israel
11.3	Jordan
11.4	West Bank
11.5	Extraction
12	Recession and environmental concerns
13	See also
14	References
15	Further reading
16	External links
Etymology and toponymy
In Hebrew, the Dead Sea is About this soundYÄm ha-Melaá¸¥ (helpÂ·info) (×× ××××â), meaning "sea of salt" (Genesis 14:3). The Bible uses this term alongside two others: the Sea of the Arabah (YÄm ha-âÄrÄvÃ¢ ×× ××¢×¨××â), and the Eastern Sea (ha-YÄm ha-kadmoni ××× ××§×××× ×â). The designation "Dead Sea" never appears in the Bible. In prose sometimes the term YÄm ha-MÄvet (×× ×××××ªâ, "sea of death") is used, due to the scarcity of aquatic life there.[13]

In Arabic, the Dead Sea is called About this soundal-Bahr al-Mayyit (helpÂ·info)[5] ("the Dead Sea"), or less commonly baá¸¥ráµ lÅ«á¹­áµ (Ø¨Ø­Ø± ÙÙØ·, "the Sea of Lot"). Another historic name in Arabic was the "Sea of ZoÊ¼ar", after a nearby town in biblical times. The Greeks called it Lake Asphaltites (Attic Greek á¼¡ ÎÎ¬Î»Î±ÏÏÎ± á¼ÏÏÎ±Î»Ïá¿ÏÎ·Ï, hÄ ThÃ¡latta asphaltÄ©tÄs, "the Asphaltite[14] sea").

Geography

Satellite photograph showing the location of the Dead Sea east of the Mediterranean Sea
The Dead Sea is an endorheic lake located in the Jordan Rift Valley, a geographic feature formed by the Dead Sea Transform (DST). This left lateral-moving transform fault lies along the tectonic plate boundary between the African Plate and the Arabian Plate. It runs between the East Anatolian Fault zone in Turkey and the northern end of the Red Sea Rift offshore of the southern tip of Sinai. It is here that the Upper Jordan River/Sea of Galilee/Lower Jordan River water system comes to an end.

The Jordan River is the only major water source flowing into the Dead Sea, although there are small perennial springs under and around the Dead Sea, forming pools and quicksand pits along the edges.[15] There are no outlet streams.

The Mujib River, biblical Arnon, is one of the larger water sources of the Dead Sea other than the Jordan.[16] The Wadi Mujib valley, 420 m below the sea level in the southern part of the Jordan valley, is a biosphere reserve, with an area of 212 km2 (82 sq mi).[17] Other more substantial sources are Wadi Darajeh (Arabic)/Nahal Dragot (Hebrew), and Nahal Arugot that ends at Ein Gedi (German article at: de:Nachal Arugot).[16] Wadi Hasa (biblical Zered) is another wadi flowing into the Dead Sea.

Rainfall is scarcely 100 mm (4 in) per year in the northern part of the Dead Sea and barely 50 mm (2 in) in the southern part.[18] The Dead Sea zone's aridity is due to the rainshadow effect of the Judaean Mountains. The highlands east of the Dead Sea receive more rainfall than the Dead Sea itself.

To the west of the Dead Sea, the Judaean mountains rise less steeply and are much lower than the mountains to the east. Along the southwestern side of the lake is a 210 m (700 ft) tall halite mineral formation called Mount Sodom.

Geology

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Dead Sea geology" â news Â· newspapers Â· books Â· scholar Â· JSTOR (July 2020) (Learn how and when to remove this template message)

The Jordanian shore of the Dead Sea, showing salt deposits left behind by falling water levels.
Formation theories
There are two contending hypotheses about the origin of the low elevation of the Dead Sea. The older hypothesis is that the Dead Sea lies in a true rift zone, an extension of the Red Sea Rift, or even of the Great Rift Valley of eastern Africa. A more recent hypothesis is that the Dead Sea basin is a consequence of a "step-over" discontinuity along the Dead Sea Transform, creating an extension of the crust with consequent subsidence.

Sedom Lagoon
During the late Pliocene-early Pleistocene,[19] around 3.7 million years ago,[citation needed] what is now the valley of the Jordan River, Dead Sea, and the northern Wadi Arabah was repeatedly inundated by waters from the Mediterranean Sea.[19] The waters formed in a narrow, crooked bay that is called by geologists the Sedom Lagoon, which was connected to the sea through what is now the Jezreel Valley.[citation needed] The floods of the valley came and went depending on long-scale changes in the tectonic and climatic conditions.[19]

The Sedom Lagoon extended at its maximum from the Sea of Galilee in the north to somewhere around 50 km (30 mi) south of the current southern end of the Dead Sea, and the subsequent lakes never surpassed this expanse. The Hula Depression was never part of any of these water bodies due to its higher elevation and the high threshold of the Korazim block separating it from the Sea of Galilee basin.[20]

Salt deposits
The Sedom Lagoon deposited evaporites mainly consisting of rock salt, which eventually reached a thickness of 2.3 km (1.43 mi) on the old basin floor in the area of today's Mount Sedom.[21]

Lake formation
Approximately two million years ago,[citation needed] the land between the Rift Valley and the Mediterranean Sea rose to such an extent that the ocean could no longer flood the area. Thus, the long lagoon became a landlocked lake.[20]

The first prehistoric lake to follow the Sedom Lagoon is named Lake Amora (which possibly appeared in the early Pleistocene; its sediments developed into the Amora (Samra) Formation, dated to over 200-80 kyr BP), followed by Lake Lisan (c. 70-14 kyr) and finally by the Dead Sea.[19]

Lake salinity
The water levels and salinity of the successive lakes (Amora, Lisan, Dead Sea) have either risen or fallen as an effect of the tectonic dropping of the valley bottom, and due to climate variation. As the climate became more arid, Lake Lisan finally shrank and became saltier, leaving the Dead Sea as its last remainder.[19][20]

From 70,000 to 12,000 years ago, Lake Lisan's level was 100 m (330 ft) to 250 m (820 ft) higher than its current level. Its level fluctuated dramatically, rising to its highest level around 26,000 years ago, indicating a very wet climate in the Near East.[22] Around 10,000 years ago, the lake's level dropped dramatically, probably even lower than today. During the last several thousand years, the lake has fluctuated approximately 400 m (1,300 ft), with some significant drops and rises. Current theories as to the cause of this dramatic drop in levels rule out volcanic activity; therefore, it may have been a seismic event.

Salt mounts formation
In prehistoric times[dubious â discuss], great amounts of sediment collected on the floor of Lake Amora. The sediment was heavier than the salt deposits and squeezed the salt deposits upwards into what are now the Lisan Peninsula and Mount Sodom (on the southwest side of the lake). Geologists explain the effect in terms of a bucket of mud into which a large flat stone is placed, forcing the mud to creep up the sides of the bucket. When the floor of the Dead Sea dropped further due to tectonic forces, the salt mounts of Lisan and Mount Sodom stayed in place as high cliffs (see salt dome).

Climate
The Dead Sea has a hot desert climate (KÃ¶ppen climate classification BWh), with year-round sunny skies and dry air. It has less than 50 millimetres (2 in) mean annual rainfall and a summer average temperature between 32 and 39 Â°C (90 and 102 Â°F). Winter average temperatures range between 20 and 23 Â°C (68 and 73 Â°F). The region has weaker ultraviolet radiation, particularly the UVB (erythrogenic rays). Given the higher atmospheric pressure, the air has a slightly higher oxygen content (3.3% in summer to 4.8% in winter) as compared to oxygen concentration at sea level.[23][24] Barometric pressures at the Dead Sea were measured between 1061 and 1065 hPa and clinically compared with health effects at higher altitude.[25] (This barometric measure is about 5% higher than sea level standard atmospheric pressure of 1013.25 hPa, which is the global ocean mean or ATM.) The Dead Sea affects temperatures nearby because of the moderating effect a large body of water has on climate. During the winter, sea temperatures tend to be higher than land temperatures, and vice versa during the summer months. This is the result of the water's mass and specific heat capacity. On average, there are 192 days above 30 Â°C (86 Â°F) annually.[26]

Climate data for Dead Sea, Sedom (390 m below sea level)
Month	Jan	Feb	Mar	Apr	May	Jun	Jul	Aug	Sep	Oct	Nov	Dec	Year
Record high Â°C (Â°F)	26.4
(79.5)	30.4
(86.7)	33.8
(92.8)	42.5
(108.5)	45.0
(113.0)	46.4
(115.5)	47.0
(116.6)	44.5
(112.1)	43.6
(110.5)	40.0
(104.0)	35.0
(95.0)	28.5
(83.3)	47.0
(116.6)
Average high Â°C (Â°F)	20.5
(68.9)	21.7
(71.1)	24.8
(76.6)	29.9
(85.8)	34.1
(93.4)	37.6
(99.7)	39.7
(103.5)	39.0
(102.2)	36.5
(97.7)	32.4
(90.3)	26.9
(80.4)	21.7
(71.1)	30.4
(86.7)
Daily mean Â°C (Â°F)	16.6
(61.9)	17.7
(63.9)	20.8
(69.4)	25.4
(77.7)	29.4
(84.9)	32.6
(90.7)	34.7
(94.5)	34.5
(94.1)	32.4
(90.3)	28.6
(83.5)	23.1
(73.6)	17.9
(64.2)	26.1
(79.0)
Average low Â°C (Â°F)	12.7
(54.9)	13.7
(56.7)	16.7
(62.1)	20.9
(69.6)	24.7
(76.5)	27.6
(81.7)	29.6
(85.3)	29.9
(85.8)	28.3
(82.9)	24.7
(76.5)	19.3
(66.7)	14.1
(57.4)	21.9
(71.4)
Record low Â°C (Â°F)	5.4
(41.7)	6.0
(42.8)	8.0
(46.4)	11.5
(52.7)	19.0
(66.2)	23.0
(73.4)	26.0
(78.8)	26.8
(80.2)	24.2
(75.6)	17.0
(62.6)	9.8
(49.6)	6.0
(42.8)	5.4
(41.7)
Average precipitation mm (inches)	7.8
(0.31)	9.0
(0.35)	7.6
(0.30)	4.3
(0.17)	0.2
(0.01)	0.0
(0.0)	0.0
(0.0)	0.0
(0.0)	0.0
(0.0)	1.2
(0.05)	3.5
(0.14)	8.3
(0.33)	41.9
(1.65)
Average precipitation days	3.3	3.5	2.5	1.3	0.2	0.0	0.0	0.0	0.0	0.4	1.6	2.8	15.6
Average relative humidity (%)	41	38	33	27	24	23	24	27	31	33	36	41	32
Source: Israel Meteorological Service[27]
Chemistry

Halite deposits (and teepee structure) along the western Dead Sea coast
With 34.2% salinity (in 2011), it is one of the world's saltiest bodies of water, though Lake Vanda in Antarctica (35%), Lake Assal in Djibouti (34.8%), Lagoon GarabogazkÃ¶l in the Caspian Sea (up to 35%) and some hypersaline ponds and lakes of the McMurdo Dry Valleys in Antarctica (such as Don Juan Pond (44%)) have reported higher salinities.

Until the winter of 1978â79, when a major mixing event took place,[28] the Dead Sea was composed of two stratified layers of water that differed in temperature, density, age, and salinity. The topmost 35 meters (115 ft) or so of the Dead Sea had an average salinity of 342 parts per thousand (in 2002), and a temperature that swung between 19 Â°C (66 Â°F) and 37 Â°C (99 Â°F). Underneath a zone of transition, the lowest level of the Dead Sea had waters of a consistent 22 Â°C (72 Â°F) temperature and complete saturation of sodium chloride (NaCl).[citation needed] Since the water near the bottom is saturated, the salt precipitates out of solution onto the sea floor.

Beginning in the 1960s, water inflow to the Dead Sea from the Jordan River was reduced as a result of large-scale irrigation and generally low rainfall. By 1975, the upper water layer was saltier than the lower layer. Nevertheless, the upper layer remained suspended above the lower layer because its waters were warmer and thus less dense. When the upper layer cooled so its density was greater than the lower layer, the waters mixed (1978â79). For the first time in centuries, the lake was a homogeneous body of water. Since then, stratification has begun to redevelop.[28]


Pebbles cemented with Halite on the western shore of the Dead Sea near Ein Gedi
The mineral content of the Dead Sea is very different from that of ocean water. The exact composition of the Dead Sea water varies mainly with season, depth and temperature. In the early 1980s, the concentration of ionic species (in g/kg) of Dead Sea surface water was Clâ (181.4), Brâ (4.2), SO42â (0.4), HCO3â (0.2), Ca2+ (14.1), Na+ (32.5), K+ (6.2) and Mg2+ (35.2). The total salinity was 276 g/kg.[29] These results show that the composition of the salt, as anhydrous chlorides on a weight percentage basis, was calcium chloride (CaCl2) 14.4%, potassium chloride (KCl) 4.4%, magnesium chloride (MgCl2) 50.8% and sodium chloride (NaCl) 30.4%. In comparison, the salt in the water of most oceans and seas is approximately 85% sodium chloride. The concentration of sulfate ions (SO42â) is very low, and the concentration of bromide ions (Brâ) is the highest of all waters on Earth.


Beach pebbles made of Halite; western coast
The salt concentration of the Dead Sea fluctuates around 31.5%. This is unusually high and results in a nominal density of 1.24 kg/l. Anyone can easily float in the Dead Sea because of natural buoyancy. In this respect the Dead Sea is similar to the Great Salt Lake in Utah in the United States.

An unusual feature of the Dead Sea is its discharge of asphalt. From deep seeps, the Dead Sea constantly spits up small pebbles and blocks of the black substance.[30] Asphalt-coated figurines and bitumen-coated Neolithic skulls from archaeological sites have been found. Egyptian mummification processes used asphalt imported from the Dead Sea region.[31][32]

Putative therapies

This article needs more medical references for verification or relies too heavily on primary sources. Please review the contents of the article and add the appropriate references if you can. Unsourced or poorly sourced material may be challenged and removed.
Find sources: "Dead Sea" â news Â· newspapers Â· books Â· scholar Â· JSTOR (March 2015)
Rod of Asclepius2.svg
The Dead Sea area has become a location for health research and potential treatment for several reasons. The mineral content of the water, the low content of pollens and other allergens in the atmosphere, the reduced ultraviolet component of solar radiation, and the higher atmospheric pressure at this great depth each may have specific health effects. For example, persons experiencing reduced respiratory function from diseases such as cystic fibrosis seem to benefit from the increased atmospheric pressure.[33]

The region's climate and low elevation have made it a popular center for assessment of putative therapies:

Climatotherapy: Treatment which exploits local climatic features such as temperature, humidity, sunshine, barometric pressure and special atmospheric constituents
Heliotherapy: Treatment that exploits the biological effects of the sun's radiation
Thalassotherapy: Treatment that exploits bathing in Dead Sea water
Climatotherapy at the Dead Sea may be a therapy for psoriasis[34] by sunbathing for long periods in the area due to its position below sea level and subsequent result that UV rays are partially blocked by the increased cloud cover[citation needed] over the Dead Sea.[35]

Rhinosinusitis patients receiving Dead Sea saline nasal irrigation exhibited improved symptom relief compared to standard hypertonic saline spray in one study.[36]

Dead Sea mud pack therapy has been suggested to temporarily relieve pain in patients with osteoarthritis of the knees. According to researchers of the Ben Gurion University of the Negev, treatment with mineral-rich mud compresses can be used to augment conventional medical therapy.[37]


Panorama of the Dead Sea from the MÃ¶venpick Resort, Jordan.
Fauna and flora

Dead Sea in the morning, seen from Masada
The sea is called "dead" because its high salinity prevents macroscopic aquatic organisms, such as fish and aquatic plants, from living in it, though minuscule quantities of bacteria and microbial fungi are present.

In times of flood, the salt content of the Dead Sea can drop from its usual 35% to 30% or lower. The Dead Sea temporarily comes to life in the wake of rainy winters. In 1980, after one such rainy winter, the normally dark blue Dead Sea turned red. Researchers from Hebrew University of Jerusalem found the Dead Sea to be teeming with a type of alga called Dunaliella. Dunaliella in turn nourished carotenoid-containing (red-pigmented) halobacteria, whose presence caused the color change. Since 1980, the Dead Sea basin has been dry and the algae and the bacteria have not returned in measurable numbers.

In 2011 a group of scientists from Be'er Sheva, Israel and Germany discovered fissures in the floor of the Dead Sea by scuba diving and observing the surface. These fissures allow fresh and brackish water to enter the Dead Sea. They sampled biofilms surrounding the fissures and discovered numerous species of bacteria and archaea.[38]

Many animal species live in the mountains surrounding the Dead Sea. Hikers can see ibex, hares, hyraxes, jackals, foxes, and even leopards. Hundreds of bird species inhabit the zone as well. Both Jordan and Israel have established nature reserves around the Dead Sea.

The delta of the Jordan River was formerly a jungle of papyrus and palm trees. The Jewish historian Flavius Josephus described Jericho as "the most fertile spot in Judea". In Roman and Byzantine times, sugarcane,[dubious â discuss] henna, and sycamore fig all made the lower Jordan valley wealthy. One of the most valuable products produced by Jericho was the sap of the balsam tree, which could be made into perfume. By the 19th century, Jericho's fertility had disappeared.[dubious â discuss]

Human settlement
There are several small communities near the Dead Sea. These include Ein Gedi, Neve Zohar and the Israeli settlements in the Megilot Regional Council: Kalya, Mitzpe Shalem and Avnat. There is a nature preserve at Ein Gedi, and several Dead Sea hotels are located on the southwest end at Ein Bokek near Neve Zohar. Highway 90 runs north-south on the Israeli side for a total distance of 565 km (351 mi) from Metula on the Lebanese border in the north to its southern terminus at the Egyptian border near the Red Sea port of Eilat.

Potash City is a small community on the Jordanian side of the Dead Sea, and others including Suweima. Highway 65 runs north-south on the Jordanian side from near Jordan's northern tip down past the Dead Sea to the port of Aqaba.

Human history

Mount Sodom, Israel, showing the so-called "Lot's Wife" pillar (made of Halite (mineral) like the rest of the mountain)
Biblical period
Dwelling in caves near the Dead Sea is recorded in the Hebrew Bible as having taken place before the Israelites came to Canaan, and extensively at the time of King David.

Just northwest of the Dead Sea is Jericho. Somewhere, perhaps on the southeastern shore, would be the cities mentioned in the Book of Genesis which were said to have been destroyed in the time of Abraham: Sodom and Gomorra (Genesis 18) and the three other "Cities of the Plain", Admah, Zeboim and Zoar (Deuteronomy 29:23). Zoar escaped destruction when Abraham's nephew Lot escaped to Zoar from Sodom (Genesis 19:21â22). Before the destruction, the Dead Sea was a valley full of natural tar pits, which was called the vale of Siddim. King David was said to have hidden from Saul at Ein Gedi nearby.

In Ezekiel 47:8â9 there is a specific prophecy that the sea will "be healed and made fresh", becoming a normal lake capable of supporting marine life. A similar prophecy is stated in Zechariah 14:8, which says that "living waters will go out from Jerusalem, half of them to the eastern sea [likely the Dead Sea] and half to the western sea [the Mediterranean]."

Greek and Roman period
Aristotle wrote about the remarkable waters. The Nabateans and others discovered the value of the globs of natural asphalt that constantly floated to the surface where they could be harvested with nets. The Egyptians were steady customers, as they used asphalt in the embalming process that created mummies. The Ancient Romans knew the Dead Sea as "Palus Asphaltites"[39] (Asphalt Lake).


A cargo boat on the Dead Sea as seen on the Madaba Map, from the 6th century AD
The Dead Sea was an important trade route with ships carrying salt, asphalt and agricultural produce. Multiple anchorages existed on both sides of the sea, including in Ein Gedi, Khirbet Mazin (where the ruins of a Hasmonean-era dry dock are located), Numeira and near Masada.[40][41]

King Herod the Great built or rebuilt several fortresses and palaces on the western bank of the Dead Sea. The most famous was Masada, where in 70 CE a small group of Jewish zealots fled after the fall of the destruction of the Second Temple. The zealots survived until 73 CE, when a siege by the X Legion ended in the deaths by suicide of its 960 inhabitants. Another historically important fortress was Machaerus (×××××¨), on the eastern bank, where, according to Josephus, John the Baptist was imprisoned by Herod Antipas and died.[42]

Also in Roman times, some Essenes settled on the Dead Sea's western shore; Pliny the Elder identifies their location with the words, "on the west side of the Dead Sea, away from the coast ... [above] the town of Engeda" (Natural History, Bk 5.73); and it is therefore a hugely popular but contested hypothesis today, that same Essenes are identical with the settlers at Qumran and that "the Dead Sea Scrolls" discovered during the 20th century in the nearby caves had been their own library.

Josephus identified the Dead Sea in geographic proximity to the ancient Biblical city of Sodom. However, he referred to the lake by its Greek name, Asphaltites.[43]

Various sects of Jews settled in caves overlooking the Dead Sea. The best known of these are the Essenes of Qumran, who left an extensive library known as the Dead Sea Scrolls.[44] The town of Ein Gedi, mentioned many times in the Mishna, produced persimmon for the temple's fragrance and for export, using a secret recipe. "Sodomite salt" was an essential mineral for the temple's holy incense, but was said to be dangerous for home use and could cause blindness.[45] The Roman camps surrounding Masada were built by Jewish slaves receiving water from the towns around the lake. These towns had drinking water from the Ein Feshcha springs and other sweetwater springs in the vicinity.[46]

Byzantine period
Intimately connected with the Judean wilderness to its northwest and west, the Dead Sea was a place of escape and refuge. The remoteness of the region attracted Greek Orthodox monks since the Byzantine era. Their monasteries, such as Saint George in Wadi Kelt and Mar Saba in the Judaean Desert, are places of pilgrimage.

Modern times

The southern basin of the Dead Sea as of 1817-18, with the Lisan Peninsula and its ford (now named Lynch Strait). North is to the right.
In the 19th century the River Jordan and the Dead Sea were explored by boat primarily by Christopher Costigan in 1835, Thomas Howard Molyneux in 1847, William Francis Lynch in 1848, and John MacGregor in 1869.[47] The full text of W. F. Lynch's 1949 book Narrative of the United States' Expedition to the River Jordan and the Dead Sea is available online. Charles Leonard Irby and James Mangles travelled along the shores of the Dead Sea already in 1817â18, but didn't navigate on its waters.[48]


World's lowest (dry) point, Jordan, 1971
Explorers and scientists arrived in the area to analyze the minerals and research the unique climate.

After the find of the "Moabite Stone" in 1868 on the plateau east of the Dead Sea, Moses Wilhelm Shapira and his partner Salim al-Khouri forged and sold a whole range of presumed "Moabite" antiquities, and in 1883 Shapira presented what is now known as the "Shapira Strips", a supposedly ancient scroll written on leather strips which he claimed had been found near the Dead Sea. The strips were declared to be forgeries and Shapira took his own life in disgrace.

In the late 1940s and early 1950s, hundreds of religious documents dated between 150 BCE and 70 CE were found in caves near the ancient settlement of Qumran, about one mile (1.6 kilometres) inland from the northwestern shore of the Dead Sea (presently in the West Bank). They became known and famous as the Dead Sea Scrolls.

The world's lowest roads, Highway 90, run along the Israeli and West Bank shores of the Dead Sea, along with Highway 65 on the Jordanian side, at 393 m (1,289 ft) below sea level.

Tourism and leisure

Ein Bokek, a resort on the Israeli shore
British Mandate period
A golf course named for Sodom and Gomorrah was built by the British at Kalia on the northern shore.

Israel
The first major Israeli hotels were built in nearby Arad, and since the 1960s at the Ein Bokek resort complex.

Israel has 15 hotels along the Dead Sea shore, generating total revenues of $291 million in 2012. Most Israeli hotels and resorts on the Dead Sea are on a six-kilometre (3.7-mile) stretch of the southern shore.[49]

Jordan

Kempinski Hotel, one of the many hotels on the Jordanian shore
On the Jordanian side, nine international franchises have opened seaside resort hotels near the King Hussein Bin Talal Convention Center, along with resort apartments, on the eastern shore of the Dead Sea. The 9 hotels have boosted the Jordanian side's capacity to 2,800 rooms.[50]

On November 22, 2015, the Dead Sea panorama road was included along with 40 archaeological locations in Jordan, to become live on Google Street View.[51]

West Bank
The Palestinian Dead Sea Coast is about 40 kilometres (25 miles) long. The World Bank estimates that a Palestinian Dead Sea tourism industry could generate $290 million of revenues per year and 2,900 jobs.[49] However, Palestinians have been unable to obtain construction permits for tourism-related investments on the Dead Sea.[49] According to the World Bank, Officials in the Palestinian Ministry of Tourism and Antiquities state that the only way to apply for such permits is through the Joint Committees established under the Oslo Agreement, but the relevant committee has not met with any degree of regularity since 2000.[49]

Chemical industry

View of salt evaporation pans on the Dead Sea, taken in 1989 from the Space Shuttle Columbia (STS-28). The southern half is separated from the northern half at what used to be the Lisan Peninsula because of the fall in level of the Dead Sea.

View of the mineral evaporation ponds almost 12 years later (STS-102). A northern and small southeastern extension were added and the large polygonal ponds subdivided.

The dwindling water level of the Dead Sea
British Mandate period
In the early part of the 20th century, the Dead Sea began to attract interest from chemists who deduced the sea was a natural deposit of potash (potassium chloride) and bromine. The Palestine Potash Company was chartered in 1929, after its founder, Siberian Jewish engineer and pioneer of Lake Baikal exploitation, Moses Novomeysky, worked for the charter for over ten years having first visited the area in 1911.[52] The first plant, on the north shore of the Dead Sea at Kalya, commenced production in 1931[52] and produced potash by solar evaporation of the brine. Employing Arabs and Jews, it was an island of peace in turbulent times.[53] The company quickly grew into the largest industrial site in the Middle East,[citation needed] and in 1934 built a second plant on the southwest shore, in the Mount Sodom area, south of the 'Lashon' region of the Dead Sea. Palestine Potash Company supplied half of Britain's potash during World War II. Both plants were destroyed by the Jordanians in the 1948 ArabâIsraeli War.[54]

Israel
The Dead Sea Works was founded in 1952 as a state-owned enterprise based on the remnants of the Palestine Potash Company.[55] In 1995, the company was privatized and it is now owned by Israel Chemicals. From the Dead Sea brine, Israel produces (2001) 1.77 million tons potash, 206,000 tons elemental bromine, 44,900 tons caustic soda, 25,000 tons magnesium metal, and sodium chloride. Israeli companies generate around US$3 billion annually from the sale of Dead Sea minerals (primarily potash and bromine), and from other products that are derived from Dead Sea Minerals.[49]

Jordan
On the Jordanian side of the Dead Sea, Arab Potash (APC), formed in 1956, produces 2.0 million tons of potash annually, as well as sodium chloride and bromine. The plant is located at Safi, South Aghwar Department, in the Karak Governorate.

Jordanian Dead Sea mineral industries generate about $1.2 billion in sales (equivalent to 4 percent of Jordan's GDP).

West Bank
The Palestinian Dead Sea Coast is about 40 kilometres (25 miles) long. The Palestinian economy is unable to benefit from Dead Sea chemicals due to restricted access, permit issues and the uncertainties of the investment climate.[49] The World Bank estimates that a Palestinian Dead Sea chemicals industry could generate $918M incremental value added per year, "almost equivalent to the contribution of the entire manufacturing sector of Palestinian territories today".[49]

Extraction
Both companies, Dead Sea Works Ltd. and Arab Potash, use extensive salt evaporation pans that have essentially diked the entire southern end of the Dead Sea for the purpose of producing carnallite, potassium magnesium chloride, which is then processed further to produce potassium chloride. The ponds are separated by a central dike that runs roughly north-south along the international border. The power plant on the Israeli side allows production of magnesium metal (by a subsidiary, Dead Sea Magnesium Ltd.).

Due to the popularity of the sea's therapeutic and healing properties, several companies have also shown interest in the manufacturing and supplying of Dead Sea salts as raw materials for body and skin care products.

Recession and environmental concerns

Gully in unconsolidated Dead Sea sediments exposed by recession of water levels. It was excavated by floods from the Judean Mountains in less than a year.
Since 1930, when its surface was 1,050 km2 (410 sq mi) and its level was 390 m (1,280 ft) below sea level, the Dead Sea has been monitored continuously.[56] In recent decades,[which?] the Dead Sea has been rapidly shrinking because of diversion of incoming water from the Jordan River to the north. The southern end is fed by a canal maintained by the Dead Sea Works, a company that converts the sea's raw materials. From a water surface of 395 m (1,296 ft) below sea level in 1970[57] it fell 22 m (72 ft) to 418 m (1,371 ft) below sea level in 2006, reaching a drop rate of 1 m (3 ft) per year. As the water level decreases, the characteristics of the Sea and surrounding region may substantially change.

The Dead Sea level drop has been followed by a groundwater level drop, causing brines that used to occupy underground layers near the shoreline to be flushed out by freshwater. This is believed to be the cause of the recent appearance of large sinkholes along the western shoreâincoming freshwater dissolves salt layers, rapidly creating subsurface cavities that subsequently collapse to form these sinkholes.[58]

In May 2009 at the World Economic Forum, Jordan announced its plans to construct the "Jordan National Red Sea Development Project" (JRSP). This is a plan to convey seawater from the Red Sea near Aqaba to the Dead Sea. Water would be desalinated along the route to provide fresh water to Jordan, with the brine discharge sent to the Dead Sea for replenishment. Israel has expressed its support and will likely benefit from some of the water delivery to its Negev region.[59][60]

At a regional conference in July 2009, officials expressed concern about the declining water levels. Some suggested industrial activities around the Dead Sea might need to be reduced. Others advised environmental measures to restore conditions such as increasing the volume of flow from the Jordan River to replenish the Dead Sea. Currently, only sewage and effluent from fish ponds run in the river's channel. Experts also stressed the need for strict conservation efforts. They said agriculture should not be expanded, sustainable support capabilities should be incorporated into the area and pollution sources should be reduced.[61]


The planned Red SeaâDead Sea Water Conveyance, whose first phase will begin construction in 2021, will work towards stabilizing the falling levels of the Dead Sea
Year	Water level (m)	Surface (km2)
1930	â390	1050
1980	â400	680
1992	â407	675
1997	â411	670
2004	â417	662
2010	â423	655
2016	â430.5	605
Sources: Israel Oceanographic and Limnological Research,[4] Haaretz,[2] Jewish Virtual Library,[62][63] Jordan Valley Authority.[64]

In October 2009, the Jordanians announced accelerated plans to extract around 300 million cubic metres (11 billion cubic feet) of water per year from the Red Sea, desalinate it for use as fresh water and send the waste water to the Dead Sea by tunnel, despite concerns about inadequate time to assess the potential environmental impact. According to Jordan's minister for water, General Maysoun Zu'bi, this project could be considered as the first phase of the Red SeaâDead Sea Water Conveyance.[65]

In December 2013, Israel, Jordan and the Palestinian Authority signed an agreement for laying a water pipeline to link the Red Sea with the Dead Sea. The pipeline will be 180 km (110 mi) long and is estimated to take up to five years to complete.[66] In January 2015 it was reported that the level of water is now dropping by 1 m (3 ft) a year.[67]

On 27 November 2016, it was announced that the Jordanian government is shortlisting five consortiums to implement the project. Jordan's ministry of Water and Irrigation said that the $100 million first phase of the project will begin construction in the first quarter of 2018, and will be completed by 2021.[12]


Views in 1972, 1989, and 2011 compared[68]
See also
Aral Sea
Salton Sea
List of drying lakes
List of places on land with elevations below sea level
MediterraneanâDead Sea Canal
World Discoveries III: Dead Sea
Benjamin Elazari Volcani
PEF rock with the Dead Sea level reference line used between 1900-1913
icon	Water portal
icon	Asia portal
References
 "Virtual Israel Experience: The Dead Sea". Jewish Virtual Library. Retrieved 21 January 2013.
 "The Dead Sea Is Dying Fast: Is It Too Late to Save It, or Was It Always a Lost Cause?". Haaretz. 7 October 2016. Archived from the original on 22 December 2016.
 Dead Sea Data Summary 2015.Water Authority of Israel.
"Red Sea - Dead Sea Water Conveyance Study Program". The World Bank Group. 2013. Archived from the original on 2013-09-15.
 "Long-Term changes in the Dead Sea". Israel Oceanographic and Limnological Research - Israel Marine Data Center (ISRAMAR).
 The first article al- is unnecessary and usually not used.
 Dead Sea: Israel and Jordan. Library of Congress: Subject Headings. 1: A-E (14th ed.). Washington, D.C.: Cataloging Distribution Service, Library of Congress. 1991. p. 1163. ISSN 1048-9711. Retrieved 30 December 2019.
 Moshe Sharon (1999). Bani Na'im: Maqam an-Nabi Lut. Corpus Inscriptionum Arabicarum Palaestinae (CIAP). Two: B-C. Leiden-Boston-KÃ¶ln: Brill. p. 15 (of pp.12â21). ISBN 978-90-04-11083-0. Retrieved 30 December 2019.
 "Israel and Jordan Sign 'Historic' $900 Million Deal to Save the Dead Sea". Newsweek. 2015-02-27.
 Goetz, P. W., ed. (1986). "Dead Sea". The New EncyclopÃ¦dia Britannica. 3 (15th ed.). Chicago. p. 937.
 R W McColl, ed. (2005). Encyclopedia of world geography. Facts on File. p. 237. ISBN 9780816072293.
 "Dead Sea - Composition of Dead Sea Water". Archived from the original on 2013-11-04.
 "5 alliances shortlisted to execute Red-Dead's phase I". The Jordan Times. 27 November 2016. Retrieved 3 December 2016.
 David Bridger; Samuel Wolk (September 1976). The New Jewish Encyclopedia. Behrman House, Inc. p. 109. ISBN 978-0-87441-120-1. Retrieved July 25, 2011. It was named the "Dead Sea" because of the fact that no living thing can exist there, since the water is extremely salty and bitter.
 See bitumen and asphalt for more about asphaltite.
 "Springs and quicksand at the Dead Sea". Archived from the original on November 22, 2008. Retrieved August 27, 2008.
 "Red Sea â Dead Sea Water Conveyance Study (RSDSC) Program: Dead Sea Study, July 2010, p. 64" (PDF). Archived from the original (PDF) on 2016-12-24. Retrieved 2016-10-14.
 "Mujib". UNESCO. Retrieved 7 May 2016.
 "Dead Sea". Exact Me.org. Archived from the original on January 20, 2013. Retrieved January 21, 2013.
 Mordechai Stein. "The limnological history of late Pleistocene â Holocene water bodies in the Dead Sea basin" (PDF). Archived from the original (PDF) on 2013-05-13.
 Uri Kafri; Yoseph Yechieli (2010). Groundwater Base Level Changes and Adjoining Hydrological Systems. Springer Science & Business Media. p. 123. Bibcode:2010gblc.book.....K. ISBN 9783642139444.
 Ben-Avraham, Zvi; Katsman, Regina (2015). "The formation of graben morphology in the Dead Sea Fault, and its implications". Geophysical Research Letters. American Geophysical Union. 42 (17). 2.2. Sedimentary Regime, p. 6991 (of 6989â6996). Bibcode:2015GeoRL..42.6989B. doi:10.1002/2015GL065111. Estuarine-lagoonal series of syn-rift evaporites of the latest MioceneâPliocene ages. Sedimentary regime and mineral composition indicate that .... the Sedom formation in the DSB [Dead Sea Basin] .... , consisting mainly of halite, can be related to ingression of sea waters .... through the Yezreel Valley inland into the Jordan-Arava rift valley (from the Sea of Galilee to the present-day Dead Sea....) in the Late Neogene. After its disconnection from the open sea that could be associated with either eustatic changes in the sea, tectonic uplift of Judea-Samaria anticline, or other processes [Stein, 2014], the rift valley was occupied by a series of hypersaline terminal lakes. They occasionally evaporated and precipitated halite. .... Restoration of the Sedom diapir to its original uniform thickness covering the basin floor yields 2.3 km.
 Geochemical Society; Meteoritical Society (1971). Geochimica et Cosmochimica Acta. Pergamon Press. Retrieved April 12, 2011.
 "Natural Resources". Dead Sea Research Center. Retrieved 27 October 2013.
 "Lowest Elevation: Dead Sea". Extreme Science. Retrieved May 22, 2007.
 Kramer, MR; Springer C; Berkman N; Glazer M; Bublil M; Bar-Yishay E; Godfrey S (March 1998). "Rehabilitation of hypoxemic patients with COPD at low altitude at the Dead Sea, the lowest place on earth" (PDF). Chest. 113 (3): 571â575. doi:10.1378/chest.113.3.571. PMID 9515826. Archived from the original (PDF) on 2013-10-29.
 "Climatological Averages for Dead Sea". IMS. Archived from the original on June 7, 2011. Retrieved June 9, 2011.
 "Averages and Records for several places in Israel". Israel Meteorological Service. June 2011. Archived from the original on 2010-09-14.
 "Dead Sea Canal". American.edu. 1996-12-09. Archived from the original on May 22, 2009. Retrieved May 5, 2009.
 I. Steinhorn, In Situ Salt Precipitation at the Dead Sea, Limnol. Oceanogr. 28(3),1983, 580-583
 Bein, A.; O. Amit (2007). "The evolution of the Dead Sea floating asphalt blocks: simulations by pyrolisis". Journal of Petroleum Geology. 2 (4): 439â447. Bibcode:1980JPetG...2..439B. doi:10.1111/j.1747-5457.1980.tb00971.x. Archived from the original on 2013-01-05.
 Niemi, Tina M., Zvi Ben-Avraham and Joel Gat, eds., The Dead Sea: the lake and its setting, 1997, Oxford University Press, p. 251 ISBN 978-0-19-508703-1
 J. RullkÃ¶tter; A. Nissenbaum (December 1988). "Dead sea asphalt in Egyptian mummies: Molecular evidence". Naturwissenschaften. 75 (12): 618â621. Bibcode:1988NW.....75..618R. doi:10.1007/BF00366476. PMID 3237249. S2CID 29037897.
 "Asthma, Cystic Fibrosis, Chronic Obstructive Lung Disease". Dead Sea Research Center. Retrieved May 22, 2007.
 Cohen, Arnon D.; VanâDijk, Dina; Naggan, Lechaim; Vardy, Daniel A. (2005). "Effectiveness of climatotherapy at the Dead Sea for psoriasis vulgaris: A community-oriented study introducing the Beer Sheva Psoriasis Severity Score". Dermatological Treatment. 16 (5â6): 308â313. doi:10.1080/09546630500375841. PMID 16428150. S2CID 27903493.
 S. Halevy; et al. (1997). "Dead sea bath salt for the treatment of psoriasis vulgaris: a double-blind controlled study". Journal of the European Academy of Dermatology and Venereology. 9 (3): 237â242. doi:10.1111/j.1468-3083.1997.tb00509.x. S2CID 71957649.
 Michael Friedman; Ramakrishnan Vidyasagar; Ninos Joseph (June 2006). "A Randomized, Prospective, Double-Blind Study on the Efficacy of Dead Sea Salt Nasal Irrigations" (PDF). The Laryngoscope. 116 (6): 878â882. doi:10.1097/01.mlg.0000216798.10007.76. PMID 16735920. S2CID 13013715.
 Flusser, Daniel; Abu-Shakra, Mahmoud; Friger, Michael; Codish, Shlomi; Sukenik, Shaul (August 2002). "Therapy With Mud Compresses for Knee Osteoarthritis: Comparison of Natural Mud Preparations With Mineral-Depleted Mud" (PDF). Journal of Clinical Rheumatology. 8 (4): 197â203. doi:10.1097/00124743-200208000-00003. PMID 17041359. S2CID 7647456. Archived from the original (PDF) on 2011-08-16.
 Ionescu, Danny; Siebert, Christian; Polerecky, Lubos; Munwes, Yaniv Y.; Lott, Christian; HÃ¤usler, Stefan; BiÅ¾iÄ-Ionescu, Mina; Quast, Christian; Peplies, JÃ¶rg; GlÃ¶ckner, Frank Oliver; Ramette, Alban; RÃ¶diger, Tino; Dittmar, Thorsten; Oren, Aharon; Geyer, Stefan; StÃ¤rk, Hans-Joachim; Sauter, Martin; Licha, Tobias; Laronne, Jonathan B.; De Beer, Dirk (2012). "Microbial and Chemical Characterization of Underwater Fresh Water Springs in the Dead Sea". PLOS ONE. 7 (6): e38319. Bibcode:2012PLoSO...738319I. doi:10.1371/journal.pone.0038319. PMC 3367964. PMID 22679498.
 "Asphaltites examples from ancient sources". Wordnik.com. Retrieved 2013-08-22.
 Hadas, Gideon (April 2011). "Dead Sea Anchorages". Revue Biblique. 118 (2): 161â179. JSTOR 44092052.
 Sailing the Dead Sea, Israel Museum
 Josephus, Antiquities of the Jews 18.119[permanent dead link].
 Josephus. "9". Antiquities of the Jews. 1.
 Found today in the Shrine of the Book at the Israel Museum of Jerusalem
 "Sodomite salt could cause blindness". Archived from the original on 2009-08-15.
 A synagogue mosaic floor (circa 100 BCE) at Ein Gedi repeats the Mishna, portraying a curse on whoever reveals the town's secret persimmon recipe. Papyrus parchments found in caves near the Dead Sea document the vast amount of cultivated land in the area, especially persimmon trees, but also olive and date trees
 "History of the Dead Sea - Discover the Dead Sea with Us!". 1 July 2016.
 "'The unfortunate Costigan', first surveyor of the Dead Sea". 25 February 2013.
 World Bank, Poverty Reduction and Economic Management Department, Area C and the Future of the Palestinian Economy, October 2, 2013
 "Dead Sea, Aqaba hotels packed during Eid Al Fitr holiday". The Jordan Times. 10 July 2016. Retrieved 12 July 2016.
 "Google Street View".
 Jacob Norris (11 April 2013). Land of Progress: Palestine in the Age of Colonial Development, 1905-1948. OUP Oxford. pp. 159â. ISBN 978-0-19-966936-3.
 "Wealth From The Dead Sea". Popular Mechanics. Vol. 54 no. 5. Chicago: Hearst Magazines. November 1930. pp. 794â798.
 Hurlbert, Stuart H. (6 December 2012). Saline Lakes V: Proceedings of the Vth International Symposium on Inland Saline Lakes, held in Bolivia, 22â29 March 1991. Springer Science & Business Media. ISBN 9789401120760 â via Google Books.
 Schechter, Asher (14 April 2013). "Who Really Owns the Dead Sea?". Haaretz.
 "Overview of Middle East Water Resources_Dead Sea". Jewish Virtual Library. December 1998. Retrieved 31 May 2014.
 C. Klein; A. Flohn. Contribution to the Knowledge in the Fluctuations of the Dead Sea Level. 38. Theoretical and Applied Climatology. pp. 151â156, 1987.
 M. Abelson; Y. Yechieli; O. Crouvi; G. Baer; D. Wachs; A. Bein; V. Shtivelman (2006). Evolution of the Dead Sea Sinkholes in. special paper 401. Geological Society of America. pp. 241â253.
 "Jordan, Israel agree $900 million Red Sea-Dead Sea project". Reuters. 26 February 2015. Retrieved 11 December 2018.
 Jordan Red Sea Project: Archived 2012-03-24 at the Wayback Machine Original: Jordan Red Sea Project Description, retrieved on May 11, 2011
 Ehud Zion Waldoks (July 8, 2009). "Back from the Dead?". The Jerusalem Post. Archived from the original on October 27, 2013.
 "Water level of the Dead Sea". Jewish Virtual Library. Retrieved 31 May 2014.
 "Water level and surface area of the Dead Sea". Jewish Virtual Library. Retrieved 31 May 2014.
 Eng. Saâad Abu Hammour, JVA. "River Basin Management" (PDF). Jordan Valley Authority. Archived from the original (PDF) on 2014-05-31. Retrieved 31 May 2014.
 "Jordan to refill shrinking Dead Sea". Daily Telegraph. 10 October 2009.
 Sherwood, Harriet (2013-12-09). "Dead Sea neighbours agree to pipeline to pump water from Red Sea". The Guardian.
 Catholic Online. "Dead Sea Dying: Levels of salt water are dropping by three feet annually".
 "The Dead Sea : Image of the Day". 6 April 2012.
Further reading
The World Bank, 2013, "The Red Sea - Dead Sea Water Conveyance Study Program", and source of basic data on the Dead Sea.
Yehouda Enzel, et al., eds (2006) New Frontiers in Dead Sea Paleoenvironmental Research, Geological Society of America, ISBN 0-8137-2401-5
Niemi, Tina M., Ben-Avraham, Z., and Gat, J., eds., 1997, The Dead Sea: The Lake and Its Setting: N.Y., Oxford University Press, 286 p.
World Bank, Poverty Reduction and Economic Management Department, Area C and the Future of the Palestinian Economy, October 2, 2013
External links
 Media related to Dead Sea at Wikimedia Commons
 Dead Sea travel guide from Wikivoyage (Israeli and West Bank part and Jordanian part)
 The dictionary definition of Dead Sea at Wiktionary





Photograph a historic site, help Wikipedia, and win a prize. Participate in the world's largest photography competition this month!
Learn more
Alice's Adventures in Wonderland
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"Alice in Wonderland" redirects here. For other uses, see Alice in Wonderland (disambiguation).
Alice's Adventures in Wonderland
Alice's Adventures in Wonderland cover (1865).jpg
Cover of the original edition (1865)
Author	Lewis Carroll
Illustrator	John Tenniel
Country	United Kingdom
Language	English
Genre	Fantasy
Literary nonsense
Publisher	Macmillan
Publication date	26 November 1865
Followed by	Through the Looking-Glass 
Alice's Adventures in Wonderland (commonly shortened to Alice in Wonderland) is an 1865 novel by English author Lewis Carroll (the pseudonym of Charles Dodgson).[1] It tells of a young girl named Alice, who falls through a rabbit hole into a subterranean fantasy world populated by peculiar, anthropomorphic creatures. It is considered to be one of the best examples of the literary nonsense genre.[2][3] The tale plays with logic, giving the story lasting popularity with adults as well as with children.[2]

One of the best-known and most popular works of English-language fiction, its narrative, structure, characters and imagery have been enormously influential in popular culture and literature, especially in the fantasy genre.[3] The work has never been out of print, and it has been translated into at least 97 languages.[4] Its ongoing legacy encompasses many adaptations for stage, screen, radio, art, ballet, theme parks, board games, and video games.[5] Carroll published a sequel in 1871, entitled Through the Looking-Glass, and a shortened version for young children, The Nursery "Alice", in 1890.


Contents
1	Background
1.1	"All in the golden afternoon..."
1.2	Manuscript: Alice's Adventures Under Ground
2	Synopsis
3	Characters
3.1	Character allusions
4	Poems and songs
5	Writing style and themes
5.1	Symbolism
5.2	Language
5.3	Mathematics
5.4	Eating and devouring
6	Illustrations
7	Publication history
7.1	Publication timeline
8	Reception by reviewers
9	Adaptations and influence
9.1	Cinema and television
9.2	Comic strips and books
9.3	Live performance
9.4	Gallery
10	See also
11	References
12	External links
Background
"All in the golden afternoon..."
Alice's Adventures in Wonderland was published in 1865. It was inspired when, three years earlier on 4 July,[6] Lewis Carroll and the Reverend Robinson Duckworth rowed up the Isis river in a boat with three young girls. This day was known as the "golden afternoon,"[7] prefaced in the novel as a poem. The poem might be a confusion or even another Alice-tale, for it turns out that particular day was cool, cloudy and rainy.[8] The three girls would be the daughters of scholar Henry Liddell: Lorina Charlotte Liddell (aged 13; "Prima" in the book's prefatory verse); Alice Pleasance Liddell (aged 10; "Secunda" in the verse); and Edith Mary Liddell (aged 8; "Tertia" in the verse).[9]

The journey began at Folly Bridge, Oxford and ended five miles away in the Oxfordshire village of Godstow. During the trip Dodgson told the girls a story that featured a bored little girl named Alice who goes looking for an adventure. The girls loved it, and Alice Liddell asked Dodgson to write it down for her.[10]

Manuscript: Alice's Adventures Under Ground

A page from the original manuscript copy of Alice's Adventures Under Ground, 1864, held in the British Library
He began writing the manuscript of the story the next day, although that earliest version is lost to history. The girls and Dodgson took another boat trip a month later when he elaborated the plot to the story of Alice, and in November he began working on the manuscript in earnest.[11]

To add the finishing touches he researched natural history in connection with the animals presented in the book, and then had the book examined by other children â particularly those of George MacDonald. Though Dodgson did add his own illustrations, he subsequently approached John Tenniel to illustrate the book for publication, telling him that the story had been well liked by children.[11]

On 26 November 1864, Dodgson gave Alice the handwritten manuscript of Alice's Adventures Under Ground, with illustrations by Dodgson himself, dedicating it as "A Christmas Gift to a Dear Child in Memory of a Summer's Day".[12] Some, including Martin Gardner, speculate that there was an earlier version that was destroyed later by Dodgson when he wrote a more elaborate copy by hand.[13]

Before Alice received her copy, Dodgson was already preparing it for publication and expanding the 15,500-word original to 27,500 words,[14] most notably adding the episodes about the Cheshire Cat and the Mad Tea Party.

Synopsis

The White Rabbit
Chapter One â Down the Rabbit Hole: Alice, a seven-year-old girl, is feeling bored and drowsy while sitting on the riverbank with her elder sister. She notices a talking, clothed white rabbit with a pocket watch run past. She follows it down a rabbit hole where she suddenly falls a long way to a curious hall with many locked doors of all sizes. She finds a little key to a door too small for her to fit through, but through it, she sees an attractive garden. She then discovers a bottle on a table labelled "DRINK ME," the contents of which cause her to shrink too small to reach the key which she had left on the table. She subsequently eats a cake labelled "EAT ME" in currants as the chapter closes.

Chapter Two â The Pool of Tears: The chapter opens with Alice growing to such a tremendous size that her head hits the ceiling. Unhappy, Alice begins to cry and her tears literally flood the hallway. Shrinking down again due to a fan she had picked up, Alice swims through her own tears and meets a mouse, who is swimming as well. Alice, thinking he may be a French mouse, tries to make small talk with him in elementary French. Her opening gambit "OÃ¹ est ma chatte?" (transl.â"Where is my cat?"), however, offends the mouse, who then tries to escape her.

Chapter Three â The Caucus Race and a Long Tale: The sea of tears becomes crowded with other animals and birds that have been swept away by the rising waters. Alice and the other animals convene on the bank and the question among them is how to get dry again. Mouse gives them a very dry lecture on William the Conqueror. A dodo decides that the best thing to dry them off would be a Caucus-Race, which consists of everyone running in a circle with no clear winner. Alice eventually frightens all the animals away, unwittingly, by talking about her (moderately ferocious) cat.

Chapter Four â The Rabbit Sends a Little Bill: White Rabbit appears again in search of the Duchess's gloves and fan. Mistaking her for his maidservant, Mary Ann, Rabbit orders Alice to go into the house and retrieve them. Inside the house she finds another little bottle and drinks from it, immediately beginning to grow again. The horrified Rabbit orders his gardener, Bill the Lizard, to climb on the roof and go down the chimney. Outside, Alice hears the voices of animals that have gathered to gawk at her giant arm. The crowd hurls pebbles at her, which turn into little cakes. Alice eats them, and they reduce her again in size.

Chapter Five â Advice from a Caterpillar: Alice comes upon a mushroom and sitting on it is a blue caterpillar smoking a hookah. Caterpillar questions Alice, who begins to admit to her current identity crisis, compounded by her inability to remember a poem. Before crawling away, the caterpillar tells Alice that one side of the mushroom will make her taller and the other side will make her shorter. She breaks off two pieces from the mushroom. One side makes her shrink smaller than ever, while another causes her neck to grow high into the trees, where a pigeon mistakes her for a serpent. With some effort, Alice brings herself back to her normal height. She stumbles upon a small estate and uses the mushroom to reach a more appropriate height.


The Cheshire Cat
Chapter Six â Pig and Pepper: A fish-footman has an invitation for the Duchess of the house, which he delivers to a frog-footman. Alice observes this transaction and, after a perplexing conversation with the frog, lets herself into the house. The Duchess's cook is throwing dishes and making a soup that has too much pepper, which causes Alice, the Duchess, and her baby (but not the cook or grinning Cheshire Cat) to sneeze violently. Alice is given the baby by the Duchess and, to Alice's surprise, the baby turns into a pig. The Cheshire Cat appears in a tree, directing her to the March Hare's house. He disappears but his grin remains behind to float on its own in the air prompting Alice to remark that she has often seen a cat without a grin but never a grin without a cat.

Chapter Seven â A Mad Tea-Party: Alice becomes a guest at a "mad" tea party along with the March Hare, the Hatter, and a very tired Dormouse, who falls asleep frequently only to be violently awakened moments later by the March Hare and the Hatter. The characters give Alice many riddles and stories, including the famous "why is a raven like a writing desk?." The Hatter reveals that they have tea all day because Time has punished him by eternally standing still at 6 PM (tea time). Alice becomes insulted and tired of being bombarded with riddles and she leaves claiming that it was the stupidest tea party that she had ever been to.


Alice trying to play croquet with a Flamingo
Chapter Eight â The Queen's Croquet Ground: Alice leaves the tea party and enters the garden where she comes upon three living playing cards painting the white roses on a rose tree red because The Queen of Hearts hates white roses. A procession of more cards, kings and queens and even the White Rabbit enters the garden. Alice then meets the King and Queen. The Queen, a figure difficult to please, introduces her signature phrase "Off with his head!" which she utters at the slightest dissatisfaction with a subject. Alice is invited (or some might say ordered) to play a game of croquet with the Queen and the rest of her subjects but the game quickly descends into chaos. Live flamingos are used as mallets and hedgehogs as balls and Alice once again meets the Cheshire Cat. The Queen of Hearts then orders the Cat to be beheaded, only to have her executioner complain that this is impossible since the head is all that can be seen of him. Because the cat belongs to the Duchess, the Queen is prompted to release the Duchess from prison to resolve the matter.

Chapter Nine â The Mock Turtle's Story: The Duchess is brought to the croquet ground at Alice's request. She ruminates on finding morals in everything around her. The Queen of Hearts dismisses her on the threat of execution and she introduces Alice to the Gryphon, who takes her to the Mock Turtle. The Mock Turtle is very sad, even though he has no sorrow. He tries to tell his story about how he used to be a real turtle in school, which the Gryphon interrupts so they can play a game.

Chapter Ten â Lobster Quadrille: The Mock Turtle and the Gryphon dance to the Lobster Quadrille, while Alice recites (rather incorrectly) "'Tis the Voice of the Lobster". The Mock Turtle sings them "Beautiful Soup" during which the Gryphon drags Alice away for an impending trial.


The Queen of Hearts glaring at Alice, screaming "Off with her head! Offâ". "Nonsense!" said Alice, very loudly and decidedly, and the Queen was silent.
Chapter Eleven â Who Stole the Tarts?: Alice attends a trial whereby the Knave of Hearts is accused of stealing the Queen's tarts. The jury is composed of various animals, including Bill the Lizard, the White Rabbit is the court's trumpeter, and the judge is the King of Hearts. During the proceedings, Alice finds that she is steadily growing larger. The dormouse scolds Alice and tells her she has no right to grow at such a rapid pace and take up all the air. Alice scoffs and calls the dormouse's accusation ridiculous because everyone grows and she cannot help it. Meanwhile, witnesses at the trial include the Hatter, who displeases and frustrates the King through his indirect answers to the questioning, and the Duchess's cook.

Chapter Twelve â Alice's Evidence: Alice is then called up as a witness. She accidentally knocks over the jury box with the animals inside them and the King orders the animals be placed back into their seats before the trial continues. The King and Queen order Alice to be gone, citing Rule 42 ("All persons more than a mile high to leave the court"), but Alice disputes their judgement and refuses to leave. She argues with the King and Queen of Hearts over the ridiculous proceedings, eventually refusing to hold her tongue, only to say, "It's not that I was the one who stole the tarts in the first place", in the process. Finally, the Queen confirms that Alice was the culprit responsible of stealing the tarts after all (which automatically pardons the Knave of Hearts of his charges), and shouts, "Off with her head!", but Alice is unafraid, calling them just a pack of cards; although Alice holds her own for a time, the card guards soon gang up and start to swarm all over her. Alice's sister wakes her up from a dream, brushing what turns out to be some leaves and not a shower of playing cards from Alice's face. Alice leaves her sister on the bank to imagine all the curious happenings for herself.

Characters
Further information: List of minor characters in the Alice series
The main characters in Alice's Adventures in Wonderland are the following:

Alice
The White Rabbit
The Mouse
The Dodo
The Lory
The Eaglet
The Duck
Pat
Bill the Lizard
Puppy
The Caterpillar
The Duchess
The Cheshire Cat
The Mad Hatter
The March Hare
The Dormouse
The Queen of Hearts
The King of Hearts
The Knave of Hearts
The Gryphon
The Mock Turtle
Character allusions

Jessie Willcox Smith's illustration of Alice surrounded by the characters of Wonderland (1923)

Mad tea party. Theophilus Carter, an Oxford furniture dealer, has been suggested as a model for The Hatter
In The Annotated Alice, Martin Gardner provides background information for the characters. The members of the boating party that first heard Carroll's tale show up in Chapter 3 ("A Caucus-Race and a Long Tale"). Alice Liddell herself is there, while Carroll is caricatured as the Dodo (because Dodgson stuttered when he spoke, he sometimes pronounced his last name as Dodo-Dodgson). The Duck refers to Canon Duckworth, and the Lory and Eaglet to Alice Liddell's sisters Lorina and Edith.[13]:27

Bill the Lizard may be a play on the name of British Prime Minister Benjamin Disraeli.[15] One of Tenniel's illustrations in Through the Looking-Glassâthe 1871 sequel to Aliceâdepicts the character referred to as the "Man in White Paper" (whom Alice meets as a fellow passenger riding on the train with her) as a caricature of Disraeli, wearing a paper hat.[13]:172 The illustrations of the Lion and the Unicorn (also in Looking-Glass) bear a striking resemblance to Tenniel's Punch illustrations of Gladstone and Disraeli as well.[13]:226

Gardner has suggested that the Hatter is a reference to Theophilus Carter, a furniture dealer known in Oxford, and that Tenniel apparently drew the Hatter to resemble Carter, on a suggestion of Carroll's.[13]:69 The Dormouse tells a story about three little sisters named Elsie, Lacie, and Tillie. These are the Liddell sisters: Elsie is L.C. (Lorina Charlotte); Tillie is Edith (her family nickname is Matilda); and Lacie is an anagram of Alice.[13]:75

The Mock Turtle speaks of a drawling-master, "an old conger eel," who came once a week to teach "Drawling, Stretching, and Fainting in Coils." This is a reference to the art critic John Ruskin, who came once a week to the Liddell house to teach the children drawing, sketching, and painting in oils. (The children did, in fact, learn well; Alice Liddell, for one, produced a number of skilful watercolours.)[13]:98

The Mock Turtle also sings "Turtle Soup." This is a parody of a song called "Star of the Evening, Beautiful Star", which was performed as a trio by Lorina, Alice and Edith Liddell for Lewis Carroll in the Liddell home during the same summer in which he first told the story of Alice's Adventures Under Ground.[16]

Poems and songs
Carroll wrote multiple poems and songs for Alice's Adventures in Wonderland, including:

"All in the golden afternoon..."âthe prefatory verse to the book, an original poem by Carroll that recalls the rowing expedition on which he first told the story of Alice's adventures underground
"How Doth the Little Crocodile"âa parody of Isaac Watts' nursery rhyme, "Against Idleness and Mischief"
"The Mouse's Tale"âan example of concrete poetry
"You Are Old, Father William"âa parody of Robert Southey's "The Old Man's Comforts and How He Gained Them"
The Duchess's lullaby, "Speak roughly to your little boy..."âa parody of David Bates' "Speak Gently"
"Twinkle, Twinkle, Little Bat"âa parody of Jane Taylor's "Twinkle Twinkle Little Star"
"The Lobster Quadrille"âa parody of Mary Botham Howitt's "The Spider and the Fly"
"'Tis the Voice of the Lobster"âa parody of Isaac Watts' "The Sluggard"
"Beautiful Soup"âa parody of James M. Sayles's "Star of the Evening, Beautiful Star"
"The Queen of Hearts"âan actual nursery rhyme
"They told me you had been to her..."âWhite Rabbit's evidence
Writing style and themes
Symbolism

Three cards painting the white rose tree red to cover it up from the Queen of Hearts. A red rose symbolised the English House of Lancaster, a white rose their rival House of York.
Martin Gardner, along with other scholars, have shown the book to be filled with many parodies of Victorian popular culture, suggesting it belongs in spirit with W. S. Gilbert and Alfred Cellier's Topsyturveydom.[17]

Most of the book's adventures may have been based on or influenced by people, situations, and buildings in Oxford and at Christ Church. For example, the "Rabbit Hole" symbolised the actual stairs in the back of the Christ Church's main hall. A carving of a griffon and rabbit, as seen in Ripon Cathedral, where Carroll's father was a canon, may have provided inspiration for the tale.[18]

In the eighth chapter, three cards are painting the roses on a rose tree red, because they had accidentally planted a white-rose tree that The Queen of Hearts hates. Red roses symbolised the English House of Lancaster, while white roses symbolised their rival House of York, thus the wars between them were the Wars of the Roses.[19]

While the book has remained in print and continually inspires new adaptations, the cultural material from which it draws has become largely specialized knowledge. Dr Leon Coward asserts the book 'suffers' from "readings which reflect today's fascination with postmodernism and psychology, rather than delving into an historically informed interpretation," and speculates that this has been partly driven by audiences encountering the narrative through a 'second-hand' source, explaining "our impressions of the original text are based on a multiplicity of reinterpretations. We don't necessarily realise we're missing anything in understanding the original product, because we're usually never dealing with the original product."[17]

Language
It has been suggested by several people, including Martin Gardner and Selwyn Goodacre,[20] that Dodgson had an interest in the French language, choosing to make references and puns about it in the story. It is most likely that these are references to French lessonsâa common feature of a Victorian middle-class girl's upbringing. For example, in the second chapter Alice posits that the mouse may be French. She therefore chooses to speak the first sentence of her French lesson-book to it: "OÃ¹ est ma chatte?" ("Where is my cat?"). In Henri BuÃ©'s French translation, Alice posits that the mouse may be Italian and speaks Italian to it.

Pat's "Digging for apples" could be a cross-language pun, as pomme de terre (literally; "apple of the earth") means potato and pomme means apple.[21]

In the second chapter, Alice initially addresses the mouse as "O Mouse", based on her memory of the noun declensions "in her brother's Latin Grammar, 'A mouse â of a mouse â to a mouse â a mouse â O mouse!'" These words correspond to the first five of Latin's six cases, in a traditional order established by medieval grammarians: mus (nominative), muris (genitive), muri (dative), murem (accusative), (O) mus (vocative). The sixth case, mure (ablative) is absent from Alice's recitation.

Mathematics
As Carroll was a mathematician at Christ Church, it has been suggested that there are many references and mathematical concepts in both this story and Through the Looking-Glass.[20][22] Literary scholar Melanie Bayley asserted in the magazine New Scientist that Dodgson wrote Alice in Wonderland in its final form as a scathing satire on new modern mathematics that were emerging in the mid-19th century.[23]

Examples of references to mathematics in Alice include:

Chapter 1 ("Down the Rabbit-Hole"): in the midst of shrinking, Alice waxes philosophic concerning what final size she will end up as, perhaps "going out altogether, like a candle"; this pondering reflects the concept of a limit.
Chapter 2 ("The Pool of Tears"): Alice tries to perform multiplication but produces some odd results: "Let me see: four times five is twelve, and four times six is thirteen, and four times seven isâoh dear! I shall never get to twenty at that rate!" This explores the representation of numbers using different bases and positional numeral systems: 4 Ã 5 = 12 in base 18 notation, 4 Ã 6 = 13 in base 21 notation, and 4 Ã 7 could be 14 in base 24 notation. Continuing this sequence, going up three bases each time, the result will continue to be less than 20 in the corresponding base notation. (After 4 Ã 12 = 19 in Base 39, the product would be 4 Ã 13 = 1A in Base 42, then 1B, 1C, 1D, and so on.)
Chapter 7 ("A Mad Tea-Party"): The March Hare, the Hatter, and the Dormouse give several examples in which the semantic value of a sentence A is not the same value of the converse of A (for example, "Why, you might just as well say that 'I see what I eat' is the same thing as 'I eat what I see'!"); in logic and mathematics, this is discussing a converse relation. Alice also ponders what it means when the changing of seats around the circular table places them back at the beginning. This is an observation of addition on the ring of integers modulo N.
The Cheshire cat fades until it disappears entirely, leaving only its wide grin, suspended in the air, leading Alice to marvel and note that she has seen a cat without a grin, but never a grin without a cat. Deep abstraction of concepts, such as non-Euclidean geometry, abstract algebra, and the beginnings of mathematical logic, was taking over mathematics at the time Dodgson was writing. Dodgson's delineation of the relationship between cat and grin can be taken to represent the very concept of mathematics and number itself. For example, instead of considering two or three apples, one may easily consider the concept of 'apple', upon which the concepts of 'two' and 'three' may seem to depend. A far more sophisticated jump is to consider the concepts of 'two' and 'three' by themselves, just like a grin, originally seemingly dependent on the cat, separated conceptually from its physical object.
Eating and devouring
Carina Garland notes how the world is "expressed via representations of food and appetite", naming Alice's frequent desire for consumption (of both food and words), her 'Curious Appetites'.[24] Often, the idea of eating coincides to make gruesome images. After the riddle "Why is a raven like a writing-desk?", the Hatter claims that Alice might as well say, "I see what I eatâ¦I eat what I see" and so the riddle's solution, put forward by Boe Birns, could be that "A raven eats worms; a writing desk is worm-eaten"; this idea of food encapsulates idea of life feeding on life itself, for the worm is being eaten and then becomes the eater  â a horrific image of mortality.[25]

Nina Auerbach discusses how the novel revolves around eating and drinking which "motivates much of her [Alice's] behaviour", for the story is essentially about things "entering and leaving her mouth".[26] The animals of Wonderland are of particular interest, for Alice's relation to them shifts constantly because, as Lovell-Smith states, Alice's changes in size continually reposition her in the food chain, serving as a way to make her acutely aware of the âeat or be eatenâ attitude that permeates Wonderland.[27]

Illustrations
Main article: Illustrators of Alice's Adventures in Wonderland
Gallery

Alice by John Tenniel, 1865
The manuscript was illustrated by Dodgson himself who added 37 illustrationsâprinted in a facsimile edition in 1887.[12]:117 John Tenniel provided 42 wood engraved illustrations for the published version of the book. The first print run was destroyed (or sold to the United States[28]) at Carroll's request because he was dissatisfied with the quality. The book was reprinted and published in 1866.[12]

John Tenniel's illustrations of Alice do not portray the real Alice Liddell, who had dark hair and a short fringe. The Guardian states, âJohn Tennielâs illustrations to this first edition remain indelibly Alice, with her apron and puffed sleeves and sweep of blond hair.â[29] Alice has provided a challenge for other illustrators, including those of 1907 by Charles Pears and the full series of colour plates and line-drawings by Harry Rountree published in the (inter-War) Children's Press (Glasgow) edition. Other significant illustrators include: Arthur Rackham (1907), Willy Pogany (1929), Mervyn Peake (1946), Ralph Steadman (1967), Salvador DalÃ­ (1969), Graham Overden (1969), Max Ernst (1970), Peter Blake (1970), Tove Jansson (1977), Anthony Browne (1988), Helen Oxenbury (1999), and Lisbeth Zwerger (1999).

Publication history

Title page of the original edition, Macmillan, London (1865)
On 26 November 1865, Dodgson's tale was published by Macmillan of London as Alice's Adventures in Wonderland under the pseudonym "Lewis Carroll" with illustrations by John Tenniel.[30] The first print run of 2,000 was held back because Tenniel objected to the print quality.[12] A new edition, released in December of the same year for the Christmas market, but carrying an 1866 date, was quickly printed.[30] The text blocks of the original edition were removed from the binding and sold with Dodgson's permission to the New York publishing house of D. Appleton & Company. The binding for the Appleton Alice was identical to the 1866 Macmillan Alice, except for the publisher's name at the foot of the spine. The title page of the Appleton Alice was an insert cancelling the original Macmillan title page of 1865, and bearing the New York publisher's imprint and the date 1866.[30]

The entire print run sold out quickly. Alice was a publishing sensation, beloved by children and adults alike.[30] Among its first avid readers were Queen Victoria and the young Oscar Wilde.[31][32][33] The book has never been out of print.[30] Alice's Adventures in Wonderland has been translated into at least 97 languages,[4] or as many as 174 languages.[34] There have now been over a hundred editions of the book, as well as countless adaptations in other media, especially theatre and film.

The book is commonly referred to by the abbreviated title Alice in Wonderland, which has been popularised by the numerous stage, film and television adaptations of the story produced over the years. Some printings of this title contain both Alice's Adventures in Wonderland and its sequel Through the Looking-Glass, and What Alice Found There.

Publication timeline
The following list is a timeline of major publication events related to Alice's Adventures in Wonderland:


1884 edition by Macmillan
1865: First U.K. edition (the second printing); first U.S. edition (the first printing of U.K. edition).[35]
1869: Published in German as Alice's Abenteuer im Wunderland, translated by Antonie Zimmermann.[36]
1869: Published in French as Aventures d'Alice au pays des merveilles, translated by Henri BuÃ©.[37]
1870: Published in Swedish as Alice's Ãventyr i Sagolandet, translated by Emily Nonnen.[38]
1871: Dodgson meets another Alice, Alice Raikes, during his time in London. He talks with her about her reflection in a mirror, leading to the sequel, Through the Looking-Glass, and What Alice Found There, which sells even better.
1872: Published in Italian as Le Avventure di Alice nel Paese delle Meraviglie, translated by Teodorico PietrocÃ²la Rossetti.[39]
1879: First Russian edition published as Ð¡Ð¾Ð½Ñ Ð² ÑÐ°ÑÑÑÐ²Ðµ ÐÐ¸Ð²Ð°, translated by Yury Nesterenko.[40]
1882: Selchow & Righter releases The Game of Alice in Wonderland, the first game based on the book.[41]
1886: Carroll publishes a facsimile of the earlier Alice's Adventures Under Ground manuscript.
1890: Carroll publishes The Nursery "Alice", a special edition "to be read by Children aged from Nought to Five".

Cover of the 1898 edition
1899: First Japanese edition of an Alice novel is published as é¡ä¸ç, a translation of Through the Looking-Glass rather than the first book.[42]
1905: Mrs J. C. Gorham publishes Alice's Adventures in Wonderland Retold in Words of One Syllable in a series of such books published by A. L. Burt Company, aimed at young readers. (ISBN 978-1-904808-44-2)
1906: Published in Finnish as Liisan seikkailut ihmemaailmassa, translated by Anni Swan.[42]
1907: Copyright on Alice's Adventures in Wonderland expires in UK, entering the tale into the public domain. At least 8 new editions would be published in this year alone.[43]
1910: Published in Esperanto as La Aventuroj de Alicio en Mirlando, translated by E. L. Kearney.[42]
1915: A dramatic screenplay script rendering of Alice in Wonderland by Alice Gerstenberg is published as Alice in Wonderland; a dramatization of Lewis Carrolls 'Alices adventures in Wonderland' and 'Through the looking glass.[44]
1916: First edition of the Windermere Series is published: Alice's Adventures in Wonderland, illustrated by Milo Winter.[42]
1928: The manuscript of Alice's Adventures Under Ground written and illustrated by Carroll, which he had given to Alice Liddell, was sold at Sotheby's on 3 April. It sold to Philip Rosenbach for Â£15,400, a world record for the sale of a manuscript at the time.[45]
1945: Animated picture book is published, with illustrations and paper engineering by Julian Wehr.[46]
1960: American writer Martin Gardner publishes a special edition, The Annotated Alice, incorporating the text of both Alice's Adventures in Wonderland and Through the Looking-Glass. It includes extensive annotations explaining the hidden allusions in the books, as well as full texts of the Victorian-era poems parodied in them. Later editions expand on these annotations.[47]
1961: The Folio Society edition is published, including 42 illustrations by John Tenniel.
1988: Lewis Carroll and Anthony Browne, illustrator of an edition from Julia MacRae Books, wins the Kurt Maschler Award (aka the Emil) for the year's best British "work of imagination for children, in which text and illustration are integrated so that each enhances and balances the other."[48]
1998: Carroll's own copy of Alice, one of only six surviving copies of the 1865 first edition, is sold at an auction for US$1.54 million to an anonymous American buyer, becoming the most expensive children's book (or 19th-century work of literature) ever sold to that point.[49]
1999: Lewis Carroll and Helen Oxenbury, illustrator of an edition from Walker Books, win the Kurt Maschler Award for integrated writing and illustration.[48]
2001: Harper Collins publishes an edition with illustrations by Deloss McGraw.[50]
2007: In celebration of the British Kate Greenaway Medal's 50th anniversary (1955â2005), the 1999 Walker Books edition illustrated by Helen Oxenbury is named as one of the top ten Medal-winning works, composing the ballot for a public election of the all-time favourite.[51]
2008: Folio publishes Alice's Adventures Under Ground facsimile edition (limited to 3,750 copies, boxed with The Original Alice pamphlet).
2009: Children's book collector and former American football player Pat McInally reportedly sold Alice Liddell's own copy at auction for US$115,000.[52]
Reception by reviewers

Painting of a mother reading Aliceâs Adventures in Wonderland to her daughter. George Dunlop Leslie, 1879
The book Alice in Wonderland failed to be named in an 1888 poll of the publishing season's most popular children's stories. Generally it received poor reviews, with reviewers giving more credit to Tenniel's illustrations than to Carroll's story. At the release of Through the Looking-Glass, the first Alice tale gained in popularity and by the end of the 19th century Sir Walter Besant wrote that Alice in Wonderland "was a book of that extremely rare kind which will belong to all the generations to come until the language becomes obsolete".[53]

In 2014, Robert McCrum named the tale "one of the best loved in the English canon", and called it "perhaps the greatest, possibly most influential, and certainly the most world-famous Victorian English fiction".[30]

Adaptations and influence
Main article: Works based on Alice in Wonderland

An enormously popular figure in pop culture, a Halloween costume of Alice (and the Queen of Hearts) during a parade in Vancouver, Canada
In 2015, Robert Douglas-Fairhurst in The Guardian wrote, âSince the first publication of Aliceâs Adventures in Wonderland 150 years ago, Lewis Carrollâs work has spawned a whole industry, from films and theme park rides to products such as a 'cute and sassy' Alice costume ('petticoat and stockings not included'). The blank-faced little girl made famous by John Tenniel's original illustrations has become a cultural inkblot we can interpret in any way we like."[5]

Alice and the rest of Wonderland continue to inspire or influence many other works of art to this day, sometimes indirectly via the 1951 Disney movie, for example. References, homages, reworkings and derivative works can be found in many works of literature, film, theatre, visual art, music, and games (such as playing cards).[54] Labelled âa dauntless, no-nonsense heroineâ by The Guardian, the character of the plucky, yet proper, Alice has proven immensely popular and inspired similar heroines in literature and pop culture, many also named Alice in homage.[29]

Cinema and television
Main article: Films and television programmes based on Alice in Wonderland
The book has inspired numerous film and television adaptations which have multiplied as the original work is now in the public domain in all jurisdictions. The following list is of direct adaptations of Adventures in Wonderland (sometimes merging it with Through the Looking-Glass), not other sequels or works otherwise inspired by the works (such as Tim Burton's 2010 film Alice in Wonderland):


Screenshot from the first film version of Alice in Wonderland in 1903. The original copy is held by the British Film Institute.
Alice in Wonderland (1903), a British silent film directed by Cecil Hepworth and Percy Stow, with May Clark as Alice.[55]
Alice's Adventures in Wonderland (1910), a silent film directed by Edwin Stanton Porter
Alice in Wonderland (1915), a silent film directed by W. W. Young
Alice in Wonderland (1931), the first talkie adaptation, directed by Bud Pollard
Alice in Wonderland (1933), a film version directed by Norman Z. McLeod, US
Alice in Wonderland (1937), a TV adaptation directed by George More O'Ferrall
Alice in Wonderland (1937) TV adaptation again directed by George More O'ferrall with Usula Henray as Alice.[56]
Alice in Wonderland (1944) TV adaptation of Eva La Gaillenne's stage version of both books, US.[57]
Alice (1946), a BBC production starring Vivian Pickles directed by George More O'Ferrall, UK
Alice in Wonderland and Through the Looking-Glass (1948) BBC TV broadcast.[58]

Olivia de Havilland as Alice for the 1933 stage play.
Alice in Wonderland (1949), a live-action/animated film with stop motion segments, directed by Dallas Bower
Through the Crystal Ball: Alice in Wonderland (1949) US TV performance.[59]
Alice in Wonderland (1950), televised on the CBS Ford Theatre, with Iris Mann as Alice, directed by Franklin J. Schaffner
Alice in Wonderland (1951), a film version in traditional animation from Walt Disney Feature Animation. Arguably the most well known of the Alice film adaptations, and today considered one of Disney's great classics.[60]
Alice au pays des Merveilles (1951) France TV broadcast of a stage version
Alice in Wonderland (1954) BBC broadcast of a ballet version.[61]
Alice in Wonderland (1955), a live television adaptation of the 1932 Eva LeGallienne /Florida Friebus stage adaptation of the novel, directed for television by George Schaefer for the Hallmark Hall of Fame
The Adventures of Alice (1960), a televised opera.[62]
The BP Super Show: Alice in Wonderland (1962) Australian TV musical special.[63]
Alice in Wonderland (1965), a TV movie directed by Dennis Potter
Alice in Wonderland (1966), an animated Hanna-Barbera TV movie with Janet Waldo as Alice
Alice in Wonderland (1966), a BBC television play directed by Jonathan Miller
Alice au pays des merveilles (1972), a version made for television, by Jean-Christophe Averty.
Alice's Adventures in Wonderland (1972), a musical film version starring Fiona Fullerton as Alice
Alice in Wonderland (sometimes listed as Alice in Wonderland: An X-Rated Musical Comedy) (1976), an American erotic musical comedy film, starring Kristine DeBell
Nel Mondo Di Alice (In the World of Alice) Italian TV series in 4 parts.[64]
Alice in Wonderland (1983), a PBS Great Performances presentation of a 1982 stage play which was in turn a revival of the 1932 LeGallienne production
Alice in Wonderland (1985), a two-part made-for-TV special produced by Irwin Allen and featuring a large all-star cast
Alice in a Winter Wonderland (1985): a BBC One Christmas Special parody, performed by sketch-comedy duo The Two Ronnies, featuring Ronnie Barker and Ronnie Corbett.[65][66]
Alice in Wonderland (1986), a BBC adaptation directed by Barry Letts and starring Kate Dorning
Alice (1988 film) by Jan Å vankmajer, stop motion and live action
Alice in Wonderland (1999), a 1999 television movie first shown on NBC and then shown on British television on Channel 4
Comic strips and books

The Westminster Alice (1902), published in London by the Westminster Gazette
Alice in Wonderland (1934â1935) was a comic strip adaptation drawn by Edward D. Kuekes and written by Olive Ray Scott. This version also featured a "topper" strip, Knurl the Gnome. The strip was distributed by United Feature Syndicate.[67]

Literary and comic-book adaptations include:

The Westminster Alice (1902): a political parody by Hector Hugh Munro (Saki), illustrated by Francis Carruthers Gould.[68]
Walt Disney's Alice in Wonderland (1951, Dell Comics).[69]
Walt Disney's Alice in Wonderland (1965, Gold Key Comics)
Walt Disney's Alice in Wonderland (Whitman, 1984)
"The Complete Alice in Wonderland" (2005, Dynamite Entertainment).[70]
Return to Wonderland (2009, Zenescope Entertainment).[71]
Alice in Wonderland (2011, Zenescope Entertainment)
Alice in Weirdworld (2020, Flying Buffalo Incorporated)
Live performance

1898 revival of the musical Alice in Wonderland. Popular among London theatregoers, the play was frequently revived during Christmas season over the next four decades.
The first full major production of 'Alice' books during Carroll's lifetime was Alice in Wonderland, an 1886 musical play in London's West End by H. Saville Clark (book) and Walter Slaughter (music), which played at the Prince of Wales Theatre. Carroll attended a performance on 30 December 1886, writing in his diary he enjoyed it.[72] The musical was frequently revived during West End Christmas seasons during the four decades after its premiere, including a London production at the Globe Theatre in 1888, with Isa Bowman as Alice.[73]


Maidie Andrews as Alice in Alice Through the Looking-Glass at the Comedy Theatre, London during the Christmas period 1903â04. Pictured in The Tatler (January 1904)
As the book and its sequel are Carroll's most widely recognised works, they have also inspired numerous live performances, including plays, operas, ballets, and traditional English pantomimes. These works range from fairly faithful adaptations to those that use the story as a basis for new works. An example of the latter is The Eighth Square, a murder mystery set in Wonderland, written by Matthew Fleming and music and lyrics by Ben J. Macpherson. This goth-toned rock musical premiered in 2006 at the New Theatre Royal in Portsmouth, England.

Over the years, many notable people in the performing arts have been involved in Alice productions. Actress Eva Le Gallienne famously adapted both Alice books for the stage in 1932; this production has been revived in New York in 1947 and 1982. One of the most well-known American productions was Joseph Papp's 1980 staging of Alice in Concert at the Public Theater in New York City. Elizabeth Swados wrote the book, lyrics, and music. Based on both Alice's Adventures in Wonderland and Through the Looking-Glass, Papp and Swados had previously produced a version of it at the New York Shakespeare Festival. The actress Meryl Streep played Alice, the White Queen, and Humpty Dumpty.[74] The cast also included Debbie Allen, Michael Jeter, and Mark Linn-Baker. Performed on a bare stage with the actors in modern dress, the play is a loose adaptation, with song styles ranging the globe. A community theatre production of Alice was Olivia de Havilland's first foray onto the stage.


Production of Alice in Wonderland by the Kansas City Ballet in 2013
Similarly, the 1992 operatic production Alice used both Alice books as its inspiration. It also employs scenes with Charles Dodgson, a young Alice Liddell, and an adult Alice Liddell, to frame the story. Paul Schmidt wrote the play, with Tom Waits and Kathleen Brennan writing the music. Although the original production in Hamburg, Germany, received only a small audience, Tom Waits released the songs as the album Alice in 2002. A musical adaption was written by Michael Sirotta and Heather M. Dominick in 1997, titled Alice in Wonderland, a Musical Adventure.[75][76]

The English composer Joseph Horovitz composed an Alice in Wonderland ballet commissioned by the London Festival Ballet in 1953. It was performed frequently in England and the US.[77] A ballet by Christopher Wheeldon and Nicholas Wright commissioned for The Royal Ballet entitled Alice's Adventures in Wonderland premiered in February 2011 at the Royal Opera House in London.[78][79] The ballet was based on the novel Wheeldon grew up reading as a child and is generally faithful to the original story, although some critics claimed it may have been too faithful.[80] The ballet overall stays generally light hearted for its running time of an hour and forty minutes. The ballet returned to the Royal Opera House in 2012.[81]

Gerald Barry's 2016 one-act opera, Alice's Adventures Under Ground, first staged in 2020 at the Royal Opera House, is a conflation of the two Alice books.[82]

Gallery

The cover illustration of The Nursery "Alice", by E. Gertrude Thomson

 

The White Rabbit by John Tenniel, coloured

 

Mad tea party, John Tenniel, 1865

 

Statue of Alice in Rymill Park, Adelaide, South Australia

 

Alice in Wonderland sculpture in Central Park, New York

 

Lewis Carroll memorial window at All Saints' Church, Daresbury, Cheshire

See also
Through the Looking Glass
Translations of Alice's Adventures in Wonderland
Translations of Through the Looking-Glass
References
 BBC's Greatest English Books list
 Lecercle, Jean-Jacques (1994) Philosophy of nonsense: the intuitions of Victorian nonsense literature Routledge, New York. ISBN 978-0-415-07652-4. p. 1 ff
 Schwab, Gabriele (1996) "Chapter 2: Nonsense and Metacommunication: Alice in Wonderland" in The mirror and the killer-queen: otherness in literary language Indiana University Press, Bloomington, Indiana. ISBN 978-0-253-33037-6. pp. 49â102
 Bandersnatch: The Newsletter of The Lewis Carroll Society, Issue 149 (January 2011). p. 11.
 "Alice in Wonderland: the never-ending adventures". The Guardian. Retrieved 5 November 2019.
 "The real Alice". Story Museum. Archived from the original on 17 November 2010. Retrieved 24 April 2010.
 Lewis Carroll, "Alice on the Stage, The Theatre, April 1887
 Astronomical and Meteorological Observations Made at the Radcliffe Observatory, Oxford, Vol. 23
 The Background & History of Alice In Wonderland. Bedtime-Story Classics. Retrieved 29 January 2007.
 "Meet the Girl Who Inspired 'Alice in Wonderland'". The Atlantic. Retrieved 14 April 2020.
 Carpenter, Humphrey (1985). Secret Gardens: The Golden Age of Children's Literature. Houghton Mifflin. p. 57. ISBN 978-0-395-35293-9.
 Ray, Gordon Norton (1991). The Illustrator and the book in England from 1790 to 1914. New York: Dover. p. 117. ISBN 978-0-486-26955-9.
 Gardner, Martin (2000). The Annotated Alice: The Definitive Edition. New York: W. W. Norton & Company. ISBN 978-0-393-04847-6.
 Everson, Michael (2009) "Foreword", in Carroll, Lewis (2009). Alice's Adventures under Ground. Evertype. ISBN 978-1-904808-39-8.
 Brooker, Will (2004). Alice's Adventures: Lewis Carroll in Popular Culture. New York: Continuum. pp. 69â70. ISBN 978-0-8264-1433-5.
 The diary of Lewis Carroll, 1 August 1862 entry
 Coward, Leon (November 2015). "Alice in Wonderland: 150 years". The Greek Australian Vema. Sydney, Australia: St Andrews Orthodox Press. p. 18.
 "Ripon Tourist Information". Hello-Yorkshire.co.uk. Archived from the original on 26 November 2009. Retrieved 1 December 2009.
 "Alice's Adventures In Wonderland Turns 150". Marie Claire. Retrieved 14 April 2020.
 Gardner, Martin (1990). More Annotated Alice. New York: Random House. p. 363. ISBN 978-0-394-58571-0.
 Lewis Carroll (2009). Alice's Adventures in Wonderland and Through the Looking-Glass. Oxford University Press. ISBN 978-0-19-955829-2.
 Bayley, Melanie (6 March 2010). "Algebra in Wonderland". The New York Times. Archived from the original on 12 March 2010. Retrieved 13 March 2010.
 Bayley, Melanie. "Alice's adventures in algebra: Wonderland solved". New Scientist. Retrieved 21 August 2012.
 Garland, C. (2008). "Curious Appetites: Food, Desire, Gender and Subjectivity in Lewis Carroll's Alice Texts". The Lion and the Unicorn. 32: 22â39. doi:10.1353/uni.2008.0004.
 Boe Birns, Margaret (1984). "Solving the Mad Hatter's Riddle". The Massachusetts Review. 25 (3): 457â468 (462). JSTOR 25089579.
 Auerbach, Nina (1973). "Alice and Wonderland: A Curious Child". Victorian Studies. 17 (1): 31â47 (39). JSTOR 3826513.
 Lovell-Smith, R. (2004). "The Animals of Wonderland: Tenniel as Carroll's Reader". Criticism. 45 (4): 383â415. doi:10.1353/crt.2004.0020.
 Ovenden, Graham (1972). The Illustrators of Alice. New York: St. Martin's Press. p. 102. ISBN 978-0-902620-25-4.
 "The Guardian view on Alice in Wonderland: a dauntless, no-nonsense heroine". The Guardian. Retrieved 5 November 2019.
 "The 100 best novels: No 18 â Alice's Adventures in Wonderland by Lewis Carroll (1865)". The Guardian. Retrieved 15 April 2020.
 "10 things you didn't know about Alice in Wonderland". The Guardian. Retrieved 29 October 2019.
 Tucker, Rebecca (25 February 2011). "Oscar Reads: Alice's Adventures in Wonderland, by Lewis Carroll". Retrieved 15 April 2017.
 "Alice in Wonderland". QI. Quite Interesting Ltd. Archived from the original on 22 April 2017. Retrieved 15 April 2017.
 "Alice in a World of Wonderlands â The Books". aliceinaworldofwonderlands.com.
 Carroll, Lewis (1995). The Complete, Fully Illustrated Works. New York: Gramercy Books. ISBN 978-0-517-10027-1.
 "Lewis Carroll's Alice's Adventures in Wonderland â in German". Evertype.com. Retrieved 28 October 2019
 "Lewis Carroll's Alice's Adventures in Wonderland â in French". Evertype.com. Retrieved 28 October 2019
 "Lewis Carroll's Alice's Adventures in Wonderland â in Swedish". Evertype.com. Retrieved 28 October 2019
 "Lewis Carroll's Alice's Adventures in Wonderland â in Italian". Evertype.com. Retrieved 28 October 2019
 "Lewis Carroll's Alice's Adventures in Wonderland â in Russian". Evertype.com. Retrieved 28 October 2019
 "Pastimes: Children's Games and Their Literary Inspirations". Indiana University. Retrieved 9 October 2012.
 âWonderland and Carrollianaâ. Evertype.com. Retrieved 28 October 2019
 Page 11 of Introduction, by John Davies, of Ovenden, Graham (1972). The Illustrators of Alice. New York: St. Martin's Press. p. 102. ISBN 978-0-902620-25-4.
 Gerstenberg, Alice (1915). Alice in Wonderland; a dramatization of Lewis Carrolls "Alices adventures in Wonderland" and "Through the looking glass," Chicago: A. C. McClurg & Co. Produced by the Players Producing Company of Chicago, at the Fine Arts Theater, Chicago (2/11/1915) and Booth Theater, New York (3/23/1915).
 Basbanes, Nicholas (1999). A Gentle Madness: Bibliophiles, Bibliomanes, and the Eternal Passion for Books. Macmillan. ISBN 978-0-8050-6176-5.
 Wehr, Julian; Carroll, Lewis. The animated picture book of Alice in Wonderland. OCLC 11319992.
 Self, Will (25 December 2000). "Dirty Old Man" on The New Statesman. Review.
 "Kurt Maschler Awards". Book Awards. bizland.com. Retrieved 7 October 2013.
 "Auction Record for an Original 'Alice'". The New York Times. 11 December 1998. p. B30.
 âAlice's Adventures in Wonderlandâ. Publishers Weekly. Retrieved 28 October 2019
 "70 Years Celebration: Anniversary Top Tens" Archived 27 October 2016 at the Wayback Machine. The CILIP Carnegie & Kate Greenaway Children's Book Awards. CILIP. Retrieved 7 October 2013.
 "Real Alice in Wonderland book sells for $115,000 in USA". BBC News. 17 December 2009. Retrieved 8 January 2012.
 Carpenter, Humphrey (1985). Secret Gardens: The Golden Age of Children's Literature. Houghton Mifflin. p. 68. ISBN 978-0-395-35293-9.
 "Novels About Playing Cards: Alice in Wonderland". 6 May 2020. Retrieved 7 May 2020.
 Mills, Ted (31 March 2016). "The First Film Adaptation of Alice in Wonderland (1903)". Open Culture. Retrieved 19 May 2017.
 "Phantomwise". Phantomwise. Retrieved 23 April 2016.
 "Phantomwise". Phantomwise. Retrieved 23 April 2016.
 "Alice's Adventures in Wonderland and Through the Looking-Glass". British Universities Film & Video Council. Retrieved 6 July 2017.
 "Phantomwise". Phantomwise. Retrieved 23 April 2016.
 Louis Peitzman, "17 Adaptations Of "Alice's Adventures In Wonderland" Through The Years", BuzzFeed (11 October 2013), describing the 1951 Disney film as "undoubtedly the most well known".
 "All-in-the-golden-afternoon96, still-she-haunts-me-phantomwise: London's..." All-in-the-golden-afternoon96. Retrieved 23 April 2016.
 "Phantomwise". Phantomwise. Retrieved 23 April 2016.
 "Phantomwise". Phantomwise. Retrieved 23 April 2016.
 fictionrare2 (29 September 2014), Nel mondo di Alice 1p, retrieved 23 April 2016
 "Episode #12.1" â via www.imdb.com.
 "Watch two classic Two Ronnies Christmas songs". Smooth Radio.
 "Ed Kuekes". Lambiek Comiclopedia. Lambiek. 11 April 2008. Retrieved 9 August 2017.
 Cox, Michael (2005). The Concise Oxford Chronology of English Literature. Oxford University Press. p. 428.
 Holtz, Allan (2012). American Newspaper Comics: An Encyclopedic Reference Guide. Ann Arbor: The University of Michigan Press. p. 49. ISBN 9780472117567.
 "Join Dynamite Entertainment For "The Complete Alice In Wonderland"". CBR. Retrieved 4 July 2020.
 Raven Gregory on Return to Wonderland Archived 2009-02-08 at the Wayback Machine, Newsarama, 4 July 2020
 Carroll, Lewis (1979). The Letters of Lewis Carroll, Volumes 1-2. Oxford University Press. p. 657. Dec. 30th.âTo London with Mâ, and took her to âAlice in Wonderland,â Mr. Savile Clarke's play at the Prince of Wales's Theatre... as a whole, the play seems a success.
 Moses, Belle (2009). Lewis Carroll in Wonderland and at Home: The Story of His Life. BiblioBazaar. pp. 244â247. ISBN 978-1-103-29348-3.
 "'Alice' Through the Years: 16 Actresses Who Played the Iconic Character". Hollywood Reporter. Retrieved 15 April 2020.
 "Winter Children's Theatre "Alice in Wonderland" Runs Through March 24th". North Coast Oregon. Retrieved 9 January 2015.
 "First Redeemer conservatory of music and fine arts presents Alice in Wonderland". APPEN MEDIA GROUP. Retrieved 9 January 2015.
 "Horovitz Alice in Wonderland (excs)". Gramophone.co.uk. Retrieved 18 May 2020.
 "Royal Ballet Takes a Chance on Alice". Time. 3 March 2011.
 "Joby Talbot â Alice's Adventures in Wonderland (2010) â Music Sales Classical". Chesternovello.com. Retrieved 4 August 2013.
 Sulcas, Roslyn (1 March 2011). "'Alice in Wonderland' at the Royal Ballet â Review". The New York Times.
 "Calendar < Events â Royal Opera House < August 2013". Roh.org.uk. Retrieved 4 August 2013.
 "Alice's Adventures Under Ground". Royal Opera House. Retrieved 6 February 2020.
Citations

Carpenter, Humphrey (1985). Secret Gardens: The Golden Age of Children's Literature. Houghton Mifflin. ISBN 978-0-395-35293-9.
Gardner, Martin (2000). The Annotated Alice: the definitive edition. New York and London: W. W. Norton & Company. ISBN 978-0-393-04847-6.
Ray, Gordon Norton (1991). The Illustrator and the book in England from 1790 to 1914. New York: Dover. ISBN 978-0-486-26955-9.
External links
Alice's Adventures in Wonderland
at Wikipedia's sister projects
Definitions from Wiktionary
Media from Wikimedia Commons
Quotations from Wikiquote
Texts from Wikisource
Textbooks from Wikibooks
	Children's literature portal
icon	Novels portal
Project Gutenberg:
Alice's Adventures in Wonderland, plain text
Alice's Adventures Under Ground, HTML with facsimiles of original manuscript pages, and illustrations by Carroll
LibriVox:
 Alice's Adventures in Wonderland public domain audiobook at LibriVox
 Alice's Adventures Underground public domain audiobook at LibriVox
GASL.org: First editions of Alice's Adventures in Wonderland and Through the Looking-Glass, and What Alice Found There With 92 Illustrations by Tenniel, 1866/1872.
Images of the 1st editions of the book and other works by Lewis Carroll: https://sites.google.com/site/lewiscarroll1steditions/
British Library: Original manuscript and drawings by Lewis Carroll (requires Flash)
Afterlife of Alice and Her Adventures in Wonderland is a collection of various editions of Alice's Adventures in Wonderland, Through the Looking-Glass, What Alice Found There, and other similar works.
vte
Lewis Carroll's Alice
vte
Fantasy fiction
Authority control Edit this at Wikidata	
BNC: 000110019BNE: XX3383585BNF: cb12011248f (data)GND: 4295896-9LCCN: n79080561MusicBrainz work: 15d6d96e-f878-43ce-adb6-79f7dbff8c2bNKC: aun2017953712NLA: 35039339NLI: 001786369NSK: 000259321SUDOC: 028235045VIAF: 180646266WorldCat Identities (via VIAF): 180646266
Categories: 1865 British novelsAlice's Adventures in WonderlandChildren's fantasy novelsBritish children's novelsEnglish novelsFictional subterraneaFiction about size changeWorks by Lewis CarrollVictorian novels1865 fantasy novelsMacmillan Publishers booksAnimals in mediaHigh fantasy novelsSurreal comedyCultural depictions of Benjamin DisraeliD. Appleton & Company booksBritish novels adapted into filmsBritish novels adapted into playsBooks illustrated by John TennielNovels adapted into video gamesBritish children's booksNovels set in fictional countriesFictional fungiBooks illustrated by Arthur Rackham
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons
Wikiquote
Wikisource

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Latina
à¤®à¤°à¤¾à¤ à¥
Tiáº¿ng Viá»t
ä¸­æ
78 more
Edit links
This page was last edited on 18 October 2020, at 17:28 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Autograder - Engineering Networks Fall 2020 - Reliable UDP Part 2
Hi, Daksha Maruti!
Submit
My Submissions
All Submissions
Nov 04, '20, 07:19 PM
45/50
Nov 04, '20, 07:07 PM
38/50
Nov 04, '20, 07:04 PM
34/50
Nov 04, '20, 06:53 PM
45/50
Nov 04, '20, 04:24 PM
10/50
Nov 04, '20, 04:22 PM
5/50
Nov 04, '20, 04:09 PM
10/50
Nov 04, '20, 04:00 PM
10/50
Nov 04, '20, 03:58 PM
9/50
Nov 04, '20, 03:38 PM
9/50
Nov 04, '20, 03:10 PM
6/50
Nov 04, '20, 02:32 PM
5/50
Nov 04, '20, 02:13 PM
6/50
Nov 04, '20, 01:44 PM
5/50
Nov 04, '20, 01:41 PM
6/50
Nov 04, '20, 01:39 PM
5/50
Nov 04, '20, 12:56 PM
10/50
Nov 04, '20, 12:50 PM
10/50
Nov 04, '20, 12:45 PM
9/50
Nov 04, '20, 01:28 AM
9/50
Nov 04, '20, 01:23 AM
10/50
Nov 04, '20, 01:15 AM
9/50
Nov 04, '20, 12:58 AM
9/50
Nov 04, '20, 12:07 AM
4/50
Nov 03, '20, 09:21 PM
10/50
Nov 03, '20, 09:02 PM
9/50
Nov 03, '20, 08:58 PM
5/50
Nov 03, '20, 08:49 PM
9/50
Nov 03, '20, 08:44 PM
10/50
Nov 03, '20, 08:29 PM
0/50
Oct 30, '20, 02:51 PM
0/50
Submitted by: dnagre@iu.edu on November 04, 2020, 07:19 PM
Score: 45/50
url.txt
bandwidth of network
Test Case
Passed
Score
Setup
iperf3 worst case
file transfer
Test Case
Passed
Score
checkout git repo
setup environment
ideal network (-r 2) (5kB/2s)
5/5
larger file, ideal network (100kB/2s)
5/5
lossless network with latency (50kB/2s)
10/10
lossy network with latency (50kB/1s)
10/10
larger file, lossy network with latency (100kB/2s)
10/10
potato network (duplicate test 1 for partial credit) (raw debug output) (10kB/1s)
1/1
potato network (duplicate test 2 for partial credit) (10kB/1s)
1/1
potato network (duplicate test 3 for partial credit) (10kB/1s)
1/1
potato network (duplicate test 4 for partial credit) (10kB/1s)
1/1
potato network (duplicate test 5 for partial credit) (10kB/1s)
1/1
larger file, potato network (duplicate test 1 for partial credit) (raw debug output) (1MB/6s)
0/1
send file
test file
larger file, potato network (duplicate test 2 for partial credit) (1MB/6s)
0/1
send file
test file
larger file, potato network (duplicate test 3 for partial credit) (1MB/6s)
0/1
send file
test file
Correctness
Actual exit status:0
Output:
Output Diffs
Expected Output
Student Output

Flow control (data)
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Not to be confused with Control flow.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.

Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.


Contents
1	Stop-and-wait
1.1	Operations
1.2	Pros and cons of stop and wait
2	Sliding Window
2.1	Go Back N
2.2	Selective Repeat
3	Comparison
3.1	Stop-and-Wait
3.2	Selective Repeat
4	Transmit flow control
4.1	Hardware flow control
4.2	Software flow control
5	Open-loop flow control
6	Closed-loop flow control
7	See also
8	References
9	External links
Stop-and-wait
Main article: Stop-and-wait ARQ
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.

Operations
Sender: Transmits a single frame at a time.
Sender waits to receive ACK within time out.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Go to step 1 when ACK is received, or time out is hit.
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).

The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.

Pros and cons of stop and wait
Pros

The only advantage of this method of flow control is its simplicity.

Cons

The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]

Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]

Sliding Window
Main article: Sliding Window Protocol
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]

Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.

Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.

Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.

Go Back N
Main article: Go-Back-N ARQ
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.

The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.

To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]

Selective Repeat
Main article: Selective Repeat ARQ
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]

Comparison
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.

Stop-and-Wait
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]

With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]

Selective Repeat
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]

Transmit flow control
Transmit flow control may occur:

between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be

either stop-and-wait,
or use a sliding window.
Flow control can be performed

either by control signal lines in a data communication interface (see serial port and RS-232),
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
Hardware flow control
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:

RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:

In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.

Polarity:
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
Software flow control
Main article: Software flow control
Conversely, XON/XOFF is usually referred to as software flow control.

Open-loop flow control
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.

Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]

Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.

Closed-loop flow control
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.

This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:

A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.

In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.

The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.

See also
Software flow control
Computer networking
Traffic contract
Congestion control
Teletraffic engineering in broadband networks
Teletraffic engineering
Ethernet flow control
Handshaking
References
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 Webster Dictionary definition last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
Sliding window:

[1] last accessed 27 November 2012.
External links
RS-232 flow control and handshaking
Categories: Flow control (data)Network performanceLogical link controlData transmission
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
Deutsch
ÙØ§Ø±Ø³Û
FranÃ§ais
íêµ­ì´
Italiano
æ¥æ¬èª
Ð ÑÑÑÐºÐ¸Ð¹
4 more
Edit links
This page was last edited on 18 July 2020, at 11:55 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Data transmission
From Wikipedia, the free encyclopedia
  (Redirected from Data communications)
Jump to navigationJump to search
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.

Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.

Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.


Contents
1	Distinction between related subjects
2	Protocol layers and sub-topics
3	Applications and history
4	Serial and parallel transmission
5	Communication channels
6	Asynchronous and synchronous data transmission
7	See also
8	References
Distinction between related subjects
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.

Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.

The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]

The theoretical aspects of data transmission are covered by information theory and coding theory.

Protocol layers and sub-topics
OSI model
by layer
7.  Application layer[show]
6.  Presentation layer[show]
5.  Session layer[show]
4.  Transport layer[show]
3.  Network layer[show]
2.  Data link layer[show]
1.  Physical layer[show]
vte
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:

Layer 1, the physical layer:
Channel coding including
Digital modulation schemes
Line coding schemes
Forward error correction (FEC) codes
Bit synchronization
Multiplexing
Equalization
Channel models
Layer 2, the data link layer:
Channel access schemes, media access control (MAC)
Packet mode communication and Frame synchronization
Error detection and automatic repeat request (ARQ)
Flow control
Layer 6, the presentation layer:
Source coding (digitization and data compression), and information theory.
Cryptography (may occur at any layer)
It is also common to deal with the cross-layer design of those three layers.[6]

Applications and history
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.

Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.

Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.

In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.

Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.

Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.

The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.

Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.

While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.

Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.

Serial and parallel transmission
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.

In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.

Communication channels
Main article: communication channel
Some communications channel types include:

Data transmission circuit
Full-duplex
Half-duplex
Multi-drop:
Bus network
Mesh network
Ring network
Star network
Wireless network
Point-to-point
Simplex
Asynchronous and synchronous data transmission
Main article: Comparison of synchronous and asynchronous signalling
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.

Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.

See also
Computer networking
Communication
Data migration
Information theory
Media (communication)
Network security
Node-to-node data transfer
Packet switching
Signal processing
Telecommunication
Transmission (disambiguation)
References
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
28 more
Edit links
This page was last edited on 30 October 2020, at 04:19 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Modulation
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For other uses, see Modulation (disambiguation).

This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
Passband modulation
Analog modulation
AMFMPMQAMSMSSB
Digital modulation
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
Hierarchical modulation
QAMWDM
Spread spectrum
CSSDSSSFHSSTHSS
See also
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
vte
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.

Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.

In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.

In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]

A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.

The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).

The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.

The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.


Contents
1	Analog modulation methods
2	Digital modulation methods
2.1	Fundamental digital modulation methods
2.2	Modulator and detector principles of operation
2.3	List of common digital modulation techniques
2.4	Automatic digital modulation recognition (ADMR)
2.5	Digital baseband modulation or line coding
3	Pulse modulation methods
4	Miscellaneous modulation techniques
5	See also
6	References
7	Further reading
8	External links
Analog modulation methods

A low-frequency message signal (top) may be carried by an AM or FM radio wave.

Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.

A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:

Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Double-sideband modulation (DSB)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband reduced carrier transmission (DSB-RC)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Vestigial sideband modulation (VSB, or VSB-AM)
Quadrature amplitude modulation (QAM)
Angle modulation, which is approximately constant envelope
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Digital modulation methods
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).


Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.

According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.

Fundamental digital modulation methods
The most fundamental digital modulation techniques are based on keying:

PSK (phase-shift keying): a finite number of phases are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.

In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.

If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.

For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.

In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.

Modulator and detector principles of operation
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).

These are the general steps used by the modulator to transmit data:

Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
At the receiver side, the demodulator typically performs:

Bandpass filtering.
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Parallel-to-serial conversion of the codewords into a bit stream.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.

Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.

List of common digital modulation techniques
The most common digital modulation techniques are:

Phase-shift keying (PSK)
Binary PSK (BPSK), using M=2 symbols
Quadrature PSK (QPSK), using M=4 symbols
8PSK, using M=8 symbols
16PSK, using M=16 symbols
Differential PSK (DPSK)
Differential QPSK (DQPSK)
Offset QPSK (OQPSK)
Ï/4âQPSK
Frequency-shift keying (FSK)
Audio frequency-shift keying (AFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Dual-tone multi-frequency (DTMF)
Amplitude-shift keying (ASK)
On-off keying (OOK), the most common ASK form
M-ary vestigial sideband modulation, for example 8VSB
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Continuous phase modulation (CPM) methods
Minimum-shift keying (MSK)
Gaussian minimum-shift keying (GMSK)
Continuous-phase frequency-shift keying (CPFSK)
Orthogonal frequency-division multiplexing (OFDM) modulation
Discrete multitone (DMT), including adaptive modulation and bit-loading
Wavelet modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Spread-spectrum techniques
Direct-sequence spread spectrum (DSSS)
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).

OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.

Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.

Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]

There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.

Digital baseband modulation or line coding
Main article: Line code
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]

vte
Line coding (digital baseband transmission)
Pulse modulation methods
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.

Analog-over-analog methods

Pulse-amplitude modulation (PAM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-position modulation (PPM)
Analog-over-digital methods

Pulse-code modulation (PCM)
Differential PCM (DPCM)
Adaptive DPCM (ADPCM)
Delta modulation (DM or Î-modulation)
Delta-sigma modulation (âÎ)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Pulse-density modulation (PDM)
Miscellaneous modulation techniques
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
Adaptive modulation
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
See also
	Wikimedia Commons has media related to Modulation.
Channel access methods
Channel coding
Codec
Communications channel
Demodulation
Electrical resonance
Heterodyne
Line code
Mechanically induced modulation
Modem
Modulation order
Neuromodulation
RF modulator
Ring modulation
Telecommunication
Types of radio emissions
References

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
Further reading
Multipliers vs. Modulators Analog Dialogue, June 2013
External links
Interactive presentation of soft-demapping for AWGN-channel in a web-demo Institute of Telecommunications, University of Stuttgart
Modem (Modulation and Demodulation)
vte
Telecommunications
vte
Analog and digital audio broadcasting
Categories: Frequency mixersHistory of radioHistory of televisionPhysical layer protocolsRadio modulation modesTelecommunication theoryTelevision terminology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
52 more
Edit links
This page was last edited on 17 October 2020, at 19:13 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Radio broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from Radio broadcast)
Jump to navigationJump to search

Long wave radio broadcasting station, Motala, Sweden

Slovak Radio Building, Bratislava, Slovakia (architects: Å tefan Svetko, Å tefan ÄurkoviÄ and BarnabÃ¡Å¡ Kissling, 1967â1983)

Broadcasting tower in Trondheim, Norway
Radio broadcasting is transmission of audio (sound), sometimes with related metadata, by radio waves intended to reach a wide audience. In terrestrial radio broadcasting the radio waves are broadcast by a land-based radio station, while in satellite radio the radio waves are broadcast by a satellite in Earth orbit. To receive the content the listener must have a broadcast radio receiver (radio). Stations are often affiliated with a radio network which provides content in a common radio format, either in broadcast syndication or simulcast or both. Radio stations broadcast with several different types of modulation: AM radio stations transmit in AM (amplitude modulation), FM radio stations transmit in FM (frequency modulation), which are older analog audio standards, while newer digital radio stations transmit in several digital audio standards: DAB (digital audio broadcasting), HD radio, DRM (Digital Radio Mondiale). Television broadcasting is a separate service which also uses radio frequencies to broadcast television (video) signals.


Contents
1	History
2	Stations
3	Types
3.1	AM
3.1.1	Shortwave, medium wave and long wave
3.2	FM
3.3	Pirate radio
3.4	Terrestrial digital radio
4	Extensions
4.1	Satellite
5	Program formats
6	See also
7	References
8	Further reading
9	External links
History
See also: History of radio Â§ Broadcasting, and History of broadcasting

Advertisement placed in the November 5, 1919 Nieuwe Rotterdamsche Courant announcing PCGG's debut broadcast scheduled for the next evening.[1]
The earliest radio stations were radiotelegraphy systems and did not carry audio. For audio broadcasts to be possible, electronic detection and amplification devices had to be incorporated.

The thermionic valve (a kind of vacuum tube) was invented in 1904 by the English physicist John Ambrose Fleming. He developed a device he called an "oscillation valve" (because it passes current in only one direction). The heated filament, or cathode, was capable of thermionic emission of electrons that would flow to the plate (or anode) when it was at a higher voltage. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons. Later known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector.[2] This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker. However, what was still required was an amplifier.

The triode (mercury-vapor filled with a control grid) was patented on March 4, 1906, by the Austrian Robert von Lieben[3][4][5] independent from that, on October 25, 1906,[6][7] Lee De Forest patented his three-element Audion. It wasn't put to practical use until 1912 when its amplifying ability became recognized by researchers.[8]

By about 1920, valve technology had matured to the point where radio broadcasting was quickly becoming viable.[9][10] However, an early audio transmission that could be termed a broadcast may have occurred on Christmas Eve in 1906 by Reginald Fessenden, although this is disputed.[11] While many early experimenters attempted to create systems similar to radiotelephone devices by which only two parties were meant to communicate, there were others who intended to transmit to larger audiences. Charles Herrold started broadcasting in California in 1909 and was carrying audio by the next year. (Herrold's station eventually became KCBS).

In The Hague, the Netherlands, PCGG started broadcasting on November 6, 1919, making it, arguably the first commercial broadcasting station. In 1916, Frank Conrad, an electrical engineer employed at the Westinghouse Electric Corporation, began broadcasting from his Wilkinsburg, Pennsylvania garage with the call letters 8XK. Later, the station was moved to the top of the Westinghouse factory building in East Pittsburgh, Pennsylvania. Westinghouse relaunched the station as KDKA on November 2, 1920, as the first commercially licensed radio station in America.[12] The commercial broadcasting designation came from the type of broadcast license; advertisements did not air until years later. The first licensed broadcast in the United States came from KDKA itself: the results of the Harding/Cox Presidential Election. The Montreal station that became CFCF began broadcast programming on May 20, 1920, and the Detroit station that became WWJ began program broadcasts beginning on August 20, 1920, although neither held a license at the time.

In 1920, wireless broadcasts for entertainment began in the UK from the Marconi Research Centre 2MT at Writtle near Chelmsford, England. A famous broadcast from Marconi's New Street Works factory in Chelmsford was made by the famous soprano Dame Nellie Melba on June 15, 1920, where she sang two arias and her famous trill. She was the first artist of international renown to participate in direct radio broadcasts. The 2MT station began to broadcast regular entertainment in 1922. The BBC was amalgamated in 1922 and received a Royal Charter in 1926, making it the first national broadcaster in the world,[13][14] followed by Czech Radio and other European broadcasters in 1923.

Radio Argentina began regularly scheduled transmissions from the Teatro Coliseo in Buenos Aires on August 27, 1920, making its own priority claim. The station got its license on November 19, 1923. The delay was due to the lack of official Argentine licensing procedures before that date. This station continued regular broadcasting of entertainment and cultural fare for several decades.[15]

Radio in education soon followed and colleges across the U.S. began adding radio broadcasting courses to their curricula. Curry College in Milton, Massachusetts introduced one of the first broadcasting majors in 1932 when the college teamed up with WLOE in Boston to have students broadcast programs.[16]

Stations
"Radio station" redirects here. For a broader concept, see Radio communication station.
A radio broadcasting station is usually associated with wireless transmission, though in practice broadcasting transmission (sound and television) take place using both wires and radio waves. The point of this is that anyone with the appropriate receiving technology can receive the broadcast.[17]


Use of a sound broadcasting station
In line to ITU Radio Regulations (article1.61) each broadcasting station shall be classified by the service in which it operates permanently or temporarily.

Types

Transmission diagram of sound broadcasting (AM and FM)
Broadcasting by radio takes several forms. These include AM and FM stations. There are several subtypes, namely commercial broadcasting, non-commercial educational (NCE) public broadcasting and non-profit varieties as well as community radio, student-run campus radio stations, and hospital radio stations can be found throughout the world. Many stations broadcast on shortwave bands using AM technology that can be received over thousands of miles (especially at night). For example, the BBC, VOA, VOR, and Deutsche Welle have transmitted via shortwave to Africa and Asia. These broadcasts are very sensitive to atmospheric conditions and solar activity.

Nielsen Audio, formerly known as Arbitron, the United States-based company that reports on radio audiences, defines a "radio station" as a government-licensed AM or FM station; an HD Radio (primary or multicast) station; an internet stream of an existing government-licensed station; one of the satellite radio channels from XM Satellite Radio or Sirius Satellite Radio; or, potentially, a station that is not government licensed.[18]

AM
Main article: AM broadcasting

AM broadcasting stations in 2006
AM stations were the earliest broadcasting stations to be developed. AM refers to amplitude modulation, a mode of broadcasting radio waves by varying the amplitude of the carrier signal in response to the amplitude of the signal to be transmitted. The medium-wave band is used worldwide for AM broadcasting. Europe also uses the long wave band. In response to the growing popularity of FM stereo radio stations in the late 1980s and early 1990s, some North American stations began broadcasting in AM stereo, though this never gained popularity, and very few receivers were ever sold.

The signal is subject to interference from electrical storms (lightning) and other electromagnetic interference (EMI).[19] One advantage of AM radio signal is that it can be detected (turned into sound) with simple equipment. If a signal is strong enough, not even a power source is needed; building an unpowered crystal radio receiver was a common childhood project in the early decades of AM broadcasting.

AM broadcasts occur on North American airwaves in the medium wave frequency range of 525 to 1705 kHz (known as the âstandard broadcast bandâ). The band was expanded in the 1990s by adding nine channels from 1605 to 1705 kHz. Channels are spaced every 10 kHz in the Americas, and generally every 9 kHz everywhere else.

AM transmissions cannot be ionospherically propagated during the day due to strong absorption in the D-layer of the ionosphere. In a crowded channel environment, this means that the power of regional channels which share a frequency must be reduced at night or directionally beamed in order to avoid interference, which reduces the potential nighttime audience. Some stations have frequencies unshared with other stations in North America; these are called clear-channel stations. Many of them can be heard across much of the country at night. During the night, absorption largely disappears and permits signals to travel to much more distant locations via ionospheric reflections. However, fading of the signal can be severe at night.

AM radio transmitters can transmit audio frequencies up to 15 kHz (now limited to 10 kHz in the US due to FCC rules designed to reduce interference), but most receivers are only capable of reproducing frequencies up to 5 kHz or less. At the time that AM broadcasting began in the 1920s, this provided adequate fidelity for existing microphones, 78 rpm recordings, and loudspeakers. The fidelity of sound equipment subsequently improved considerably, but the receivers did not. Reducing the bandwidth of the receivers reduces the cost of manufacturing and makes them less prone to interference. AM stations are never assigned adjacent channels in the same service area. This prevents the sideband power generated by two stations from interfering with each other.[20] Bob Carver created an AM stereo tuner employing notch filtering that demonstrated that an AM broadcast can meet or exceed the 15 kHz baseband bandwidth allotted to FM stations without objectionable interference. After several years, the tuner was discontinued. Bob Carver had left the company and the Carver Corporation later cut the number of models produced before discontinuing production completely.[citation needed]

Shortwave, medium wave and long wave
See shortwave for the differences between shortwave, medium wave, and long wave spectra. Shortwave is used largely for national broadcasters, international propaganda, or religious broadcasting organizations.[21]

FM
Main article: FM broadcasting

FM radio broadcast stations in 2006
FM refers to frequency modulation, and occurs on VHF airwaves in the frequency range of 88 to 108 MHz everywhere except Japan and Russia. Russia, like the former Soviet Union, uses 65.9 to 74 MHz frequencies in addition to the world standard. Japan uses the 76 to 90 MHz frequency band.

Edwin Howard Armstrong invented FM radio to overcome the problem of radio-frequency interference (RFI), which plagued AM radio reception. At the same time, greater fidelity was made possible by spacing stations further apart in the radio frequency spectrum. Instead of 10 kHz apart, as on the AM band in the US, FM channels are 200 kHz (0.2 MHz) apart. In other countries, greater spacing is sometimes mandatory, such as in New Zealand, which uses 700 kHz spacing (previously 800 kHz). The improved fidelity made available was far in advance of the audio equipment of the 1940s, but wide interchannel spacing was chosen to take advantage of the noise-suppressing feature of wideband FM.

Bandwidth of 200 kHz is not needed to accommodate an audio signal â 20 kHz to 30 kHz is all that is necessary for a narrowband FM signal. The 200 kHz bandwidth allowed room for Â±75 kHz signal deviation from the assigned frequency, plus guard bands to reduce or eliminate adjacent channel interference. The larger bandwidth allows for broadcasting a 15 kHz bandwidth audio signal plus a 38 kHz stereo "subcarrier"âa piggyback signal that rides on the main signal. Additional unused capacity is used by some broadcasters to transmit utility functions such as background music for public areas, GPS auxiliary signals, or financial market data.

The AM radio problem of interference at night was addressed in a different way. At the time FM was set up, the available frequencies were far higher in the spectrum than those used for AM radio - by a factor of approximately 100. Using these frequencies meant that even at far higher power, the range of a given FM signal was much shorter; thus its market was more local than for AM radio. The reception range at night is the same as in the daytime. All FM broadcast transmissions are line-of-sight, and ionospheric bounce is not viable. The much larger bandwidths, compared to AM and SSB, are more susceptible to phase dispersion. Propagation speeds (celerities) are fastest in the ionosphere at the lowest sideband frequency. The celerity difference between the highest and lowest sidebands is quite apparent to the listener. Such distortion occurs up to frequencies of approximately 50 MHz. Higher frequencies do not reflect from the ionosphere, nor from storm clouds. Moon reflections have been used in some experiments, but require impractical power levels.

The original FM radio service in the U.S. was the Yankee Network, located in New England.[22][23][24] Regular FM broadcasting began in 1939 but did not pose a significant threat to the AM broadcasting industry. It required purchase of a special receiver. The frequencies used, 42 to 50 MHz, were not those used today. The change to the current frequencies, 88 to 108 MHz, began after the end of World War II and was to some extent imposed by AM broadcasters as an attempt to cripple what was by now realized to be a potentially serious threat.

FM radio on the new band had to begin from the ground floor. As a commercial venture, it remained a little-used audio enthusiasts' medium until the 1960s. The more prosperous AM stations, or their owners, acquired FM licenses and often broadcast the same programming on the FM station as on the AM station ("simulcasting"). The FCC limited this practice in the 1960s. By the 1980s, since almost all new radios included both AM and FM tuners, FM became the dominant medium, especially in cities. Because of its greater range, AM remained more common in rural environments.

Pirate radio
Main article: Pirate radio
Pirate radio is illegal or non-regulated radio transmission. It is most commonly used to describe illegal broadcasting for entertainment or political purposes. Sometimes it is used for illegal two-way radio operation. Its history can be traced back to the unlicensed nature of the transmission, but historically there has been occasional use of sea vesselsâfitting the most common perception of a pirateâas broadcasting bases. Rules and regulations vary largely from country to country, but often the term pirate radio generally describes the unlicensed broadcast of FM radio, AM radio, or shortwave signals over a wide range. In some places, radio stations are legal where the signal is transmitted, but illegal where the signals are receivedâespecially when the signals cross a national boundary. In other cases, a broadcast may be considered "pirate" due to the type of content, its transmission format, or the transmitting power (wattage) of the station, even if the transmission is not technically illegal (such as a webcast or an amateur radio transmission). Pirate radio stations are sometimes referred to as bootleg radio or clandestine stations.

Terrestrial digital radio
Main articles: Digital audio broadcasting, HD radio, ISDB, and Digital Radio Mondiale
Digital radio broadcasting has emerged, first in Europe (the UK in 1995 and Germany in 1999), and later in the United States, France, the Netherlands, South Africa, and many other countries worldwide. The simplest system is named DAB Digital Radio, for Digital Audio Broadcasting, and uses the public domain EUREKA 147 (Band III) system. DAB is used mainly in the UK and South Africa. Germany and the Netherlands use the DAB and DAB+ systems, and France uses the L-Band system of DAB Digital Radio.

The broadcasting regulators of the United States and Canada have chosen to use HD radio, an in-band on-channel system that puts digital broadcasts at frequencies adjacent to the analog broadcast. HD Radio is owned by a consortium of private companies that is called iBiquity. An international non-profit consortium Digital Radio Mondiale (DRM), has introduced the public domain DRM system, which is used by a relatively small number of broadcasters worldwide.

Extensions
Extensions of traditional radio-wave broadcasting for audio broadcasting in general include cable radio, local wire television networks, DTV radio, satellite radio, and internet radio via streaming media on the Internet.

Satellite
[icon]
This section needs expansion. You can help by adding to it. (November 2008)
Main article: Satellite radio
The enormous entry costs of space-based satellite transmitters and restrictions on available radio spectrum licenses has restricted growth of Satellite radio broadcasts. In the US and Canada, just two services, XM Satellite Radio and Sirius Satellite Radio exist. Both XM and Sirius are owned by Sirius XM Satellite Radio, which was formed by the merger of XM and Sirius on July 29, 2008, whereas in Canada, XM Radio Canada and Sirius Canada remained separate companies until 2010. Worldspace in Africa and Asia, and MobaHO! in Japan and the ROK were two unsuccessful satellite radio operators which have gone out of business.

Program formats
Main article: Radio format
Radio program formats differ by country, regulation, and markets. For instance, the U.S. Federal Communications Commission designates the 88â92 megahertz band in the U.S. for non-profit or educational programming, with advertising prohibited.

In addition, formats change in popularity as time passes and technology improves. Early radio equipment only allowed program material to be broadcast in real time, known as live broadcasting. As technology for sound recording improved, an increasing proportion of broadcast programming used pre-recorded material. A current trend is the automation of radio stations. Some stations now operate without direct human intervention by using entirely pre-recorded material sequenced by computer control.

See also
Broadcasting construction permit
Call sign
Disc jockey (DJ)
History of broadcasting
International broadcasting
List of radio topics
Low power radio station
Radio
Radio antenna
Radio network
Radio personality
RF modulation
Sports commentator
Television station
References
 "Vintage Radio Web: Philips" (vintageradio.nl)
 Guarnieri, M. (2012). "The age of vacuum tubes: Early devices and the rise of radio communications". IEEE Ind. Electron. M.: 41â43. doi:10.1109/MIE.2012.2182822.
 Schmidt, Hans-Thomas. "Die LiebenrÃ¶hre". Umleitung zur Homepage von H.-T. Schmidt (in German). Retrieved August 10, 2019. DRP 179807
 Tapan K. Sarkar (ed.) "History of wireless", John Wiley and Sons, 2006. ISBN 0-471-71814-9, p.335
 SÅgo Okamura (ed), History of Electron Tubes, IOS Press, 1994 ISBN 90-5199-145-2 page 20
 "US841387A - Device for amplifying feeble electrical currents". Google Patents. October 25, 1906. Retrieved August 10, 2019.
 "US879532A - Space telegraphy". Google Patents. January 29, 1907. Retrieved August 10, 2019.
 Nebeker, Frederik (2009). Dawn of the Electronic Age: Electrical Technologies in the Shaping of the Modern World, 1914 to 1945. John Wiley & Sons. pp. 14â15. ISBN 978-0470409749.
 "Making the Modern World - Mass consumption". webarchive.nationalarchives.gov.uk. Archived from the original on April 5, 2017.
 Guarnieri, M. (2012). "The age of vacuum tubes: the conquest of analog communications". IEEE Ind. Electron. M.: 52â54. doi:10.1109/MIE.2012.2193274.
 Fessenden â The Next Chapter RWonline.com Archived September 16, 2009, at the Wayback Machine
 Baudino, Joseph E; John M. Kittross (Winter 1977). "Broadcasting's Oldest Stations: An Examination of Four Claimants". Journal of Broadcasting. 21: 61â82. doi:10.1080/08838157709363817. Archived from the original on March 6, 2008. Retrieved January 18, 2013.
 "CARS - Marconi Hall Street, New Street and 2MT callsign". www.g0mwt.org.uk.
 "BBC History â The BBC takes to the Airwaves". BBC News.
 Atgelt, Carlos A. "Early History of Radio Broadcasting in Argentina." The Broadcast Archive (Oldradio.com).
 "Curry College - Home". www.curry.edu. Retrieved July 13, 2018.
 Neira, Bob. "Broadcasting". modestoradiomuseum. modestoradiomuseum.
 "What is a Radio Station?". Radio World. p. 6.
 Based on the "interference" entry of The Concise Oxford English Dictionary, 11th edition, online
 "Types of Technology, FM vs AM". kwarner.bravehost.com. July 13, 2012. Archived from the original on July 13, 2012. Retrieved August 10, 2019.
 Grodkowski, Paul (August 24, 2015). Beginning Shortwave Radio Listening. Booktango. ISBN 9781468964240.
 Halper, Donna L. "John Shepard's FM StationsâAmerica's first FM network." Boston Radio Archives (BostonRadio.org).
 "The Yankee Network in 1936". The Archives @ BostonRadio.org. Retrieved August 10, 2019.
 "FM Broadcasting Chronology". Jeff Miller Pages. June 23, 2017. Retrieved August 10, 2019.
Further reading
Briggs Asa. The History of Broadcasting in the United Kingdom (Oxford University Press, 1961).
Crisell, Andrew. An Introductory History of British Broadcasting (2002) excerpt
Ewbank Henry and Lawton Sherman P. Broadcasting: Radio and Television (Harper & Brothers, 1952).
Fisher, Marc. Something In The Air: Radio, Rock, and the Revolution That Shaped A Generation (Random House, 2007).
Hausman, Carl, Messere, Fritz, Benoit, Philip, and O'Donnell, Lewis, Modern Radio Production, 9th ed., (Cengage, 2013)
Head, Sydney W., Christopher W. Sterling, and Lemuel B. Schofield. Broadcasting in America." (7th ed. 1994).
Lewis, Tom, Empire of the Air: The Men Who Made Radio, 1st ed., New York : E. Burlingame Books, 1991. ISBN 0-06-018215-6. "Empire of the Air: The Men Who Made Radio" (1992) by Ken Burns was a PBS documentary based on the book.
Pilon, Robert, Isabelle Lamoureux, and Gilles Turcotte. Le MarchÃ© de la radio au QuÃ©bec: document de reference. [MontrÃ©al]: Association quÃ©bÃ©coise de l'industrie du dique, du spectacle et de la video, 1991. unpaged. N.B.: Comprises: Robert Pilon's and Isabelle Lamoureux' Profil du marchÃ© de radio au QuÃ©bec: un analyse de MÃ©dia-culture. -- Gilles Turcotte's Analyse comparative de l'Ã©coute des principals stations de MontrÃ©al: prepare par Info Cible.
Ray, William B. FCC: The Ups and Downs of Radio-TV Regulation (Iowa State University Press, 1990).
Russo, Alexan der. Points on the Dial: Golden Age Radio Beyond the Networks (Duke University Press; 2010) 278 pages; discusses regional and local radio as forms that "complicate" the image of the medium as a national unifier from the 1920s to the 1950s.
Scannell, Paddy, and Cardiff, David. A Social History of British Broadcasting, Volume One, 1922-1939 (Basil Blackwell, 1991).
Schramm, Wilbur, ed. The Process and Effects of Mass Communication (1955 and later editions) articles by social scientists
Schramm, Wilbur, ed. Mass Communication (1950, 2nd ed. 1960); more popular essays
Schwoch James. The American Radio Industry and Its Latin American Activities, 1900-1939 (University of Illinois Press, 1990).
Stewart, Sandy. From Coast to Coast: a Personal History of Radio in Canada (Entreprises Radio-Canada, 1985). xi, 191 p., ill., chiefly with b&w photos. ISBN 0-88794-147-8
Stewart, Sandy. A Pictorial History of Radio in Canada (Gage Publishing, 1975). v, [1], 154 p., amply ill. in b&w mostly with photos. SBN 7715-9948-X
White Llewellyn. The American Radio (University of Chicago Press, 1947).
External links
	Look up radio broadcasting in Wiktionary, the free dictionary.
General
Federal Communications Commission website - fcc.gov
DXing.info - Information about radio stations worldwide
Radio-Locator.com- Links to 13,000 radio stations worldwide
BBC reception advice
DXradio.50webs.com "The SWDXER" - with general SWL information and radio antenna tips
RadioStationZone.com - 10.000+ radio stations worldwide with ratings, comments and listen live links
Online-Radio-Stations.org - The Web Radio Tuner has a comprehensive list of over 50.000 radio stations
UnwantedEmissions.com - A general reference to radio spectrum allocations
Radio stanice - Search for radio stations throughout the Europe
Radio Emisoras Latinas - has a directory with thousands of Latin America Radio Stations
MY FM Radio Live - MY FM Radio Live - Internet radio broadcast
vte
Broadcasting
Links to related articles
Categories: Radio broadcasting
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
ÙØ§Ø±Ø³Û
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
à®¤à®®à®¿à®´à¯
TÃ¼rkÃ§e
Ø§Ø±Ø¯Ù
14 more
Edit links
This page was last edited on 26 October 2020, at 19:38 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

FM broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from FM radio)
Jump to navigationJump to search

AM and FM modulated signals for radio. AM (amplitude modulation) and FM (frequency modulation) are types of modulation (coding). The sound of the program material, usually coming from a radio studio, is used to modulate (vary) a carrier wave of a specific frequency, then broadcast.

In AM broadcasting, the amplitude of the carrier wave is modulated to encode the original sound. In FM broadcasting, the frequency of the carrier wave is modulated to encode the sound. A radio receiver extracts the original program sound from the modulated radio signal and reproduces the sound in a loudspeaker.

Position of FM radio in the electromagnetic spectrum

A commercial 35 kW FM radio transmitter built in the late 1980s. It belongs to FM radio station KWNR in Henderson, Nevada and broadcasts at a frequency of 95.5 MHz.
FM broadcasting is a method of radio broadcasting using frequency modulation (FM). Invented in 1933 by American engineer Edwin Armstrong, wide-band FM is used worldwide to provide high fidelity sound over broadcast radio. FM broadcasting is capable of higher fidelityâthat is, more accurate reproduction of the original program soundâthan other broadcasting technologies, such as AM broadcasting. Therefore, FM is used for most broadcasts of music or general audio (in the audio spectrum). FM radio stations use the very high frequency range of radio frequencies.


Contents
1	Broadcast bands
2	Technology
2.1	Modulation
2.2	Pre-emphasis and de-emphasis
2.3	Stereo FM
2.4	Quadraphonic FM
2.5	Noise reduction
2.6	Other subcarrier services
2.7	Transmission power
2.8	Reception distance
3	History
3.1	United States
3.2	Europe
3.2.1	United Kingdom
3.2.2	Italy
3.2.3	Greece
3.3	Australia
3.4	New Zealand
3.5	Trinidad and Tobago
3.6	Turkey
3.7	Other countries
3.8	ITU Conferences about FM
4	FM broadcasting switch-off
5	Small-scale use of the FM broadcast band
5.1	Consumer use of FM transmitters
5.2	Assistive listening
5.3	Microbroadcasting
5.4	Clandestine use of FM transmitters
6	See also
6.1	FM broadcasting by country
6.2	FM broadcasting (technical)
6.3	Lists
6.4	History
6.5	Bands
7	References
8	External links
Broadcast bands
Main article: FM broadcast band
Throughout the world, the FM broadcast band falls within the VHF part of the radio spectrum. Usually 87.5 to 108.0 MHz is used,[1] or some portion thereof, with few exceptions:

In the former Soviet republics, and some former Eastern Bloc countries, the older 65.8â74 MHz band is also used. Assigned frequencies are at intervals of 30 kHz. This band, sometimes referred to as the OIRT band, is slowly being phased out. Where the OIRT band is used, the 87.5â108.0 MHz band is referred to as the CCIR band.
In Japan, the band 76â95 MHz is used.
The frequency of an FM broadcast station (more strictly its assigned nominal center frequency) is usually a multiple of 100 kHz. In most of South Korea, the Americas, the Philippines and the Caribbean, only odd multiples are used. Some other countries follow this plan because of the import of vehicles, principally from the United States, with radios that can only tune to these frequencies. In some parts of Europe, Greenland and Africa, only even multiples are used. In the UK odd or even are used. In Italy, multiples of 50 kHz are used. In most countries the maximum permitted frequency error of the unmodulated carrier is specified, which typically should be within 2000 Hz of the assigned frequency.[2][3]

There are other unusual and obsolete FM broadcasting standards in some countries, with non-standard spacings of 1, 10, 30, 74, 500, and 300 kHz. However, to minimise inter-channel interference, stations operating from the same or geographically close transmitter sites tend to keep to at least a 500 kHz frequency separation even when closer frequency spacing is technically permitted. The ITU publishes Protection Ratio graphs which give the minimum spacing between frequencies based on their relative strengths.[4] Only broadcast stations with large enough geographic separations between their coverage areas can operate on close or the same frequencies.

Technology

FM has better rejection of static (RFI) than AM. This was shown in a dramatic demonstration by General Electric at its New York lab in 1940. The radio had both AM and FM receivers. With a million-volt arc as a source of interference behind it, the AM receiver produced a roar of static, while the FM receiver clearly reproduced a music program from Armstrong's experimental FM transmitter in New Jersey.

Crossed dipole antenna of station KENZ's 94.9 MHz, 48 kW transmitter on Lake Mountain, Utah. It radiates circularly polarized radio waves.
Modulation
Frequency modulation or FM is a form of modulation which conveys information by varying the frequency of a carrier wave; the older amplitude modulation or AM varies the amplitude of the carrier, with its frequency remaining constant. With FM, frequency deviation from the assigned carrier frequency at any instant is directly proportional to the amplitude of the input signal, determining the instantaneous frequency of the transmitted signal. Because transmitted FM signals use more bandwidth than AM signals, this form of modulation is commonly used with the higher (VHF or UHF) frequencies used by TV, the FM broadcast band, and land mobile radio systems.

The maximum frequency deviation of the carrier is usually specified and regulated by the licensing authorities in each country. For a stereo broadcast, the maximum permitted carrier deviation is invariably Â±75 kHz, although a little higher is permitted in the United States when SCA systems are used. For a monophonic broadcast, again the most common permitted maximum deviation is Â±75 kHz. However, some countries specify a lower value for monophonic broadcasts, such as Â±50 kHz.[5]


Armstrong's first prototype FM broadcast transmitter, located in the Empire State Building, New York City, which he used for secret tests of his system between 1934 and 1935. Licensed as experimental station W2XDG, it transmitted on 41 MHz at a power of 2 kW

Instantaneous spectrum and waterfall plot in the FM broadcast band showing three strong local stations; speech and music show different patterns of frequency vs. time. When the transmitted audio is quiet, the 19 kHz stereo pilot tones can be resolved in the spectrum.
Pre-emphasis and de-emphasis
Random noise has a triangular spectral distribution in an FM system, with the effect that noise occurs predominantly at the highest audio frequencies within the baseband. This can be offset, to a limited extent, by boosting the high frequencies before transmission and reducing them by a corresponding amount in the receiver. Reducing the high audio frequencies in the receiver also reduces the high-frequency noise. These processes of boosting and then reducing certain frequencies are known as pre-emphasis and de-emphasis, respectively.

The amount of pre-emphasis and de-emphasis used is defined by the time constant of a simple RC filter circuit. In most of the world a 50 Âµs time constant is used. In the Americas and South Korea, 75 Âµs is used.[6] This applies to both mono and stereo transmissions. For stereo, pre-emphasis is applied to the left and right channels before multiplexing.

The use of pre-emphasis becomes a problem because of the fact that many forms of contemporary music contain more high-frequency energy than the musical styles which prevailed at the birth of FM broadcasting. Pre-emphasizing these high-frequency sounds would cause excessive deviation of the FM carrier. Modulation control (limiter) devices are used to prevent this. Systems more modern than FM broadcasting tend to use either programme-dependent variable pre-emphasis; e.g., dbx in the BTSC TV sound system, or none at all.

Pre-emphasis and de-emphasis was used in the earliest days of FM broadcasting. According to a BBC report from 1946,[7] 100 Âµs was originally considered in the US, but 75 Âµs subsequently adopted.

Stereo FM
Long before FM stereo transmission was considered, FM multiplexing of other types of audio level information was experimented with.[8] Edwin Armstrong who invented FM was the first to experiment with multiplexing, at his experimental 41 MHz station W2XDG located on the 85th floor of the Empire State Building in New York City.

These FM multiplex transmissions started in November 1934 and consisted of the main channel audio program and three subcarriers: a fax program, a synchronizing signal for the fax program and a telegraph "order" channel. These original FM multiplex subcarriers were amplitude modulated.

Two musical programs, consisting of both the Red and Blue Network program feeds of the NBC Radio Network, were simultaneously transmitted using the same system of subcarrier modulation as part of a studio-to-transmitter link system. In April 1935, the AM subcarriers were replaced by FM subcarriers, with much improved results.

Show more
Show WhitespaceHide Whitespace
Actual Output
Output:No output
Error output:No output
larger file, potato network (duplicate test 4 for partial credit) (1MB/6s)
0/1
send file
test file
Correctness
Actual exit status:0
Output:
Output Diffs
Expected Output
Student Output


Flow control (data)
Flow control (data)
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Jump to navigationJump to search
Not to be confused with Control flow.
Not to be confused with Control flow.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.


Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.
Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.




Contents
Contents
1	Stop-and-wait
1	Stop-and-wait
1.1	Operations
1.1	Operations
1.2	Pros and cons of stop and wait
1.2	Pros and cons of stop and wait
2	Sliding Window
2	Sliding Window
2.1	Go Back N
2.1	Go Back N
2.2	Selective Repeat
2.2	Selective Repeat
3	Comparison
3	Comparison
3.1	Stop-and-Wait
3.1	Stop-and-Wait
3.2	Selective Repeat
3.2	Selective Repeat
4	Transmit flow control
4	Transmit flow control
4.1	Hardware flow control
4.1	Hardware flow control
4.2	Software flow control
4.2	Software flow control
5	Open-loop flow control
5	Open-loop flow control
6	Closed-loop flow control
6	Closed-loop flow control
7	See also
7	See also
8	References
8	References
9	External links
9	External links
Stop-and-wait
Stop-and-wait
Main article: Stop-and-wait ARQ
Main article: Stop-and-wait ARQ
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.


Operations
Operations
Sender: Transmits a single frame at a time.
Sender: Transmits a single frame at a time.
Sender waits to receive ACK within time out.
Sender waits to receive ACK within time out.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Go to step 1 when ACK is received, or time out is hit.
Go to step 1 when ACK is received, or time out is hit.
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).


The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.
The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.


Pros and cons of stop and wait
Pros and cons of stop and wait
Pros
Pros


The only advantage of this method of flow control is its simplicity.
The only advantage of this method of flow control is its simplicity.


Cons
Cons


The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]
The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]


Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]
Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]


Sliding Window
Sliding Window
Main article: Sliding Window Protocol
Main article: Sliding Window Protocol
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]


Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.
Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.


Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.
Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.


Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.
Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.


Go Back N
Go Back N
Main article: Go-Back-N ARQ
Main article: Go-Back-N ARQ
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.


The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.
The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.


To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]
To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]


Selective Repeat
Selective Repeat
Main article: Selective Repeat ARQ
Main article: Selective Repeat ARQ
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]


Comparison
Comparison
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.


Stop-and-Wait
Stop-and-Wait
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]


With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]
With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]


Selective Repeat
Selective Repeat
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]


Transmit flow control
Transmit flow control
Transmit flow control may occur:
Transmit flow control may occur:


between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be


either stop-and-wait,
either stop-and-wait,
or use a sliding window.
or use a sliding window.
Flow control can be performed
Flow control can be performed


either by control signal lines in a data communication interface (see serial port and RS-232),
either by control signal lines in a data communication interface (see serial port and RS-232),
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
Hardware flow control
Hardware flow control
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:


RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:


In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.


Polarity:
Polarity:
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
Software flow control
Software flow control
Main article: Software flow control
Main article: Software flow control
Conversely, XON/XOFF is usually referred to as software flow control.
Conversely, XON/XOFF is usually referred to as software flow control.


Open-loop flow control
Open-loop flow control
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.


Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]
Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]


Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.
Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.


Closed-loop flow control
Closed-loop flow control
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.


This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:
This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:


A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.
A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.


In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.
In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.


The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.
The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.


See also
See also
Software flow control
Software flow control
Computer networking
Computer networking
Traffic contract
Traffic contract
Congestion control
Congestion control
Teletraffic engineering in broadband networks
Teletraffic engineering in broadband networks
Teletraffic engineering
Teletraffic engineering
Ethernet flow control
Ethernet flow control
Handshaking
Handshaking
References
References
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 Webster Dictionary definition last accessed 3 December 2012.
 Webster Dictionary definition last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
Sliding window:
Sliding window:


[1] last accessed 27 November 2012.
[1] last accessed 27 November 2012.
External links
External links
RS-232 flow control and handshaking
RS-232 flow control and handshaking
Categories: Flow control (data)Network performanceLogical link controlData transmission
Categories: Flow control (data)Network performanceLogical link controlData transmission
Navigation menu
Navigation menu
Not logged inTalkContributionsCreate accountLog in
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ArticleTalk
ReadEditView historySearch
ReadEditView historySearch
Search Wikipedia
Search Wikipedia
Main page
Main page
Contents
Contents
Current events
Current events
Random article
Random article
About Wikipedia
About Wikipedia
Contact us
Contact us
Donate
Donate
Contribute
Contribute
Help
Help
Learn to edit
Learn to edit
Community portal
Community portal
Recent changes
Recent changes
Upload file
Upload file
Tools
Tools
What links here
What links here
Related changes
Related changes
Special pages
Special pages
Permanent link
Permanent link
Page information
Page information
Cite this page
Cite this page
Wikidata item
Wikidata item
Print/export
Print/export
Download as PDF
Download as PDF
Printable version
Printable version


Languages
Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Ø§ÙØ¹Ø±Ø¨ÙØ©
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
Deutsch
Deutsch
ÙØ§Ø±Ø³Û
ÙØ§Ø±Ø³Û
FranÃ§ais
FranÃ§ais
íêµ­ì´
íêµ­ì´
Italiano
Italiano
æ¥æ¬èª
æ¥æ¬èª
Ð ÑÑÑÐºÐ¸Ð¹
Ð ÑÑÑÐºÐ¸Ð¹
4 more
4 more
Edit links
Edit links
This page was last edited on 18 July 2020, at 11:55 (UTC).
This page was last edited on 18 July 2020, at 11:55 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki


Data transmission
Data transmission
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
  (Redirected from Data communications)
  (Redirected from Data communications)
Jump to navigationJump to search
Jump to navigationJump to search
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.


Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.
Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.


Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.
Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.




Contents
Contents
1	Distinction between related subjects
1	Distinction between related subjects
2	Protocol layers and sub-topics
2	Protocol layers and sub-topics
3	Applications and history
3	Applications and history
4	Serial and parallel transmission
4	Serial and parallel transmission
5	Communication channels
5	Communication channels
6	Asynchronous and synchronous data transmission
6	Asynchronous and synchronous data transmission
7	See also
7	See also
8	References
8	References
Distinction between related subjects
Distinction between related subjects
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.


Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.
Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.


The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]
The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]


The theoretical aspects of data transmission are covered by information theory and coding theory.
The theoretical aspects of data transmission are covered by information theory and coding theory.


Protocol layers and sub-topics
Protocol layers and sub-topics
OSI model
OSI model
by layer
by layer
7.  Application layer[show]
7.  Application layer[show]
6.  Presentation layer[show]
6.  Presentation layer[show]
5.  Session layer[show]
5.  Session layer[show]
4.  Transport layer[show]
4.  Transport layer[show]
3.  Network layer[show]
3.  Network layer[show]
2.  Data link layer[show]
2.  Data link layer[show]
1.  Physical layer[show]
1.  Physical layer[show]
vte
vte
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:


Layer 1, the physical layer:
Layer 1, the physical layer:
Channel coding including
Channel coding including
Digital modulation schemes
Digital modulation schemes
Line coding schemes
Line coding schemes
Forward error correction (FEC) codes
Forward error correction (FEC) codes
Bit synchronization
Bit synchronization
Multiplexing
Multiplexing
Equalization
Equalization
Channel models
Channel models
Layer 2, the data link layer:
Layer 2, the data link layer:
Channel access schemes, media access control (MAC)
Channel access schemes, media access control (MAC)
Packet mode communication and Frame synchronization
Packet mode communication and Frame synchronization
Error detection and automatic repeat request (ARQ)
Error detection and automatic repeat request (ARQ)
Flow control
Flow control
Layer 6, the presentation layer:
Layer 6, the presentation layer:
Source coding (digitization and data compression), and information theory.
Source coding (digitization and data compression), and information theory.
Cryptography (may occur at any layer)
Cryptography (may occur at any layer)
It is also common to deal with the cross-layer design of those three layers.[6]
It is also common to deal with the cross-layer design of those three layers.[6]


Applications and history
Applications and history
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.


Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.
Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.


Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.
Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.


In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.
In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.


Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.
Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.


Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.
Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.


The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.
The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.


Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.
Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.


While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.
While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.


Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.
Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.


Serial and parallel transmission
Serial and parallel transmission
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.


In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.
In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.


Communication channels
Communication channels
Main article: communication channel
Main article: communication channel
Some communications channel types include:
Some communications channel types include:


Data transmission circuit
Data transmission circuit
Full-duplex
Full-duplex
Half-duplex
Half-duplex
Multi-drop:
Multi-drop:
Bus network
Bus network
Mesh network
Mesh network
Ring network
Ring network
Star network
Star network
Wireless network
Wireless network
Point-to-point
Point-to-point
Simplex
Simplex
Asynchronous and synchronous data transmission
Asynchronous and synchronous data transmission
Main article: Comparison of synchronous and asynchronous signalling
Main article: Comparison of synchronous and asynchronous signalling
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.


Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.
Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.


See also
See also
Computer networking
Computer networking
Communication
Communication
Data migration
Data migration
Information theory
Information theory
Media (communication)
Media (communication)
Network security
Network security
Node-to-node data transfer
Node-to-node data transfer
Packet switching
Packet switching
Signal processing
Signal processing
Telecommunication
Telecommunication
Transmission (disambiguation)
Transmission (disambiguation)
References
References
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Navigation menu
Navigation menu
Not logged inTalkContributionsCreate accountLog in
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ArticleTalk
ReadEditView historySearch
ReadEditView historySearch
Search Wikipedia
Search Wikipedia
Main page
Main page
Contents
Contents
Current events
Current events
Random article
Random article
About Wikipedia
About Wikipedia
Contact us
Contact us
Donate
Donate
Contribute
Contribute
Help
Help
Learn to edit
Learn to edit
Community portal
Community portal
Recent changes
Recent changes
Upload file
Upload file
Tools
Tools
What links here
What links here
Related changes
Related changes
Special pages
Special pages
Permanent link
Permanent link
Page information
Page information
Cite this page
Cite this page
Wikidata item
Wikidata item
Print/export
Print/export
Download as PDF
Download as PDF
Printable version
Printable version
In other projects
In other projects
Wikimedia Commons
Wikimedia Commons


Languages
Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
Deutsch
EspaÃ±ol
EspaÃ±ol
FranÃ§ais
FranÃ§ais
íêµ­ì´
íêµ­ì´
Italiano
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
Tiáº¿ng Viá»t
ä¸­æ
ä¸­æ
28 more
28 more
Edit links
Edit links
This page was last edited on 30 October 2020, at 04:19 (UTC).
This page was last edited on 30 October 2020, at 04:19 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki


Modulation
Modulation
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Jump to navigationJump to search
For other uses, see Modulation (disambiguation).
For other uses, see Modulation (disambiguation).


This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
Passband modulation
Passband modulation
Analog modulation
Analog modulation
AMFMPMQAMSMSSB
AMFMPMQAMSMSSB
Digital modulation
Digital modulation
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
Hierarchical modulation
Hierarchical modulation
QAMWDM
QAMWDM
Spread spectrum
Spread spectrum
CSSDSSSFHSSTHSS
CSSDSSSFHSSTHSS
See also
See also
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
vte
vte
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.


Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.
Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.


In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.
In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.


In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]
In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]


A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.
A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.


The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).
The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).


The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.
The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.


The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.
The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.




Contents
Contents
1	Analog modulation methods
1	Analog modulation methods
2	Digital modulation methods
2	Digital modulation methods
2.1	Fundamental digital modulation methods
2.1	Fundamental digital modulation methods
2.2	Modulator and detector principles of operation
2.2	Modulator and detector principles of operation
2.3	List of common digital modulation techniques
2.3	List of common digital modulation techniques
2.4	Automatic digital modulation recognition (ADMR)
2.4	Automatic digital modulation recognition (ADMR)
2.5	Digital baseband modulation or line coding
2.5	Digital baseband modulation or line coding
3	Pulse modulation methods
3	Pulse modulation methods
4	Miscellaneous modulation techniques
4	Miscellaneous modulation techniques
5	See also
5	See also
6	References
6	References
7	Further reading
7	Further reading
8	External links
8	External links
Analog modulation methods
Analog modulation methods


A low-frequency message signal (top) may be carried by an AM or FM radio wave.
A low-frequency message signal (top) may be carried by an AM or FM radio wave.


Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.
Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.


A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:


Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Double-sideband modulation (DSB)
Double-sideband modulation (DSB)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband reduced carrier transmission (DSB-RC)
Double-sideband reduced carrier transmission (DSB-RC)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Vestigial sideband modulation (VSB, or VSB-AM)
Vestigial sideband modulation (VSB, or VSB-AM)
Quadrature amplitude modulation (QAM)
Quadrature amplitude modulation (QAM)
Angle modulation, which is approximately constant envelope
Angle modulation, which is approximately constant envelope
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Digital modulation methods
Digital modulation methods
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).




Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.


According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.
According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.


Fundamental digital modulation methods
Fundamental digital modulation methods
The most fundamental digital modulation techniques are based on keying:
The most fundamental digital modulation techniques are based on keying:


PSK (phase-shift keying): a finite number of phases are used.
PSK (phase-shift keying): a finite number of phases are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.


In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.
In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.


If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.
If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.


For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.
For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.


In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.
In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.


Modulator and detector principles of operation
Modulator and detector principles of operation
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).


These are the general steps used by the modulator to transmit data:
These are the general steps used by the modulator to transmit data:


Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
At the receiver side, the demodulator typically performs:
At the receiver side, the demodulator typically performs:


Bandpass filtering.
Bandpass filtering.
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Parallel-to-serial conversion of the codewords into a bit stream.
Parallel-to-serial conversion of the codewords into a bit stream.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.


Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.
Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.


List of common digital modulation techniques
List of common digital modulation techniques
The most common digital modulation techniques are:
The most common digital modulation techniques are:


Phase-shift keying (PSK)
Phase-shift keying (PSK)
Binary PSK (BPSK), using M=2 symbols
Binary PSK (BPSK), using M=2 symbols
Quadrature PSK (QPSK), using M=4 symbols
Quadrature PSK (QPSK), using M=4 symbols
8PSK, using M=8 symbols
8PSK, using M=8 symbols
16PSK, using M=16 symbols
16PSK, using M=16 symbols
Differential PSK (DPSK)
Differential PSK (DPSK)
Differential QPSK (DQPSK)
Differential QPSK (DQPSK)
Offset QPSK (OQPSK)
Offset QPSK (OQPSK)
Ï/4âQPSK
Ï/4âQPSK
Frequency-shift keying (FSK)
Frequency-shift keying (FSK)
Audio frequency-shift keying (AFSK)
Audio frequency-shift keying (AFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Dual-tone multi-frequency (DTMF)
Dual-tone multi-frequency (DTMF)
Amplitude-shift keying (ASK)
Amplitude-shift keying (ASK)
On-off keying (OOK), the most common ASK form
On-off keying (OOK), the most common ASK form
M-ary vestigial sideband modulation, for example 8VSB
M-ary vestigial sideband modulation, for example 8VSB
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Continuous phase modulation (CPM) methods
Continuous phase modulation (CPM) methods
Minimum-shift keying (MSK)
Minimum-shift keying (MSK)
Gaussian minimum-shift keying (GMSK)
Gaussian minimum-shift keying (GMSK)
Continuous-phase frequency-shift keying (CPFSK)
Continuous-phase frequency-shift keying (CPFSK)
Orthogonal frequency-division multiplexing (OFDM) modulation
Orthogonal frequency-division multiplexing (OFDM) modulation
Discrete multitone (DMT), including adaptive modulation and bit-loading
Discrete multitone (DMT), including adaptive modulation and bit-loading
Wavelet modulation
Wavelet modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Spread-spectrum techniques
Spread-spectrum techniques
Direct-sequence spread spectrum (DSSS)
Direct-sequence spread spectrum (DSSS)
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).


OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.
OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.


Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.
Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.


Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]


There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.
There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.


Digital baseband modulation or line coding
Digital baseband modulation or line coding
Main article: Line code
Main article: Line code
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]


vte
vte
Line coding (digital baseband transmission)
Line coding (digital baseband transmission)
Pulse modulation methods
Pulse modulation methods
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.


Analog-over-analog methods
Analog-over-analog methods


Pulse-amplitude modulation (PAM)
Pulse-amplitude modulation (PAM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-position modulation (PPM)
Pulse-position modulation (PPM)
Analog-over-digital methods
Analog-over-digital methods


Pulse-code modulation (PCM)
Pulse-code modulation (PCM)
Differential PCM (DPCM)
Differential PCM (DPCM)
Adaptive DPCM (ADPCM)
Adaptive DPCM (ADPCM)
Delta modulation (DM or Î-modulation)
Delta modulation (DM or Î-modulation)
Delta-sigma modulation (âÎ)
Delta-sigma modulation (âÎ)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Pulse-density modulation (PDM)
Pulse-density modulation (PDM)
Miscellaneous modulation techniques
Miscellaneous modulation techniques
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
Adaptive modulation
Adaptive modulation
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
See also
See also
	Wikimedia Commons has media related to Modulation.
	Wikimedia Commons has media related to Modulation.
Channel access methods
Channel access methods
Channel coding
Channel coding
Codec
Codec
Communications channel
Communications channel
Demodulation
Demodulation
Electrical resonance
Electrical resonance
Heterodyne
Heterodyne
Line code
Line code
Mechanically induced modulation
Mechanically induced modulation
Modem
Modem
Modulation order
Modulation order
Neuromodulation
Neuromodulation
RF modulator
RF modulator
Ring modulation
Ring modulation
Telecommunication
Telecommunication
Types of radio emissions
Types of radio emissions
References
References


This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
Further reading
Further reading
Multipliers vs. Modulators Analog Dialogue, June 2013
Mul
External links
Interactive presentation of soft-demapping for AWGN-channel in a web-demo Institute of Telecommunications, University of Stuttgart
Modem (Modulation and Demodulation)
vte
Telecommunications
vte
Analog and digital audio broadcasting
Categories: Frequency mixersHistory of radioHistory of televisionPhysical layer protocolsRadio modulation modesTelecommunication theoryTelevision terminology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
52 more
Edit links
This page was last edited on 17 October 2020, at 19:13 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Radio broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from Radio broadcast)
Jump to navigationJump to search

Long wave radio broadcasting station, Motala, Sweden

Slovak Radio Building, Bratislava, Slovakia (architects: Å tefan Svetko, Å tefan ÄurkoviÄ and BarnabÃ¡Å¡ Kissling, 1967â1983)

Broadcasting tower in Trondheim, Norway
Radio broadcasting is transmission of audio (sound), sometimes with related metadata, by radio waves intended to reach a wide audience. In terrestrial radio broadcasting the radio waves are broadcast by a land-based radio station, while in satellite radio the radio waves are broadcast by a satellite in Earth orbit. To receive the content the listener must have a broadcast radio receiver (radio). Stations are often affiliated with a radio network which provides content in a common radio format, either in broadcast syndication or simulcast or both. Radio stations broadcast with several different types of modulation: AM radio stations transmit in AM (amplitude modulation), FM radio stations transmit in FM (frequency modulation), which are older analog audio standards, while newer digital radio stations transmit in several digital audio standards: DAB (digital audio broadcasting), HD radio, DRM (Digital Radio Mondiale). Television broadcasting is a separate service which also uses radio frequencies to broadcast television (video) signals.


Contents
1	History
2	Stations
3	Types
3.1	AM
3.1.1	Shortwave, medium wave and long wave
3.2	FM
3.3	Pirate radio
3.4	Terrestrial digital radio
4	Extensions
4.1	Satellite
5	Program formats
6	See also
7	References
8	Further reading
9	External links
History
See also: History of radio Â§ Broadcasting, and History of broadcasting

Advertisement placed in the November 5, 1919 Nieuwe Rotterdamsche Courant announcing PCGG's debut broadcast scheduled for the next evening.[1]
The earliest radio stations were radiotelegraphy systems and did not carry audio. For audio broadcasts to be possible, electronic detection and amplification devices had to be incorporated.

The thermionic valve (a kind of vacuum tube) was invented in 1904 by the English physicist John Ambrose Fleming. He developed a device he called an "oscillation valve" (because it passes current in only one direction). The heated filament, or cathode, was capable of thermionic emission of electrons that would flow to the plate (or anode) when it was at a higher voltage. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons. Later known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector.[2] This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker. However, what was still required was an amplifier.

The triode (mercury-vapor filled with a control grid) was patented on March 4, 1906, by the Austrian Robert von Lieben[3][4][5] independent from that, on October 25, 1906,[6][7] Lee De Forest patented his three-element Audion. It wasn't put to practical use until 1912 when its amplifying ability became recognized by researchers.[8]

By about 1920, valve technology had matured to the point where radio broadcasting was quickly becoming viable.[9][10] However, an early audio transmission that could be termed a broadcast may have occurred on Christmas Eve in 1906 by Reginald Fessenden, although this is disputed.[11] While many early experimenters attempted to create systems similar to radiotelephone devices by which only two parties were meant to communicate, there were others who intended to transmit to larger audiences. Charles Herrold started broadcasting in California in 1909 and was carrying audio by the next year. (Herrold's station eventually became KCBS).

In The Hague, the Netherlands, PCGG started broadcasting on November 6, 1919, making it, arguably the first commercial broadcasting station. In 1916, Frank Conrad, an electrical engineer employed at the Westinghouse Electric Corporation, began broadcasting from his Wilkinsburg, Pennsylvania garage with the call letters 8XK. Later, the station was moved to the top of the Westinghouse factory building in East Pittsburgh, Pennsylvania. Westinghouse relaunched the station as KDKA on November 2, 1920, as the first commercially licensed radio station in America.[12] The commercial broadcasting designation came from the type of broadcast license; advertisements did not air until years later. The first licensed broadcast in the United States came from KDKA itself: the results of the Harding/Cox Presidential Election. The Montreal station that became CFCF began broadcast programming on May 20, 1920, and the Detroit station that became WWJ began program broadcasts beginning on August 20, 1920, although neither held a license at the time.

In 1920, wireless broadcasts for entertainment began in the UK from the Marconi Research Centre 2MT at Writtle near Chelmsford, England. A famous broadcast from Marconi's New Street Works factory in Chelmsford was made by the famous soprano Dame Nellie Melba on June 15, 1920, where she sang two arias and her famous trill. She was the first artist of international renown to participate in direct radio broadcasts. The 2MT station began to broadcast regular entertainment in 1922. The BBC was amalgamated in 1922 and received a Royal Charter in 1926, making it the first national broadcaster in the world,[13][14] followed by Czech Radio and other European broadcasters in 1923.

Radio Argentina began regularly scheduled transmissions from the Teatro Coliseo in Buenos Aires on August 27, 1920, making its own priority claim. The station got its license on November 19, 1923. The delay was due to the lack of official Argentine licensing procedures before that date. This station continued regular broadcasting of entertainment and cultural fare for several decades.[15]

Radio in education soon followed and colleges across the U.S. began adding radio broadcasting courses to their curricula. Curry College in Milton, Massachusetts introduced one of the first broadcasting majors in 1932 when the college teamed up with WLOE in Boston to have students broadcast programs.[16]

Stations
"Radio station" redirects here. For a broader concept, see Radio communication station.
A radio broadcasting station is usually associated with wireless transmission, though in practice broadcasting transmission (sound and television) take place using both wires and radio waves. The point of this is that anyone with the appropriate receiving technology can receive the broadcast.[17]


Use of a sound broadcasting station
In line to ITU Radio Regulations (article1.61) each broadcasting station shall be classified by the service in which it operates permanently or temporarily.

Types

Transmission diagram of sound broadcasting (AM and FM)
Broadcasting by radio takes several forms. These include AM and FM stations. There are several subtypes, namely commercial broadcasting, non-commercial educational (NCE) public broadcasting and non-profit varieties as well as community radio, student-run campus radio stations, and hospital radio stations can be found throughout the world. Many stations broadcast on shortwave bands using AM technology that can be received over thousands of miles (especially at night). For example, the BBC, VOA, VOR, and Deutsche Welle have transmitted via shortwave to Africa and Asia. These broadcasts are very sensitive to atmospheric conditions and solar activity.

Nielsen Audio, formerly known as Arbitron, the United States-based company that reports on radio audiences, defines a "radio station" as a government-licensed AM or FM station; an HD Radio (primary or multicast) station; an internet stream of an existing government-licensed station; one of the satellite radio channels from XM Satellite Radio or Sirius Satellite Radio; or, potentially, a station that is not government licensed.[18]

AM
Main article: AM broadcasting

AM broadcasting stations in 2006
AM stations were the earliest broadcasting stations to be developed. AM refers to amplitude modulation, a mode of broadcasting radio waves by varying the amplitude of the carrier signal in response to the amplitude of the signal to be transmitted. The medium-wave band is used worldwide for AM broadcasting. Europe also uses the long wave band. In response to the growing popularity of FM stereo radio stations in the late 1980s and early 1990s, some North American stations began broadcasting in AM stereo, though this never gained popularity, and very few receivers were ever sold.

The signal is subject to interference from electrical storms (lightning) and other electromagnetic interference (EMI).[19] One advantage of AM radio signal is that it can be detected (turned into sound) with simple equipment. If a signal is strong enough, not even a power source is needed; building an unpowered crystal radio receiver was a common childhood project in the early decades of AM broadcasting.

AM broadcasts occur on North American airwaves in the medium wave frequency range of 525 to 1705 kHz (known as the âstandard broadcast bandâ). The band was expanded in the 1990s by adding nine channels from 1605 to 1705 kHz. Channels are spaced every 10 kHz in the Americas, and generally every 9 kHz everywhere else.

AM transmissions cannot be ionospherically propagated during the day due to strong absorption in the D-layer of the ionosphere. In a crowded channel environment, this means that the power of regional channels which share a frequency must be reduced at night or directionally beamed in order to avoid interference, which reduces the potential nighttime audience. Some stations have frequencies unshared with other stations in North America; these are called clear-channel stations. Many of them can be heard across much of the country at night. During the night, absorption largely disappears and permits signals to travel to much more distant locations via ionospheric reflections. However, fading of the signal can be severe at night.

AM radio transmitters can transmit audio frequencies up to 15 kHz (now limited to 10 kHz in the US due to FCC rules designed to reduce interference), but most receivers are only capable of reproducing frequencies up to 5 kHz or less. At the time that AM broadcasting began in the 1920s, this provided adequate fidelity for existing microphones, 78 rpm recordings, and loudspeakers. The fidelity of sound equipment subsequently improved considerably, but the receivers did not. Reducing the bandwidth of the receivers reduces the cost of manufacturing and makes them less prone to interference. AM stations are never assigned adjacent channels in the same service area. This prevents the sideband power generated by two stations from interfering with each other.[20] Bob Carver created an AM stereo tuner employing notch filtering that demonstrated that an AM broadcast can meet or exceed the 15 kHz baseband bandwidth allotted to FM stations without objectionable interference. After several years, the tuner was discontinued. Bob Carver had left the company and the Carver Corporation later cut the number of models produced before discontinuing production completely.[citation needed]

Shortwave, medium wave and long wave
See shortwave for the differences between shortwave, medium wave, and long wave spectra. Shortwave is used largely for national broadcasters, international propaganda, or religious broadcasting organizations.[21]

FM
Main article: FM broadcasting

FM radio broadcast stations in 2006
FM refers to frequency modulation, and occurs on VHF airwaves in the frequency range of 88 to 108 MHz everywhere except Japan and Russia. Russia, like the former Soviet Union, uses 65.9 to 74 MHz frequencies in addition to the world standard. Japan uses the 76 to 90 MHz frequency band.

Edwin Howard Armstrong invented FM radio to overcome the problem of radio-frequency interference (RFI), which plagued AM radio reception. At the same time, greater fidelity was made possible by spacing stations further apart in the radio frequency spectrum. Instead of 10 kHz apart, as on the AM band in the US, FM channels are 200 kHz (0.2 MHz) apart. In other countries, greater spacing is sometimes mandatory, such as in New Zealand, which uses 700 kHz spacing (previously 800 kHz). The improved fidelity made available was far in advance of the audio equipment of the 1940s, but wide interchannel spacing was chosen to take advantage of the noise-suppressing feature of wideband FM.

Bandwidth of 200 kHz is not needed to accommodate an audio signal â 20 kHz to 30 kHz is all that is necessary for a narrowband FM signal. The 200 kHz bandwidth allowed room for Â±75 kHz signal deviation from the assigned frequency, plus guard bands to reduce or eliminate adjacent channel interference. The larger bandwidth allows for broadcasting a 15 kHz bandwidth audio signal plus a 38 kHz stereo "subcarrier"âa piggyback signal that rides on the main signal. Additional unused capacity is used by some broadcasters to transmit utility functions such as background music for public areas, GPS auxiliary signals, or financial market data.

The AM radio problem of interference at night was addressed in a different way. At the time FM was set up, the available frequencies were far higher in the spectrum than those used for AM radio - by a factor of approximately 100. Using these frequencies meant that even at far higher power, the range of a given FM signal was much shorter; thus its market was more local than for AM radio. The reception range at night is the same as in the daytime. All FM broadcast transmissions are line-of-sight, and ionospheric bounce is not viable. The much larger bandwidths, compared to AM and SSB, are more susceptible to phase dispersion. Propagation speeds (celerities) are fastest in the ionosphere at the lowest sideband frequency. The celerity difference between the highest and lowest sidebands is quite apparent to the listener. Such distortion occurs up to frequencies of approximately 50 MHz. Higher frequencies do not reflect from the ionosphere, nor from storm clouds. Moon reflections have been used in some experiments, but require impractical power levels.

The original FM radio service in the U.S. was the Yankee Network, located in New England.[22][23][24] Regular FM broadcasting began in 1939 but did not pose a significant threat to the AM broadcasting industry. It required purchase of a special receiver. The frequencies used, 42 to 50 MHz, were not those used today. The change to the current frequencies, 88 to 108 MHz, began after the end of World War II and was to some extent imposed by AM broadcasters as an attempt to cripple what was by now realized to be a potentially serious threat.

FM radio on the new band had to begin from the ground floor. As a commercial venture, it remained a little-used audio enthusiasts' medium until the 1960s. The more prosperous AM stations, or their owners, acquired FM licenses and often broadcast the same programming on the FM station as on the AM station ("simulcasting"). The FCC limited this practice in the 1960s. By the 1980s, since almost all new radios included both AM and FM tuners, FM became the dominant medium, especially in cities. Because of its greater range, AM remained more common in rural environments.

Pirate radio
Main article: Pirate radio
Pirate radio is illegal or non-regulated radio transmission. It is most commonly used to describe illegal broadcasting for entertainment or political purposes. Sometimes it is used for illegal two-way radio operation. Its history can be traced back to the unlicensed nature of the transmission, but historically there has been occasional use of sea vesselsâfitting the most common perception of a pirateâas broadcasting bases. Rules and regulations vary largely from country to country, but often the term pirate radio generally describes the unlicensed broadcast of FM radio, AM radio, or shortwave signals over a wide range. In some places, radio stations are legal where the signal is transmitted, but illegal where the signals are receivedâespecially when the signals cross a national boundary. In other cases, a broadcast may be considered "pirate" due to the type of content, its transmission format, or the transmitting power (wattage) of the station, even if the transmission is not technically illegal (such as a webcast or an amateur radio transmission). Pirate radio stations are sometimes referred to as bootleg radio or clandestine stations.

Terrestrial digital radio
Main articles: Digital audio broadcasting, HD radio, ISDB, and Digital Radio Mondiale
Digital radio broadcasting has emerged, first in Europe (the UK in 1995 and Germany in 1999), and later in the United States, France, the Netherlands, South Africa, and many other countries worldwide. The simplest system is named DAB Digital Radio, for Digital Audio Broadcasting, and uses the public domain EUREKA 147 (Band III) system. DAB is used mainly in the UK and South Africa. Germany and the Netherlands use the DAB and DAB+ systems, and France uses the L-Band system of DAB Digital Radio.

The broadcasting regulators of the United States and Canada have chosen to use HD radio, an in-band on-channel system that puts digital broadcasts at frequencies adjacent to the analog broadcast. HD Radio is owned by a consortium of private companies that is called iBiquity. An international non-profit consortium Digital Radio Mondiale (DRM), has introduced the public domain DRM system, which is used by a relatively small number of broadcasters worldwide.

Extensions
Extensions of traditional radio-wave broadcasting for audio broadcasting in general include cable radio, local wire television networks, DTV radio, satellite radio, and internet radio via streaming media on the Internet.

Satellite
[icon]
This section needs expansion. You can help by adding to it. (November 2008)
Main article: Satellite radio
The enormous entry costs of space-based satellite transmitters and restrictions on available radio spectrum licenses has restricted growth of Satellite radio broadcasts. In the US and Canada, just two services, XM Satellite Radio and Sirius Satellite Radio exist. Both XM and Sirius are owned by Sirius XM Satellite Radio, which was formed by the merger of XM and Sirius on July 29, 2008, whereas in Canada, XM Radio Canada and Sirius Canada remained separate companies until 2010. Worldspace in Africa and Asia, and MobaHO! in Japan and the ROK were two unsuccessful satellite radio operators which have gone out of business.

Program formats
Main article: Radio format
Radio program formats differ by country, regulation, and markets. For instance, the U.S. Federal Communications Commission designates the 88â92 megahertz band in the U.S. for non-profit or educational programming, with advertising prohibited.

In addition, formats change in popularity as time passes and technology improves. Early radio equipment only allowed program material to be broadcast in real time, known as live broadcasting. As technology for sound recording improved, an increasing proportion of broadcast programming used pre-recorded material. A current trend is the automation of radio stations. Some stations now operate without direct human intervention by using entirely pre-recorded material sequenced by computer control.

See also
Broadcasting construction permit
Call sign
Disc jockey (DJ)
History of broadcasting
International broadcasting
List of radio topics
Low power radio station
Radio
Radio antenna
Radio network
Radio personality
RF modulation
Sports commentator
Television station
References
 "Vintage Radio Web: Philips" (vintageradio.nl)
 Guarnieri, M. (2012). "The age of vacuum tubes: Early devices and the rise of radio communications". IEEE Ind. Electron. M.: 41â43. doi:10.1109/MIE.2012.2182822.
 Schmidt, Hans-Thomas. "Die LiebenrÃ¶hre". Umleitung zur Homepage von H.-T. Schmidt (in German). Retrieved August 10, 2019. DRP 179807
 Tapan K. Sarkar (ed.) "History of wireless", John Wiley and Sons, 2006. ISBN 0-471-71814-9, p.335
 SÅgo Okamura (ed), History of Electron Tubes, IOS Press, 1994 ISBN 90-5199-145-2 page 20
 "US841387A - Device for amplifying feeble electrical currents". Google Patents. October 25, 1906. Retrieved August 10, 2019.
 "US879532A - Space telegraphy". Google Patents. January 29, 1907. Retrieved August 10, 2019.
 Nebeker, Frederik (2009). Dawn of the Electronic Age: Electrical Technologies in the Shaping of the Modern World, 1914 to 1945. John Wiley & Sons. pp. 14â15. ISBN 978-0470409749.
 "Making the Modern World - Mass consumption". webarchive.nationalarchives.gov.uk. Archived from the original on April 5, 2017.
 Guarnieri, M. (2012). "The age of vacuum tubes: the conquest of analog communications". IEEE Ind. Electron. M.: 52â54. doi:10.1109/MIE.2012.2193274.
 Fessenden â The Next Chapter RWonline.com Archived September 16, 2009, at the Wayback Machine
 Baudino, Joseph E; John M. Kittross (Winter 1977). "Broadcasting's Oldest Stations: An Examination of Four Claimants". Journal of Broadcasting. 21: 61â82. doi:10.1080/08838157709363817. Archived from the original on March 6, 2008. Retrieved January 18, 2013.
 "CARS - Marconi Hall Street, New Street and 2MT callsign". www.g0mwt.org.uk.
 "BBC History â The BBC takes to the Airwaves". BBC News.
 Atgelt, Carlos A. "Early History of Radio Broadcasting in Argentina." The Broadcast Archive (Oldradio.com).
 "Curry College - Home". www.curry.edu. Retrieved July 13, 2018.
 Neira, Bob. "Broadcasting". modestoradiomuseum. modestoradiomuseum.
 "What is a Radio Station?". Radio World. p. 6.
 Based on the "interference" entry of The Concise Oxford English Dictionary, 11th edition, online
 "Types of Technology, FM vs AM". kwarner.bravehost.com. July 13, 2012. Archived from the original on July 13, 2012. Retrieved August 10, 2019.
 Grodkowski, Paul (August 24, 2015). Beginning Shortwave Radio Listening. Booktango. ISBN 9781468964240.
 Halper, Donna L. "John Shepard's FM StationsâAmerica's first FM network." Boston Radio Archives (BostonRadio.org).
 "The Yankee Network in 1936". The Archives @ BostonRadio.org. Retrieved August 10, 2019.
 "FM Broadcasting Chronology". Jeff Miller Pages. June 23, 2017. Retrieved August 10, 2019.
Further reading
Briggs Asa. The History of Broadcasting in the United Kingdom (Oxford University Press, 1961).
Crisell, Andrew. An Introductory History of British Broadcasting (2002) excerpt
Ewbank Henry and Lawton Sherman P. Broadcasting: Radio and Television (Harper & Brothers, 1952).
Fisher, Marc. Something In The Air: Radio, Rock, and the Revolution That Shaped A Generation (Random House, 2007).
Hausman, Carl, Messere, Fritz, Benoit, Philip, and O'Donnell, Lewis, Modern Radio Production, 9th ed., (Cengage, 2013)
Head, Sydney W., Christopher W. Sterling, and Lemuel B. Schofield. Broadcasting in America." (7th ed. 1994).
Lewis, Tom, Empire of the Air: The Men Who Made Radio, 1st ed., New York : E. Burlingame Books, 1991. ISBN 0-06-018215-6. "Empire of the Air: The Men Who Made Radio" (1992) by Ken Burns was a PBS documentary based on the book.
Pilon, Robert, Isabelle Lamoureux, and Gilles Turcotte. Le MarchÃ© de la radio au QuÃ©bec: document de reference. [MontrÃ©al]: Association quÃ©bÃ©coise de l'industrie du dique, du spectacle et de la video, 1991. unpaged. N.B.: Comprises: Robert Pilon's and Isabelle Lamoureux' Profil du marchÃ© de radio au QuÃ©bec: un analyse de MÃ©dia-culture. -- Gilles Turcotte's Analyse comparative de l'Ã©coute des principals stations de MontrÃ©al: prepare par Info Cible.
Ray, William B. FCC: The Ups and Downs of Radio-TV Regulation (Iowa State University Press, 1990).
Russo, Alexan der. Points on the Dial: Golden Age Radio Beyond the Networks (Duke University Press; 2010) 278 pages; discusses regional and local radio as forms that "complicate" the image of the medium as a national unifier from the 1920s to the 1950s.
Scannell, Paddy, and Cardiff, David. A Social History of British Broadcasting, Volume One, 1922-1939 (Basil Blackwell, 1991).
Schramm, Wilbur, ed. The Process and Effects of Mass Communication (1955 and later editions) articles by social scientists
Schramm, Wilbur, ed. Mass Communication (1950, 2nd ed. 1960); more popular essays
Schwoch James. The American Radio Industry and Its Latin American Activities, 1900-1939 (University of Illinois Press, 1990).
Stewart, Sandy. From Coast to Coast: a Personal History of Radio in Canada (Entreprises Radio-Canada, 1985). xi, 191 p., ill., chiefly with b&w photos. ISBN 0-88794-147-8
Stewart, Sandy. A Pictorial History of Radio in Canada (Gage Publishing, 1975). v, [1], 154 p., amply ill. in b&w mostly with photos. SBN 7715-9948-X
White Llewellyn. The American Radio (University of Chicago Press, 1947).
External links
	Look up radio broadcasting in Wiktionary, the free dictionary.
General
Federal Communications Commission website - fcc.gov
DXing.info - Information about radio stations worldwide
Radio-Locator.com- Links to 13,000 radio stations worldwide
BBC reception advice
DXradio.50webs.com "The SWDXER" - with general SWL information and radio antenna tips
RadioStationZone.com - 10.000+ radio stations worldwide with ratings, comments and listen live links
Online-Radio-Stations.org - The Web Radio Tuner has a comprehensive list of over 50.000 radio stations
UnwantedEmissions.com - A general reference to radio spectrum allocations
Radio stanice - Search for radio stations throughout the Europe
Radio Emisoras Latinas - has a directory with thousands of Latin America Radio Stations
MY FM Radio Live - MY FM Radio Live - Internet radio broadcast
vte
Broadcasting
Links to related articles
Categories: Radio broadcasting
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
ÙØ§Ø±Ø³Û
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
à®¤à®®à®¿à®´à¯
TÃ¼rkÃ§e
Ø§Ø±Ø¯Ù
14 more
Edit links
This page was last edited on 26 October 2020, at 19:38 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

FM broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from FM radio)
Jump to navigationJump to search

AM and FM modulated signals for radio. AM (amplitude modulation) and FM (frequency modulation) are types of modulation (coding). The sound of the program material, usually coming from a radio studio, is used to modulate (vary) a carrier wave of a specific frequency, then broadcast.

In AM broadcasting, the amplitude of the carrier wave is modulated to encode the original sound. In FM broadcasting, the frequency of the carrier wave is modulated to encode the sound. A radio receiver extracts the original program sound from the modulated radio signal and reproduces the sound in a loudspeaker.

Position of FM radio in the electromagnetic spectrum

A commercial 35 kW FM radio transmitter built in the late 1980s. It belongs to FM radio station KWNR in Henderson, Nevada and broadcasts at a frequency of 95.5 MHz.
FM broadcasting is a method of radio broadcasting using frequency modulation (FM). Invented in 1933 by American engineer Edwin Armstrong, wide-band FM is used worldwide to provide high fidelity sound over broadcast radio. FM broadcasting is capable of higher fidelityâthat is, more accurate reproduction of the original program soundâthan other broadcasting technologies, such as AM broadcasting. Therefore, FM is used for most broadcasts of music or general audio (in the audio spectrum). FM radio stations use the very high frequency range of radio frequencies.


Contents
1	Broadcast bands
2	Technology
2.1	Modulation
2.2	Pre-emphasis and de-emphasis
2.3	Stereo FM
2.4	Quadraphonic FM
2.5	Noise reduction
2.6	Other subcarrier services
2.7	Transmission power
2.8	Reception distance
3	History
3.1	United States
3.2	Europe
3.2.1	United Kingdom
3.2.2	Italy
3.2.3	Greece
3.3	Australia
3.4	New Zealand
3.5	Trinidad and Tobago
3.6	Turkey
3.7	Other countries
3.8	ITU Conferences about FM
4	FM broadcasting switch-off
5	Small-scale use of the FM broadcast band
5.1	Consumer use of FM transmitters
5.2	Assistive listening
5.3	Microbroadcasting
5.4	Clandestine use of FM transmitters
6	See also
6.1	FM broadcasting by country
6.2	FM broadcasting (technical)
6.3	Lists
6.4	History
6.5	Bands
7	References
8	External links
Broadcast bands
Main article: FM broadcast band
Throughout the world, the FM broadcast band falls within the VHF part of the radio spectrum. Usually 87.5 to 108.0 MHz is used,[1] or some portion thereof, with few exceptions:

In the former Soviet republics, and some former Eastern Bloc countries, the older 65.8â74 MHz band is also used. Assigned frequencies are at intervals of 30 kHz. This band, sometimes referred to as the OIRT band, is slowly being phased out. Where the OIRT band is used, the 87.5â108.0 MHz band is referred to as the CCIR band.
In Japan, the band 76â95 MHz is used.
The frequency of an FM broadcast station (more strictly its assigned nominal center frequency) is usually a multiple of 100 kHz. In most of South Korea, the Americas, the Philippines and the Caribbean, only odd multiples are used. Some other countries follow this plan because of the import of vehicles, principally from the United States, with radios that can only tune to these frequencies. In some parts of Europe, Greenland and Africa, only even multiples are used. In the UK odd or even are used. In Italy, multiples of 50 kHz are used. In most countries the maximum permitted frequency error of the unmodulated carrier is specified, which typically should be within 2000 Hz of the assigned frequency.[2][3]

There are other unusual and obsolete FM broadcasting standards in some countries, with non-standard spacings of 1, 10, 30, 74, 500, and 300 kHz. However, to minimise inter-channel interference, stations operating from the same or geographically close transmitter sites tend to keep to at least a 500 kHz frequency separation even when closer frequency spacing is technically permitted. The ITU publishes Protection Ratio graphs which give the minimum spacing between frequencies based on their relative strengths.[4] Only broadcast stations with large enough geographic separations between their coverage areas can operate on close or the same frequencies.

Technology

FM has better rejection of static (RFI) than AM. This was shown in a dramatic demonstration by General Electric at its New York lab in 1940. The radio had both AM and FM receivers. With a million-volt arc as a source of interference behind it, the AM receiver produced a roar of static, while the FM receiver clearly reproduced a music program from Armstrong's experimental FM transmitter in New Jersey.

Crossed dipole antenna of station KENZ's 94.9 MHz, 48 kW transmitter on Lake Mountain, Utah. It radiates circularly polarized radio waves.
Modulation
Frequency modulation or FM is a form of modulation which conveys information by varying the frequency of a carrier wave; the older amplitude modulation or AM varies the amplitude of the carrier, with its frequency remaining constant. With FM, frequency deviation from the assigned carrier frequency at any instant is directly proportional to the amplitude of the input signal, determining the instantaneous frequency of the transmitted signal. Because transmitted FM signals use more bandwidth than AM signals, this form of modulation is commonly used with the higher (VHF or UHF) frequencies used by TV, the FM broadcast band, and land mobile radio systems.

The maximum frequency deviation of the carrier is usually specified and regulated by the licensing authorities in each country. For a stereo broadcast, the maximum permitted carrier deviation is invariably Â±75 kHz, although a little higher is permitted in the United States when SCA systems are used. For a monophonic broadcast, again the most common permitted maximum deviation is Â±75 kHz. However, some countries specify a lower value for monophonic broadcasts, such as Â±50 kHz.[5]


Armstrong's first prototype FM broadcast transmitter, located in the Empire State Building, New York City, which he used for secret tests of his system between 1934 and 1935. Licensed as experimental station W2XDG, it transmitted on 41 MHz at a power of 2 kW

Instantaneous spectrum and waterfall plot in the FM broadcast band showing three strong local stations; speech and music show different patterns of frequency vs. time. When the transmitted audio is quiet, the 19 kHz stereo pilot tones can be resolved in the spectrum.
Pre-emphasis and de-emphasis
Random noise has a triangular spectral distribution in an FM system, with the effect that noise occurs predominantly at the highest audio frequencies within the baseband. This can be offset, to a limited extent, by boosting the high frequencies before transmission and reducing them by a corresponding amount in the receiver. Reducing the high audio frequencies in the receiver also reduces the high-frequency noise. These processes of boosting and then reducing certain frequencies are known as pre-emphasis and de-emphasis, respectively.

The amount of pre-emphasis and de-emphasis used is defined by the time constant of a simple RC filter circuit. In most of the world a 50 Âµs time constant is used. In the Americas and South Korea, 75 Âµs is used.[6] This applies to both mono and stereo transmissions. For stereo, pre-emphasis is applied to the left and right channels before multiplexing.

The use of pre-emphasis becomes a problem because of the fact that many forms of contemporary music contain more high-frequency energy than the musical styles which prevailed at the birth of FM broadcasting. Pre-emphasizing these high-frequency sounds would cause excessive deviation of the FM carrier. Modulation control (limiter) devices are used to prevent this. Systems more modern than FM broadcasting tend to use either programme-dependent variable pre-emphasis; e.g., dbx in the BTSC TV sound system, or none at all.

Pre-emphasis and de-emphasis was used in the earliest days of FM broadcasting. According to a BBC report from 1946,[7] 100 Âµs was originally considered in the US, but 75 Âµs subsequently adopted.

Stereo FM
Long before FM stereo transmission was considered, FM multiplexing of other types of audio level information was experimented with.[8] Edwin Armstrong who invented FM was the first to experiment with multiplexing, at his experimental 41 MHz station W2XDG located on the 85th floor of the Empire State Building in New York City.

These FM multiplex transmissions started in November 1934 and consisted of the main channel audio program and three subcarriers: a fax program, a synchronizing signal for the fax program and a telegraph "order" channel. These original FM multiplex subcarriers were amplitude modulated.

Two musical programs, consisting of both the Red and Blue Network program feeds of the NBC Radio Network, were simultaneously transmitted using the same system of subcarrier modulation as part of a studio-to-transmitter link system. In April 1935, the AM subcarriers were replaced by FM subcarriers, with much improved results.

Show more
Show WhitespaceHide Whitespace
Actual Output
Output:

Flow control (data)
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Not to be confused with Control flow.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.

Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.


Contents
1	Stop-and-wait
1.1	Operations
1.2	Pros and cons of stop and wait
2	Sliding Window
2.1	Go Back N
2.2	Selective Repeat
3	Comparison
3.1	Stop-and-Wait
3.2	Selective Repeat
4	Transmit flow control
4.1	Hardware flow control
4.2	Software flow control
5	Open-loop flow control
6	Closed-loop flow control
7	See also
8	References
9	External links
Stop-and-wait
Main article: Stop-and-wait ARQ
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.

Operations
Sender: Transmits a single frame at a time.
Sender waits to receive ACK within time out.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Go to step 1 when ACK is received, or time out is hit.
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).

The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.

Pros and cons of stop and wait
Pros

The only advantage of this method of flow control is its simplicity.

Cons

The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]

Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]

Sliding Window
Main article: Sliding Window Protocol
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]

Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.

Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.

Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.

Go Back N
Main article: Go-Back-N ARQ
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.

The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.

To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]

Selective Repeat
Main article: Selective Repeat ARQ
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]

Comparison
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.

Stop-and-Wait
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]

With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]

Selective Repeat
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]

Transmit flow control
Transmit flow control may occur:

between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be

either stop-and-wait,
or use a sliding window.
Flow control can be performed

either by control signal lines in a data communication interface (see serial port and RS-232),
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
Hardware flow control
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:

RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:

In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.

Polarity:
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
Software flow control
Main article: Software flow control
Conversely, XON/XOFF is usually referred to as software flow control.

Open-loop flow control
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.

Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]

Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.

Closed-loop flow control
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.

This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:

A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.

In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.

The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.

See also
Software flow control
Computer networking
Traffic contract
Congestion control
Teletraffic engineering in broadband networks
Teletraffic engineering
Ethernet flow control
Handshaking
References
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 Webster Dictionary definition last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
Sliding window:

[1] last accessed 27 November 2012.
External links
RS-232 flow control and handshaking
Categories: Flow control (data)Network performanceLogical link controlData transmission
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
Deutsch
ÙØ§Ø±Ø³Û
FranÃ§ais
íêµ­ì´
Italiano
æ¥æ¬èª
Ð ÑÑÑÐºÐ¸Ð¹
4 more
Edit links
This page was last edited on 18 July 2020, at 11:55 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Data transmission
From Wikipedia, the free encyclopedia
  (Redirected from Data communications)
Jump to navigationJump to search
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.

Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.

Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.


Contents
1	Distinction between related subjects
2	Protocol layers and sub-topics
3	Applications and history
4	Serial and parallel transmission
5	Communication channels
6	Asynchronous and synchronous data transmission
7	See also
8	References
Distinction between related subjects
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.

Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.

The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]

The theoretical aspects of data transmission are covered by information theory and coding theory.

Protocol layers and sub-topics
OSI model
by layer
7.  Application layer[show]
6.  Presentation layer[show]
5.  Session layer[show]
4.  Transport layer[show]
3.  Network layer[show]
2.  Data link layer[show]
1.  Physical layer[show]
vte
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:

Layer 1, the physical layer:
Channel coding including
Digital modulation schemes
Line coding schemes
Forward error correction (FEC) codes
Bit synchronization
Multiplexing
Equalization
Channel models
Layer 2, the data link layer:
Channel access schemes, media access control (MAC)
Packet mode communication and Frame synchronization
Error detection and automatic repeat request (ARQ)
Flow control
Layer 6, the presentation layer:
Source coding (digitization and data compression), and information theory.
Cryptography (may occur at any layer)
It is also common to deal with the cross-layer design of those three layers.[6]

Applications and history
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.

Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.

Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.

In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.

Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.

Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.

The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.

Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.

While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.

Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.

Serial and parallel transmission
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.

In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.

Communication channels
Main article: communication channel
Some communications channel types include:

Data transmission circuit
Full-duplex
Half-duplex
Multi-drop:
Bus network
Mesh network
Ring network
Star network
Wireless network
Point-to-point
Simplex
Asynchronous and synchronous data transmission
Main article: Comparison of synchronous and asynchronous signalling
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.

Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.

See also
Computer networking
Communication
Data migration
Information theory
Media (communication)
Network security
Node-to-node data transfer
Packet switching
Signal processing
Telecommunication
Transmission (disambiguation)
References
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
28 more
Edit links
This page was last edited on 30 October 2020, at 04:19 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Modulation
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For other uses, see Modulation (disambiguation).

This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
Passband modulation
Analog modulation
AMFMPMQAMSMSSB
Digital modulation
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
Hierarchical modulation
QAMWDM
Spread spectrum
CSSDSSSFHSSTHSS
See also
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
vte
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.

Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.

In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.

In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]

A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.

The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).

The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.

The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.


Contents
1	Analog modulation methods
2	Digital modulation methods
2.1	Fundamental digital modulation methods
2.2	Modulator and detector principles of operation
2.3	List of common digital modulation techniques
2.4	Automatic digital modulation recognition (ADMR)
2.5	Digital baseband modulation or line coding
3	Pulse modulation methods
4	Miscellaneous modulation techniques
5	See also
6	References
7	Further reading
8	External links
Analog modulation methods

A low-frequency message signal (top) may be carried by an AM or FM radio wave.

Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.

A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:

Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Double-sideband modulation (DSB)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband reduced carrier transmission (DSB-RC)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Vestigial sideband modulation (VSB, or VSB-AM)
Quadrature amplitude modulation (QAM)
Angle modulation, which is approximately constant envelope
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Digital modulation methods
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).


Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.

According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.

Fundamental digital modulation methods
The most fundamental digital modulation techniques are based on keying:

PSK (phase-shift keying): a finite number of phases are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.

In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.

If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.

For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.

In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.

Modulator and detector principles of operation
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).

These are the general steps used by the modulator to transmit data:

Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
At the receiver side, the demodulator typically performs:

Bandpass filtering.
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Parallel-to-serial conversion of the codewords into a bit stream.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.

Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.

List of common digital modulation techniques
The most common digital modulation techniques are:

Phase-shift keying (PSK)
Binary PSK (BPSK), using M=2 symbols
Quadrature PSK (QPSK), using M=4 symbols
8PSK, using M=8 symbols
16PSK, using M=16 symbols
Differential PSK (DPSK)
Differential QPSK (DQPSK)
Offset QPSK (OQPSK)
Ï/4âQPSK
Frequency-shift keying (FSK)
Audio frequency-shift keying (AFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Dual-tone multi-frequency (DTMF)
Amplitude-shift keying (ASK)
On-off keying (OOK), the most common ASK form
M-ary vestigial sideband modulation, for example 8VSB
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Continuous phase modulation (CPM) methods
Minimum-shift keying (MSK)
Gaussian minimum-shift keying (GMSK)
Continuous-phase frequency-shift keying (CPFSK)
Orthogonal frequency-division multiplexing (OFDM) modulation
Discrete multitone (DMT), including adaptive modulation and bit-loading
Wavelet modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Spread-spectrum techniques
Direct-sequence spread spectrum (DSSS)
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).

OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.

Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.

Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]

There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.

Digital baseband modulation or line coding
Main article: Line code
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]

vte
Line coding (digital baseband transmission)
Pulse modulation methods
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.

Analog-over-analog methods

Pulse-amplitude modulation (PAM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-position modulation (PPM)
Analog-over-digital methods

Pulse-code modulation (PCM)
Differential PCM (DPCM)
Adaptive DPCM (ADPCM)
Delta modulation (DM or Î-modulation)
Delta-sigma modulation (âÎ)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Pulse-density modulation (PDM)
Miscellaneous modulation techniques
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
Adaptive modulation
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
See also
	Wikimedia Commons has media related to Modulation.
Channel access methods
Channel coding
Codec
Communications channel
Demodulation
Electrical resonance
Heterodyne
Line code
Mechanically induced modulation
Modem
Modulation order
Neuromodulation
RF modulator
Ring modulation
Telecommunication
Types of radio emissions
References

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
Further reading
Mul
Error output:No output
larger file, potato network (duplicate test 5 for partial credit) (1MB/6s)
0/1
Student Output


Flow control (data)
Flow control (data)
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Jump to navigationJump to search
Not to be confused with Control flow.
Not to be confused with Control flow.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.
In data communications, flow control is the process of managing the rate of data transmission between two nodes to prevent a fast sender from overwhelming a slow receiver. It provides a mechanism for the receiver to control the transmission speed, so that the receiving node is not overwhelmed with data from transmitting node. Flow control should be distinguished from congestion control, which is used for controlling the flow of data when congestion has actually occurred.[1] Flow control mechanisms can be classified by whether or not the receiving node sends feedback to the sending node.


Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.
Flow control is important because it is possible for a sending computer to transmit information at a faster rate than the destination computer can receive and process it. This can happen if the receiving computers have a heavy traffic load in comparison to the sending computer, or if the receiving computer has less processing power than the sending computer.




Contents
Contents
1	Stop-and-wait
1	Stop-and-wait
1.1	Operations
1.1	Operations
1.2	Pros and cons of stop and wait
1.2	Pros and cons of stop and wait
2	Sliding Window
2	Sliding Window
2.1	Go Back N
2.1	Go Back N
2.2	Selective Repeat
2.2	Selective Repeat
3	Comparison
3	Comparison
3.1	Stop-and-Wait
3.1	Stop-and-Wait
3.2	Selective Repeat
3.2	Selective Repeat
4	Transmit flow control
4	Transmit flow control
4.1	Hardware flow control
4.1	Hardware flow control
4.2	Software flow control
4.2	Software flow control
5	Open-loop flow control
5	Open-loop flow control
6	Closed-loop flow control
6	Closed-loop flow control
7	See also
7	See also
8	References
8	References
9	External links
9	External links
Stop-and-wait
Stop-and-wait
Main article: Stop-and-wait ARQ
Main article: Stop-and-wait ARQ
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.
Stop-and-wait flow control is the simplest form of flow control. In this method the message is broken into multiple frames, and the receiver indicates its readiness to receive a frame of data. The sender waits for a receipt acknowledgement (ACK) after every frame for a specified time (called a time out). The receiver sends the ACK to let the sender know that the frame of data was received correctly. The sender will then send the next frame only after the ACK.


Operations
Operations
Sender: Transmits a single frame at a time.
Sender: Transmits a single frame at a time.
Sender waits to receive ACK within time out.
Sender waits to receive ACK within time out.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Receiver: Transmits acknowledgement (ACK) as it receives a frame.
Go to step 1 when ACK is received, or time out is hit.
Go to step 1 when ACK is received, or time out is hit.
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).
If a frame or ACK is lost during transmission then the frame is re-transmitted. This re-transmission process is known as ARQ (automatic repeat request).


The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.
The problem with Stop-and-wait is that only one frame can be transmitted at a time, and that often leads to inefficient transmission, because until the sender receives the ACK it cannot transmit any new packet. During this time both the sender and the channel are unutilised.


Pros and cons of stop and wait
Pros and cons of stop and wait
Pros
Pros


The only advantage of this method of flow control is its simplicity.
The only advantage of this method of flow control is its simplicity.


Cons
Cons


The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]
The sender needs to wait for the ACK after every frame it transmits. This is a source of inefficiency, and is particularly bad when the propagation delay is much longer than the transmission delay.[2]


Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]
Stop and wait can also create inefficiencies when sending longer transmissions.[3] When longer transmissions are sent there is more likely chance for error in this protocol. If the messages are short the errors are more likely to be detected early. More inefficiency is created when single messages are broken into separate frames because it makes the transmission longer.[4]


Sliding Window
Sliding Window
Main article: Sliding Window Protocol
Main article: Sliding Window Protocol
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]
A method of flow control in which a receiver gives a transmitter permission to transmit data until a window is full. When the window is full, the transmitter must stop transmitting until the receiver advertises a larger window.[5]


Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.
Sliding-window flow control is best utilized when the buffer size is limited and pre-established. During a typical communication between a sender and a receiver the receiver allocates buffer space for n frames (n is the buffer size in frames). The sender can send and the receiver can accept n frames without having to wait for an acknowledgement. A sequence number is assigned to frames in order to help keep track of those frames which did receive an acknowledgement. The receiver acknowledges a frame by sending an acknowledgement that includes the sequence number of the next frame expected. This acknowledgement announces that the receiver is ready to receive n frames, beginning with the number specified. Both the sender and receiver maintain what is called a window. The size of the window is less than or equal to the buffer size.


Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.
Sliding window flow control has far better performance than stop-and-wait flow control. For example, in a wireless environment if data rates are low and noise level is very high, waiting for an acknowledgement for every packet that is transferred is not very feasible. Therefore, transferring data as a bulk would yield a better performance in terms of higher throughput.


Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.
Sliding window flow control is a point to point protocol assuming that no other entity tries to communicate until the current data transfer is complete. The window maintained by the sender indicates which frames it can send. The sender sends all the frames in the window and waits for an acknowledgement (as opposed to acknowledging after every frame). The sender then shifts the window to the corresponding sequence number, thus indicating that frames within the window starting from the current sequence number can be sent.


Go Back N
Go Back N
Main article: Go-Back-N ARQ
Main article: Go-Back-N ARQ
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.
An automatic repeat request (ARQ) algorithm, used for error correction, in which a negative acknowledgement (NAK) causes retransmission of the word in error as well as the next Nâ1 words. The value of N is usually chosen such that the time taken to transmit the N words is less than the round trip delay from transmitter to receiver and back again. Therefore, a buffer is not needed at the receiver.


The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.
The normalized propagation delay (a) = âpropagation time (Tp)âtransmission time (Tt), where Tp = Length (L) over propagation velocity (V) and Tt = bitrate (r) over Framerate (F). So that a =âLFâVr.


To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]
To get the utilization you must define a window size (N). If N is greater than or equal to 2a + 1 then the utilization is 1 (full utilization) for the transmission channel. If it is less than 2a + 1 then the equation âNâ1+2a must be used to compute utilization.[6]


Selective Repeat
Selective Repeat
Main article: Selective Repeat ARQ
Main article: Selective Repeat ARQ
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]
Selective Repeat is a connection oriented protocol in which both transmitter and receiver have a window of sequence numbers. The protocol has a maximum number of messages that can be sent without acknowledgement. If this window becomes full, the protocol is blocked until an acknowledgement is received for the earliest outstanding message. At this point the transmitter is clear to send more messages.[7]


Comparison
Comparison
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.
This section is geared towards the idea of comparing Stop-and-wait, Sliding Window with the subsets of Go Back N and Selective Repeat.


Stop-and-Wait
Stop-and-Wait
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]
Error free: {\displaystyle {\frac {1}{2a+1}}}{\displaystyle {\frac {1}{2a+1}}}.[citation needed]


With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]
With errors: {\displaystyle {\frac {1-P}{2a+1}}}{\displaystyle {\frac {1-P}{2a+1}}}.[citation needed]


Selective Repeat
Selective Repeat
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]
We define throughput T as the average number of blocks communicated per transmitted block. It is more convenient to calculate the average number of transmissions necessary to communicate a block, a quantity we denote by 0, and then to determine T from the equation {\displaystyle T={\frac {1}{b}}}{\displaystyle T={\frac {1}{b}}}.[citation needed]


Transmit flow control
Transmit flow control
Transmit flow control may occur:
Transmit flow control may occur:


between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
between data terminal equipment (DTE) and a switching center, via data circuit-terminating equipment (DCE), the opposite types interconnected straightforwardly,
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
or between two devices of the same type (two DTEs, or two DCEs), interconnected by a crossover cable.
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be
The transmission rate may be controlled because of network or DTE requirements. Transmit flow control can occur independently in the two directions of data transfer, thus permitting the transfer rates in one direction to be different from the transfer rates in the other direction. Transmit flow control can be


either stop-and-wait,
either stop-and-wait,
or use a sliding window.
or use a sliding window.
Flow control can be performed
Flow control can be performed


either by control signal lines in a data communication interface (see serial port and RS-232),
either by control signal lines in a data communication interface (see serial port and RS-232),
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
or by reserving in-band control characters to signal flow start and stop (such as the ASCII codes for XON/XOFF).
Hardware flow control
Hardware flow control
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:
In common RS-232 there are pairs of control lines which are usually referred to as hardware flow control:


RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
RTS (Request To Send) and CTS (Clear To Send), used in RTS flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
DTR (Data Terminal Ready) and DSR (Data Set Ready), DTR flow control
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:
Hardware flow control is typically handled by the DTE or "master end", as it is first raising or asserting its line to command the other side:


In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
In the case of RTS control flow, DTE sets its RTS, which signals the opposite end (the slave end such as a DCE) to begin monitoring its data input line. When ready for data, the slave end will raise its complementary line, CTS in this example, which signals the master to start sending data, and for the master to begin monitoring the slave's data output line. If either end needs to stop the data, it lowers its respective "data readiness" line.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
For PC-to-modem and similar links, in the case of DTR flow control, DTR/DSR are raised for the entire modem session (say a dialup internet call where DTR is raised to signal the modem to dial, and DSR is raised by the modem when the connection is complete), and RTS/CTS are raised for each block of data.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.
An example of hardware flow control is a Half-duplex radio modem to computer interface. In this case, the controlling software in the modem and computer may be written to give priority to incoming radio signals such that outgoing data from the computer is paused by lowering CTS if the modem detects a reception.


Polarity:
Polarity:
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
RS-232 level signals are inverted by the driver ICs, so line polarity is TxD-, RxD-, CTS+, RTS+ (Clear to send when HI, Data 1 is a LO)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
for microprocessor pins the signals are TxD+, RxD+, CTS-, RTS- (Clear to send when LO, Data 1 is a HI)
Software flow control
Software flow control
Main article: Software flow control
Main article: Software flow control
Conversely, XON/XOFF is usually referred to as software flow control.
Conversely, XON/XOFF is usually referred to as software flow control.


Open-loop flow control
Open-loop flow control
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.
The open-loop flow control mechanism is characterized by having no feedback between the receiver and the transmitter. This simple means of control is widely used. The allocation of resources must be a "prior reservation" or "hop-to-hop" type.


Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]
Open-loop flow control has inherent problems with maximizing the utilization of network resources. Resource allocation is made at connection setup using a CAC (Connection Admission Control) and this allocation is made using information that is already "old news" during the lifetime of the connection. Often there is an over-allocation of resources and reserved but unused capacities are wasted. Open-loop flow control is used by ATM in its CBR, VBR and UBR services (see traffic contract and congestion control).[1]


Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.
Open-loop flow control incorporates two controls; the controller and a regulator. The regulator is able to alter the input variable in response to the signal from the controller. An open-loop system has no feedback or feed forward mechanism, so the input and output signals are not directly related and there is increased traffic variability. There is also a lower arrival rate in such system and a higher loss rate. In an open control system, the controllers can operate the regulators at regular intervals, but there is no assurance that the output variable can be maintained at the desired level. While it may be cheaper to use this model, the open-loop model can be unstable.


Closed-loop flow control
Closed-loop flow control
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.
The closed-loop flow control mechanism is characterized by the ability of the network to report pending network congestion back to the transmitter. This information is then used by the transmitter in various ways to adapt its activity to existing network conditions. Closed-loop flow control is used by ABR (see traffic contract and congestion control).[1] Transmit flow control described above is a form of closed-loop flow control.


This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:
This system incorporates all the basic control elements, such as, the sensor, transmitter, controller and the regulator. The sensor is used to capture a process variable. The process variable is sent to a transmitter which translates the variable to the controller. The controller examines the information with respect to a desired value and initiates a correction action if required. The controller then communicates to the regulator what action is needed to ensure that the output variable value is matching the desired value. Therefore, there is a high degree of assurance that the output variable can be maintained at the desired level. The closed-loop control system can be a feedback or a feed forward system:


A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.
A feedback closed-loop system has a feed-back mechanism that directly relates the input and output signals. The feed-back mechanism monitors the output variable and determines if additional correction is required. The output variable value that is fed backward is used to initiate that corrective action on a regulator. Most control loops in the industry are of the feedback type.


In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.
In a feed-forward closed loop system, the measured process variable is an input variable. The measured signal is then used in the same fashion as in a feedback system.


The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.
The closed-loop model produces lower loss rate and queuing delays, as well as it results in congestion-responsive traffic. The closed-loop model is always stable, as the number of active lows is bounded.


See also
See also
Software flow control
Software flow control
Computer networking
Computer networking
Traffic contract
Traffic contract
Congestion control
Congestion control
Teletraffic engineering in broadband networks
Teletraffic engineering in broadband networks
Teletraffic engineering
Teletraffic engineering
Ethernet flow control
Ethernet flow control
Handshaking
Handshaking
References
References
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 Network Testing Solutions, ATM Traffic Management White paper last accessed 15 March 2005.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 "ERROR CONTROL" (PDF). 28 September 2005. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 arun (20 November 2012). "Flow Control Techniques". angelfire.com. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 "last accessed 1 December 2012". people.bridgewater.edu. 1 December 2012. Retrieved 10 November 2018.
 Webster Dictionary definition last accessed 3 December 2012.
 Webster Dictionary definition last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Focal Dictionary of Telecommunications, Focal Press last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
 Data Transmission over Adpative HF Radio Communication Systems using Selective Repeat Protocol last accessed 3 December 2012.
Sliding window:
Sliding window:


[1] last accessed 27 November 2012.
[1] last accessed 27 November 2012.
External links
External links
RS-232 flow control and handshaking
RS-232 flow control and handshaking
Categories: Flow control (data)Network performanceLogical link controlData transmission
Categories: Flow control (data)Network performanceLogical link controlData transmission
Navigation menu
Navigation menu
Not logged inTalkContributionsCreate accountLog in
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ArticleTalk
ReadEditView historySearch
ReadEditView historySearch
Search Wikipedia
Search Wikipedia
Main page
Main page
Contents
Contents
Current events
Current events
Random article
Random article
About Wikipedia
About Wikipedia
Contact us
Contact us
Donate
Donate
Contribute
Contribute
Help
Help
Learn to edit
Learn to edit
Community portal
Community portal
Recent changes
Recent changes
Upload file
Upload file
Tools
Tools
What links here
What links here
Related changes
Related changes
Special pages
Special pages
Permanent link
Permanent link
Page information
Page information
Cite this page
Cite this page
Wikidata item
Wikidata item
Print/export
Print/export
Download as PDF
Download as PDF
Printable version
Printable version


Languages
Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Ø§ÙØ¹Ø±Ø¨ÙØ©
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
ÐÑÐ»Ð³Ð°ÑÑÐºÐ¸
Deutsch
Deutsch
ÙØ§Ø±Ø³Û
ÙØ§Ø±Ø³Û
FranÃ§ais
FranÃ§ais
íêµ­ì´
íêµ­ì´
Italiano
Italiano
æ¥æ¬èª
æ¥æ¬èª
Ð ÑÑÑÐºÐ¸Ð¹
Ð ÑÑÑÐºÐ¸Ð¹
4 more
4 more
Edit links
Edit links
This page was last edited on 18 July 2020, at 11:55 (UTC).
This page was last edited on 18 July 2020, at 11:55 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki


Data transmission
Data transmission
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
  (Redirected from Data communications)
  (Redirected from Data communications)
Jump to navigationJump to search
Jump to navigationJump to search
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
"Data transfer" redirects here. For sharing data between different programs or schemas, see Data exchange.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.
Data transmission and data reception (or, more broadly, data communication or digital communications) is the transfer and reception of data (a digital bitstream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.


Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.
Analog or analogue transmission is a transmission method of conveying voice, data, image, signal or video information using a continuous signal which varies in amplitude, phase, or some other property in proportion to that of a variable. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying waveforms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.


Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.
Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream, for example, using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.




Contents
Contents
1	Distinction between related subjects
1	Distinction between related subjects
2	Protocol layers and sub-topics
2	Protocol layers and sub-topics
3	Applications and history
3	Applications and history
4	Serial and parallel transmission
4	Serial and parallel transmission
5	Communication channels
5	Communication channels
6	Asynchronous and synchronous data transmission
6	Asynchronous and synchronous data transmission
7	See also
7	See also
8	References
8	References
Distinction between related subjects
Distinction between related subjects
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.
Courses and textbooks in the field of data transmission[1] as well as digital transmission[2][3] and digital communications[4][5] have similar content.


Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.
Digital transmission or data transmission traditionally belongs to telecommunications and electrical engineering. Basic principles of data transmission may also be covered within the computer science or computer engineering topic of data communications, which also includes computer networking applications and networking protocols, for example routing, switching and inter-process communication. Although the Transmission Control Protocol (TCP) involves transmission, TCP and other transport layer protocols are covered in computer networking but not discussed in a textbook or course about data transmission.


The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]
The term tele transmission involves the analog as well as digital communication. In most textbooks, the term analog transmission only refers to the transmission of an analog message signal (without digitization) by means of an analog signal, either as a non-modulated baseband signal, or as a passband signal using an analog modulation method such as AM or FM. It may also include analog-over-analog pulse modulatated baseband signals such as pulse-width modulation. In a few books within the computer networking tradition, "analog transmission" also refers to passband transmission of bit-streams using digital modulation methods such as FSK, PSK and ASK. Note that these methods are covered in textbooks named digital transmission or data transmission, for example.[1]


The theoretical aspects of data transmission are covered by information theory and coding theory.
The theoretical aspects of data transmission are covered by information theory and coding theory.


Protocol layers and sub-topics
Protocol layers and sub-topics
OSI model
OSI model
by layer
by layer
7.  Application layer[show]
7.  Application layer[show]
6.  Presentation layer[show]
6.  Presentation layer[show]
5.  Session layer[show]
5.  Session layer[show]
4.  Transport layer[show]
4.  Transport layer[show]
3.  Network layer[show]
3.  Network layer[show]
2.  Data link layer[show]
2.  Data link layer[show]
1.  Physical layer[show]
1.  Physical layer[show]
vte
vte
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:
Courses and textbooks in the field of data transmission typically deal with the following OSI model protocol layers and topics:


Layer 1, the physical layer:
Layer 1, the physical layer:
Channel coding including
Channel coding including
Digital modulation schemes
Digital modulation schemes
Line coding schemes
Line coding schemes
Forward error correction (FEC) codes
Forward error correction (FEC) codes
Bit synchronization
Bit synchronization
Multiplexing
Multiplexing
Equalization
Equalization
Channel models
Channel models
Layer 2, the data link layer:
Layer 2, the data link layer:
Channel access schemes, media access control (MAC)
Channel access schemes, media access control (MAC)
Packet mode communication and Frame synchronization
Packet mode communication and Frame synchronization
Error detection and automatic repeat request (ARQ)
Error detection and automatic repeat request (ARQ)
Flow control
Flow control
Layer 6, the presentation layer:
Layer 6, the presentation layer:
Source coding (digitization and data compression), and information theory.
Source coding (digitization and data compression), and information theory.
Cryptography (may occur at any layer)
Cryptography (may occur at any layer)
It is also common to deal with the cross-layer design of those three layers.[6]
It is also common to deal with the cross-layer design of those three layers.[6]


Applications and history
Applications and history
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.
Data (mainly but not exclusively informational) has been sent via non-electronic (e.g. optical, acoustic, mechanical) means since the advent of communication. Analog signal data has been sent electronically since the advent of the telephone. However, the first data electromagnetic transmission applications in modern time were telegraphy (1809) and teletypewriters (1906), which are both digital signals. The fundamental theoretical work in data transmission and information theory by Harry Nyquist, Ralph Hartley, Claude Shannon and others during the early 20th century, was done with these applications in mind.


Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.
Data transmission is utilized in computers in computer buses and for communication with peripheral equipment via parallel ports and serial ports such as RS-232 (1969), FireWire (1995) and USB (1996). The principles of data transmission are also utilized in storage media for Error detection and correction since 1951.


Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.
Data transmission is utilized in computer networking equipment such as modems (1940), local area networks (LAN) adapters (1964), repeaters, repeater hubs, microwave links, wireless network access points (1997), etc.


In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.
In telephone networks, digital communication is utilized for transferring many phone calls over the same copper cable or fiber cable by means of Pulse code modulation (PCM), i.e. sampling and digitization, in combination with Time division multiplexing (TDM) (1962). Telephone exchanges have become digital and software controlled, facilitating many value added services. For example, the first AXE telephone exchange was presented in 1976. Since the late 1980s, digital communication to the end user has been possible using Integrated Services Digital Network (ISDN) services. Since the end of the 1990s, broadband access techniques such as ADSL, Cable modems, fiber-to-the-building (FTTB) and fiber-to-the-home (FTTH) have become widespread to small offices and homes. The current tendency is to replace traditional telecommunication services by packet mode communication such as IP telephony and IPTV.


Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.
Transmitting analog signals digitally allows for greater signal processing capability. The ability to process a communications signal means that errors caused by random processes can be detected and corrected. Digital signals can also be sampled instead of continuously monitored. The multiplexing of multiple digital signals is much simpler to the multiplexing of analog signals.


Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.
Because of all these advantages, and because recent advances in wideband communication channels and solid-state electronics have allowed scientists to fully realize these advantages, digital communications has grown quickly. Digital communications is quickly edging out analog communication because of the vast demand to transmit computer data and the ability of digital communications to do so.


The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.
The digital revolution has also resulted in many digital telecommunication applications where the principles of data transmission are applied. Examples are second-generation (1991) and later cellular telephony, video conferencing, digital TV (1998), digital radio (1999), telemetry, etc.


Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.
Data transmission, digital transmission or digital communications is the physical transfer of data (a digital bit stream or a digitized analog signal[1]) over a point-to-point or point-to-multipoint communication channel. Examples of such channels are copper wires, optical fibers, wireless communication channels, storage media and computer buses. The data are represented as an electromagnetic signal, such as an electrical voltage, radiowave, microwave, or infrared signal.


While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.
While analog transmission is the transfer of a continuously varying analog signal over an analog channel, digital communications is the transfer of discrete messages over a digital or an analog channel. The messages are either represented by a sequence of pulses by means of a line code (baseband transmission), or by a limited set of continuously varying wave forms (passband transmission), using a digital modulation method. The passband modulation and corresponding demodulation (also known as detection) is carried out by modem equipment. According to the most common definition of digital signal, both baseband and passband signals representing bit-streams are considered as digital transmission, while an alternative definition only considers the baseband signal as digital, and passband transmission of digital data as a form of digital-to-analog conversion.


Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.
Data transmitted may be digital messages originating from a data source, for example a computer or a keyboard. It may also be an analog signal such as a phone call or a video signal, digitized into a bit-stream for example using pulse-code modulation (PCM) or more advanced source coding (analog-to-digital conversion and data compression) schemes. This source coding and decoding is carried out by codec equipment.


Serial and parallel transmission
Serial and parallel transmission
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.
In telecommunications, serial transmission is the sequential transmission of signal elements of a group representing a character or other entity of data. Digital serial transmissions are bits sent over a single wire, frequency or optical path sequentially. Because it requires less signal processing and less chances for error than parallel transmission, the transfer rate of each individual path may be faster. This can be used over longer distances as a check digit or parity bit can be sent along it easily.


In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.
In telecommunications, parallel transmission is the simultaneous transmission of the signal elements of a character or other entity of data. In digital communications, parallel transmission is the simultaneous transmission of related signal elements over two or more separate paths. Multiple electrical wires are used which can transmit multiple bits simultaneously, which allows for higher data transfer rates than can be achieved with serial transmission. This method is used internally within the computer, for example the internal buses, and sometimes externally for such things as printers, The major issue with this is "skewing" because the wires in parallel data transmission have slightly different properties (not intentionally) so some bits may arrive before others, which may corrupt the message. A parity bit can help to reduce this. However, electrical wire parallel data transmission is therefore less reliable for long distances because corrupt transmissions are far more likely.


Communication channels
Communication channels
Main article: communication channel
Main article: communication channel
Some communications channel types include:
Some communications channel types include:


Data transmission circuit
Data transmission circuit
Full-duplex
Full-duplex
Half-duplex
Half-duplex
Multi-drop:
Multi-drop:
Bus network
Bus network
Mesh network
Mesh network
Ring network
Ring network
Star network
Star network
Wireless network
Wireless network
Point-to-point
Point-to-point
Simplex
Simplex
Asynchronous and synchronous data transmission
Asynchronous and synchronous data transmission
Main article: Comparison of synchronous and asynchronous signalling
Main article: Comparison of synchronous and asynchronous signalling
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.
Asynchronous serial communication uses start and stop bits to signify the beginning and end of transmission.[7] This method of transmission is used when data are sent intermittently as opposed to in a solid stream.


Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.
Synchronous transmission synchronizes transmission speeds at both the receiving and sending end of the transmission using clock signals. The clock may be a separate signal or embedded in the data. A continual stream of data is then sent between the two nodes. Due to there being no start and stop bits the data transfer rate is more efficient.


See also
See also
Computer networking
Computer networking
Communication
Communication
Data migration
Data migration
Information theory
Information theory
Media (communication)
Media (communication)
Network security
Network security
Node-to-node data transfer
Node-to-node data transfer
Packet switching
Packet switching
Signal processing
Signal processing
Telecommunication
Telecommunication
Transmission (disambiguation)
Transmission (disambiguation)
References
References
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 A. P. Clark, "Principles of Digital Data Transmission", Published by Wiley, 1983
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 David R. Smith, "Digital Transmission Systems", Kluwer International Publishers, 2003, ISBN 1-4020-7587-1. See table-of-contents.
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Sergio Benedetto, Ezio Biglieri, "Principles of Digital Transmission: With Wireless Applications", Springer 2008, ISBN 0-306-45753-9, ISBN 978-0-306-45753-1. See table-of-contents
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 Simon Haykin, "Digital Communications", John Wiley & Sons, 1988. ISBN 978-0-471-62947-4. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 John Proakis, "Digital Communications", 4th edition, McGraw-Hill, 2000. ISBN 0-07-232111-3. See table-of-contents.
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 F. Foukalas et al., "Cross-layer design proposals for wireless mobile networks: a survey and taxonomy "
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
 "What is Asynchronous Transmission? - Definition from Techopedia". Techopedia.com. Retrieved 2017-12-08.
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Categories: Data transmissionComputer networkingMass media technologyTelecommunications
Navigation menu
Navigation menu
Not logged inTalkContributionsCreate accountLog in
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ArticleTalk
ReadEditView historySearch
ReadEditView historySearch
Search Wikipedia
Search Wikipedia
Main page
Main page
Contents
Contents
Current events
Current events
Random article
Random article
About Wikipedia
About Wikipedia
Contact us
Contact us
Donate
Donate
Contribute
Contribute
Help
Help
Learn to edit
Learn to edit
Community portal
Community portal
Recent changes
Recent changes
Upload file
Upload file
Tools
Tools
What links here
What links here
Related changes
Related changes
Special pages
Special pages
Permanent link
Permanent link
Page information
Page information
Cite this page
Cite this page
Wikidata item
Wikidata item
Print/export
Print/export
Download as PDF
Download as PDF
Printable version
Printable version
In other projects
In other projects
Wikimedia Commons
Wikimedia Commons


Languages
Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
Deutsch
EspaÃ±ol
EspaÃ±ol
FranÃ§ais
FranÃ§ais
íêµ­ì´
íêµ­ì´
Italiano
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
Tiáº¿ng Viá»t
ä¸­æ
ä¸­æ
28 more
28 more
Edit links
Edit links
This page was last edited on 30 October 2020, at 04:19 (UTC).
This page was last edited on 30 October 2020, at 04:19 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki


Modulation
Modulation
From Wikipedia, the free encyclopedia
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Jump to navigationJump to search
For other uses, see Modulation (disambiguation).
For other uses, see Modulation (disambiguation).


This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
This article may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (February 2017) (Learn how and when to remove this template message)
Passband modulation
Passband modulation
Analog modulation
Analog modulation
AMFMPMQAMSMSSB
AMFMPMQAMSMSSB
Digital modulation
Digital modulation
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
ASKAPSKCPMFSKMFSKMSKOOKPPMPSKQAMSC-FDETCMWDM
Hierarchical modulation
Hierarchical modulation
QAMWDM
QAMWDM
Spread spectrum
Spread spectrum
CSSDSSSFHSSTHSS
CSSDSSSFHSSTHSS
See also
See also
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
Capacity-approaching codesDemodulationLine codingModemAnMPoMPAMPCMPDMPWMÎÎ£MOFDMFDMMultiplexing
vte
vte
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.
Modulation is used by singers and other vocalists to modify characteristics of their voices, such as loudness or pitch.


Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.
Modulation is also a technical term to express the multiplication of the original signal by another, usually periodic, signal.


In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.
In electronics and telecommunications, modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a modulating signal that typically contains information to be transmitted. The term analog or digital modulation is used when the modulating signal is analog or digital, respectively. Most radio systems in the 20th century used so-called analog modulation techniques: frequency modulation (FM) or amplitude modulation (AM) for radio broadcast since the original signal was analog. Most, if not all, modern transmission systems use QAM (Quadrature Amplitude Modulation) which changes the amplitude and phase of the carrier signal. As the modulating signal is a sequence or stream of bit, i.e., a digital modulating signal, the term digital modulation is used. However, it must be pointed out that, usually, the sequence of bits must be converted to an analog signal prior to the modulation (multiplication) by the carrier signal.


In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]
In music production, modulation is the process of gradually changing sound properties in order to reproduce a sense of movement and depth in audio recordings. It involves the use of a source signal (known as a modulator) to control another signal (a carrier) through a variety of sound effects and methods of synthesis.[1]


A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.
A modulator is a device that performs modulation. A demodulator (sometimes detector or demod) is a device that performs demodulation, the inverse of modulation. A modem (from modulatorâdemodulator) can perform both operations.


The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).
The aim of analog modulation is to transfer an analog baseband (or lowpass) signal, for example an audio signal or TV signal, over an analog bandpass channel at a different frequency, for example over a limited radio frequency band or a cable TV network channel. The aim of digital modulation is to transfer a digital bit stream over an analog communication channel, for example over the public switched telephone network (where a bandpass filter limits the frequency range to 300â3400 Hz) or over a limited radio frequency band. Analog and digital modulation facilitate frequency division multiplexing (FDM), where several low pass information signals are transferred simultaneously over the same shared physical medium, using separate passband channels (several different carrier frequencies).


The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.
The aim of digital baseband modulation methods, also known as line coding, is to transfer a digital bit stream over a baseband channel, typically a non-filtered copper wire such as a serial bus or a wired local area network.


The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.
The aim of pulse modulation methods is to transfer a narrowband analog signal, for example, a phone call over a wideband baseband channel or, in some of the schemes, as a bit stream over another digital transmission system.




Contents
Contents
1	Analog modulation methods
1	Analog modulation methods
2	Digital modulation methods
2	Digital modulation methods
2.1	Fundamental digital modulation methods
2.1	Fundamental digital modulation methods
2.2	Modulator and detector principles of operation
2.2	Modulator and detector principles of operation
2.3	List of common digital modulation techniques
2.3	List of common digital modulation techniques
2.4	Automatic digital modulation recognition (ADMR)
2.4	Automatic digital modulation recognition (ADMR)
2.5	Digital baseband modulation or line coding
2.5	Digital baseband modulation or line coding
3	Pulse modulation methods
3	Pulse modulation methods
4	Miscellaneous modulation techniques
4	Miscellaneous modulation techniques
5	See also
5	See also
6	References
6	References
7	Further reading
7	Further reading
8	External links
8	External links
Analog modulation methods
Analog modulation methods


A low-frequency message signal (top) may be carried by an AM or FM radio wave.
A low-frequency message signal (top) may be carried by an AM or FM radio wave.


Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.
Waterfall plot of a 146.52 MHz radio carrier, with amplitude modulation by a 1,000 Hz sinusoid. Two strong sidebands at + and - 1 kHz from the carrier frequency are shown.


A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
A carrier, frequency modulated by a 1,000 Hz sinusoid. The modulation index has been adjusted to around 2.4, so the carrier frequency has small amplitude. Several strong sidebands are apparent; in principle an infinite number are produced in FM but the higher-order sidebands are of negligible magnitude.
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:
In analog modulation, the modulation is applied continuously in response to the analog information signal. Common analog modulation techniques include:


Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Amplitude modulation (AM) (here the amplitude of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Double-sideband modulation (DSB)
Double-sideband modulation (DSB)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband modulation with carrier (DSB-WC) (used on the AM radio broadcasting band)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband suppressed-carrier transmission (DSB-SC)
Double-sideband reduced carrier transmission (DSB-RC)
Double-sideband reduced carrier transmission (DSB-RC)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation (SSB, or SSB-AM)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation with carrier (SSB-WC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Single-sideband modulation suppressed carrier modulation (SSB-SC)
Vestigial sideband modulation (VSB, or VSB-AM)
Vestigial sideband modulation (VSB, or VSB-AM)
Quadrature amplitude modulation (QAM)
Quadrature amplitude modulation (QAM)
Angle modulation, which is approximately constant envelope
Angle modulation, which is approximately constant envelope
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Frequency modulation (FM) (here the frequency of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Phase modulation (PM) (here the phase shift of the carrier signal is varied in accordance with the instantaneous amplitude of the modulating signal)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Transpositional Modulation (TM), in which the waveform inflection is modified resulting in a signal where each quarter cycle is transposed in the modulation process. TM is a pseudo-analog modulation (AM). Where an AM carrier also carries a phase variable phase f(Ç¿). TM is f(AM,Ç¿)
Digital modulation methods
Digital modulation methods
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).
In digital modulation, an analog carrier signal is modulated by a discrete signal. Digital modulation methods can be considered as digital-to-analog conversion and the corresponding demodulation or detection as analog-to-digital conversion. The changes in the carrier signal are chosen from a finite number of M alternative symbols (the modulation alphabet).




Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
Schematic of 4 baud, 8 bit/s data link containing arbitrarily chosen values.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.
A simple example: A telephone line is designed for transferring audible sounds, for example, tones, and not digital bits (zeros and ones). Computers may, however, communicate over a telephone line by means of modems, which are representing the digital bits by tones, called symbols. If there are four alternative symbols (corresponding to a musical instrument that can generate four different tones, one at a time), the first symbol may represent the bit sequence 00, the second 01, the third 10 and the fourth 11. If the modem plays a melody consisting of 1000 tones per second, the symbol rate is 1000 symbols/second, or 1000 baud. Since each tone (i.e., symbol) represents a message consisting of two digital bits in this example, the bit rate is twice the symbol rate, i.e. 2000 bits per second.


According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.
According to one definition of digital signal,[2] the modulated signal is a digital signal. According to another definition, the modulation is a form of digital-to-analog conversion. Most textbooks would consider digital modulation schemes as a form of digital transmission, synonymous to data transmission; very few would consider it as analog transmission.


Fundamental digital modulation methods
Fundamental digital modulation methods
The most fundamental digital modulation techniques are based on keying:
The most fundamental digital modulation techniques are based on keying:


PSK (phase-shift keying): a finite number of phases are used.
PSK (phase-shift keying): a finite number of phases are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
FSK (frequency-shift keying): a finite number of frequencies are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
ASK (amplitude-shift keying): a finite number of amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
QAM (quadrature amplitude modulation): a finite number of at least two phases and at least two amplitudes are used.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.
In QAM, an in-phase signal (or I, with one example being a cosine waveform) and a quadrature phase signal (or Q, with an example being a sine wave) are amplitude modulated with a finite number of amplitudes and then summed. It can be seen as a two-channel system, each channel using ASK. The resulting signal is equivalent to a combination of PSK and ASK.


In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.
In all of the above methods, each of these phases, frequencies or amplitudes are assigned a unique pattern of binary bits. Usually, each phase, frequency or amplitude encodes an equal number of bits. This number of bits comprises the symbol that is represented by the particular phase, frequency or amplitude.


If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.
If the alphabet consists of {\displaystyle M=2^{N}}M=2^{N} alternative symbols, each symbol represents a message consisting of N bits. If the symbol rate (also known as the baud rate) is {\displaystyle f_{S}}f_{S} symbols/second (or baud), the data rate is {\displaystyle Nf_{S}}Nf_{S} bit/second.


For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.
For example, with an alphabet consisting of 16 alternative symbols, each symbol represents 4 bits. Thus, the data rate is four times the baud rate.


In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.
In the case of PSK, ASK or QAM, where the carrier frequency of the modulated signal is constant, the modulation alphabet is often conveniently represented on a constellation diagram, showing the amplitude of the I signal at the x-axis, and the amplitude of the Q signal at the y-axis, for each symbol.


Modulator and detector principles of operation
Modulator and detector principles of operation
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).
PSK and ASK, and sometimes also FSK, are often generated and detected using the principle of QAM. The I and Q signals can be combined into a complex-valued signal I+jQ (where j is the imaginary unit). The resulting so called equivalent lowpass signal or equivalent baseband signal is a complex-valued representation of the real-valued modulated physical signal (the so-called passband signal or RF signal).


These are the general steps used by the modulator to transmit data:
These are the general steps used by the modulator to transmit data:


Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Group the incoming data bits into codewords, one for each symbol that will be transmitted.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Map the codewords to attributes, for example, amplitudes of the I and Q signals (the equivalent low pass signal), or frequency or phase values.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Adapt pulse shaping or some other filtering to limit the bandwidth and form the spectrum of the equivalent low pass signal, typically using digital signal processing.
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Perform digital to analog conversion (DAC) of the I and Q signals (since today all of the above is normally achieved using digital signal processing, DSP).
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Generate a high-frequency sine carrier waveform, and perhaps also a cosine quadrature component. Carry out the modulation, for example by multiplying the sine and cosine waveform with the I and Q signals, resulting in the equivalent low pass signal being frequency shifted to the modulated passband signal or RF signal. Sometimes this is achieved using DSP technology, for example direct digital synthesis using a waveform table, instead of analog signal processing. In that case, the above DAC step should be done after this step.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
Amplification and analog bandpass filtering to avoid harmonic distortion and periodic spectrum.
At the receiver side, the demodulator typically performs:
At the receiver side, the demodulator typically performs:


Bandpass filtering.
Bandpass filtering.
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Automatic gain control, AGC (to compensate for attenuation, for example fading).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Frequency shifting of the RF signal to the equivalent baseband I and Q signals, or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local oscillator sine wave and cosine wave frequency (see the superheterodyne receiver principle).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Sampling and analog-to-digital conversion (ADC) (sometimes before or instead of the above point, for example by means of undersampling).
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Equalization filtering, for example, a matched filter, compensation for multipath propagation, time spreading, phase distortion and frequency selective fading, to avoid intersymbol interference and symbol distortion.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Detection of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Quantization of the amplitudes, frequencies or phases to the nearest allowed symbol values.
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Mapping of the quantized amplitudes, frequencies or phases to codewords (bit groups).
Parallel-to-serial conversion of the codewords into a bit stream.
Parallel-to-serial conversion of the codewords into a bit stream.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
Pass the resultant bit stream on for further processing such as removal of any error-correcting codes.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.
As is common to all digital communication systems, the design of both the modulator and demodulator must be done simultaneously. Digital modulation schemes are possible because the transmitter-receiver pair has prior knowledge of how data is encoded and represented in the communications system. In all digital communication systems, both the modulator at the transmitter and the demodulator at the receiver are structured so that they perform inverse operations.


Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.
Asynchronous methods do not require a receiver reference clock signal that is phase synchronized with the sender carrier signal. In this case, modulation symbols (rather than bits, characters, or data packets) are asynchronously transferred. The opposite is synchronous modulation.


List of common digital modulation techniques
List of common digital modulation techniques
The most common digital modulation techniques are:
The most common digital modulation techniques are:


Phase-shift keying (PSK)
Phase-shift keying (PSK)
Binary PSK (BPSK), using M=2 symbols
Binary PSK (BPSK), using M=2 symbols
Quadrature PSK (QPSK), using M=4 symbols
Quadrature PSK (QPSK), using M=4 symbols
8PSK, using M=8 symbols
8PSK, using M=8 symbols
16PSK, using M=16 symbols
16PSK, using M=16 symbols
Differential PSK (DPSK)
Differential PSK (DPSK)
Differential QPSK (DQPSK)
Differential QPSK (DQPSK)
Offset QPSK (OQPSK)
Offset QPSK (OQPSK)
Ï/4âQPSK
Ï/4âQPSK
Frequency-shift keying (FSK)
Frequency-shift keying (FSK)
Audio frequency-shift keying (AFSK)
Audio frequency-shift keying (AFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Multi-frequency shift keying (M-ary FSK or MFSK)
Dual-tone multi-frequency (DTMF)
Dual-tone multi-frequency (DTMF)
Amplitude-shift keying (ASK)
Amplitude-shift keying (ASK)
On-off keying (OOK), the most common ASK form
On-off keying (OOK), the most common ASK form
M-ary vestigial sideband modulation, for example 8VSB
M-ary vestigial sideband modulation, for example 8VSB
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Quadrature amplitude modulation (QAM), a combination of PSK and ASK
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Polar modulation like QAM a combination of PSK and ASK[citation needed]
Continuous phase modulation (CPM) methods
Continuous phase modulation (CPM) methods
Minimum-shift keying (MSK)
Minimum-shift keying (MSK)
Gaussian minimum-shift keying (GMSK)
Gaussian minimum-shift keying (GMSK)
Continuous-phase frequency-shift keying (CPFSK)
Continuous-phase frequency-shift keying (CPFSK)
Orthogonal frequency-division multiplexing (OFDM) modulation
Orthogonal frequency-division multiplexing (OFDM) modulation
Discrete multitone (DMT), including adaptive modulation and bit-loading
Discrete multitone (DMT), including adaptive modulation and bit-loading
Wavelet modulation
Wavelet modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Trellis coded modulation (TCM), also known as Trellis modulation
Spread-spectrum techniques
Spread-spectrum techniques
Direct-sequence spread spectrum (DSSS)
Direct-sequence spread spectrum (DSSS)
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Chirp spread spectrum (CSS) according to IEEE 802.15.4a CSS uses pseudo-stochastic coding
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
Frequency-hopping spread spectrum (FHSS) applies a special scheme for channel release
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).
MSK and GMSK are particular cases of continuous phase modulation. Indeed, MSK is a particular case of the sub-family of CPM known as continuous-phase frequency shift keying (CPFSK) which is defined by a rectangular frequency pulse (i.e. a linearly increasing phase pulse) of one-symbol-time duration (total response signaling).


OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.
OFDM is based on the idea of frequency-division multiplexing (FDM), but the multiplexed streams are all parts of a single original stream. The bit stream is split into several parallel data streams, each transferred over its own sub-carrier using some conventional digital modulation scheme. The modulated sub-carriers are summed to form an OFDM signal. This dividing and recombining help with handling channel impairments. OFDM is considered as a modulation technique rather than a multiplex technique since it transfers one bit stream over one communication channel using one sequence of so-called OFDM symbols. OFDM can be extended to multi-user channel access method in the orthogonal frequency-division multiple access (OFDMA) and multi-carrier code division multiple access (MC-CDMA) schemes, allowing several users to share the same physical medium by giving different sub-carriers or spreading codes to different users.


Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.
Of the two kinds of RF power amplifier, switching amplifiers (Class D amplifiers) cost less and use less battery power than linear amplifiers of the same output power. However, they only work with relatively constant-amplitude-modulation signals such as angle modulation (FSK or PSK) and CDMA, but not with QAM and OFDM. Nevertheless, even though switching amplifiers are completely unsuitable for normal QAM constellations, often the QAM modulation principle are used to drive switching amplifiers with these FM and other waveforms, and sometimes QAM demodulators are used to receive the signals put out by these switching amplifiers.


Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition (ADMR)
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]
Automatic digital modulation recognition in intelligent communication systems is one of the most important issues in software defined radio and cognitive radio. According to incremental expanse of intelligent receivers, automatic modulation recognition becomes a challenging topic in telecommunication systems and computer engineering. Such systems have many civil and military applications. Moreover, blind recognition of modulation type is an important problem in commercial systems, especially in software defined radio. Usually in such systems, there are some extra information for system configuration, but considering blind approaches in intelligent receivers, we can reduce information overload and increase transmission performance.[3] Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information, etc., blind identification of the modulation is made fairly difficult. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels.[4]


There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.
There are two main approaches to automatic modulation recognition. The first approach uses likelihood-based methods to assign an input signal to a proper class. Another recent approach is based on feature extraction.


Digital baseband modulation or line coding
Digital baseband modulation or line coding
Main article: Line code
Main article: Line code
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]
The term digital baseband modulation (or digital baseband transmission) is synonymous to line codes. These are methods to transfer a digital bit stream over an analog baseband channel (a.k.a. lowpass channel) using a pulse train, i.e. a discrete number of signal levels, by directly modulating the voltage or current on a cable or serial bus. Common examples are unipolar, non-return-to-zero (NRZ), Manchester and alternate mark inversion (AMI) codings.[5]


vte
vte
Line coding (digital baseband transmission)
Line coding (digital baseband transmission)
Pulse modulation methods
Pulse modulation methods
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.
Pulse modulation schemes aim at transferring a narrowband analog signal over an analog baseband channel as a two-level signal by modulating a pulse wave. Some pulse modulation schemes also allow the narrowband analog signal to be transferred as a digital signal (i.e., as a quantized discrete-time signal) with a fixed bit rate, which can be transferred over an underlying digital transmission system, for example, some line code. These are not modulation schemes in the conventional sense since they are not channel coding schemes, but should be considered as source coding schemes, and in some cases analog-to-digital conversion techniques.


Analog-over-analog methods
Analog-over-analog methods


Pulse-amplitude modulation (PAM)
Pulse-amplitude modulation (PAM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-width modulation (PWM) and Pulse-depth modulation (PDM)
Pulse-position modulation (PPM)
Pulse-position modulation (PPM)
Analog-over-digital methods
Analog-over-digital methods


Pulse-code modulation (PCM)
Pulse-code modulation (PCM)
Differential PCM (DPCM)
Differential PCM (DPCM)
Adaptive DPCM (ADPCM)
Adaptive DPCM (ADPCM)
Delta modulation (DM or Î-modulation)
Delta modulation (DM or Î-modulation)
Delta-sigma modulation (âÎ)
Delta-sigma modulation (âÎ)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Continuously variable slope delta modulation (CVSDM), also called Adaptive-delta modulation (ADM)
Pulse-density modulation (PDM)
Pulse-density modulation (PDM)
Miscellaneous modulation techniques
Miscellaneous modulation techniques
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
The use of on-off keying to transmit Morse code at radio frequencies is known as continuous wave (CW) operation.
Adaptive modulation
Adaptive modulation
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
Space modulation is a method whereby signals are modulated within airspace such as that used in instrument landing systems.
See also
See also
	Wikimedia Commons has media related to Modulation.
	Wikimedia Commons has media related to Modulation.
Channel access methods
Channel access methods
Channel coding
Channel coding
Codec
Codec
Communications channel
Communications channel
Demodulation
Demodulation
Electrical resonance
Electrical resonance
Heterodyne
Heterodyne
Line code
Line code
Mechanically induced modulation
Mechanically induced modulation
Modem
Modem
Modulation order
Modulation order
Neuromodulation
Neuromodulation
RF modulator
RF modulator
Ring modulation
Ring modulation
Telecommunication
Telecommunication
Types of radio emissions
Types of radio emissions
References
References


This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
Find sources: "Modulation" â news Â· newspapers Â· books Â· scholar Â· JSTOR (June 2008) (Learn how and when to remove this template message)
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 Rory PQ (May 8, 2019). "What Is Modulation and How Does It Improve Your Music". Icon Collective. Retrieved August 23, 2020.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 "Modulation Methods | Electronics Basics | ROHM". www.rohm.com. Retrieved 2020-05-15.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Valipour, M. Hadi; Homayounpour, M. Mehdi; Mehralian, M. Amin (2012). "Automatic digital modulation recognition in presence of noise using SVM and PSO". 6th International Symposium on Telecommunications (IST). pp. 378â382. doi:10.1109/ISTEL.2012.6483016. ISBN 978-1-4673-2073-3.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Dobre, Octavia A., Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Communications, IET 1, no. 2 (2007): 137â156. (2007). "Survey of automatic modulation classification techniques: classical approaches and new trends" (PDF). IET Communications. 1 (2): 137â156. doi:10.1049/iet-com:20050176.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
 Ke-Lin Du & M. N. S. Swamy (2010). Wireless Communication Systems: From RF Subsystems to 4G Enabling Technologies. Cambridge University Press. p. 188. ISBN 978-0-521-11403-5.
Further reading
Further reading
Multipliers vs. Modulators Analog Dialogue, June 2013
Mul
External links
Interactive presentation of soft-demapping for AWGN-channel in a web-demo Institute of Telecommunications, University of Stuttgart
Modem (Modulation and Demodulation)
vte
Telecommunications
vte
Analog and digital audio broadcasting
Categories: Frequency mixersHistory of radioHistory of televisionPhysical layer protocolsRadio modulation modesTelecommunication theoryTelevision terminology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
52 more
Edit links
This page was last edited on 17 October 2020, at 19:13 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Radio broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from Radio broadcast)
Jump to navigationJump to search

Long wave radio broadcasting station, Motala, Sweden

Slovak Radio Building, Bratislava, Slovakia (architects: Å tefan Svetko, Å tefan ÄurkoviÄ and BarnabÃ¡Å¡ Kissling, 1967â1983)

Broadcasting tower in Trondheim, Norway
Radio broadcasting is transmission of audio (sound), sometimes with related metadata, by radio waves intended to reach a wide audience. In terrestrial radio broadcasting the radio waves are broadcast by a land-based radio station, while in satellite radio the radio waves are broadcast by a satellite in Earth orbit. To receive the content the listener must have a broadcast radio receiver (radio). Stations are often affiliated with a radio network which provides content in a common radio format, either in broadcast syndication or simulcast or both. Radio stations broadcast with several different types of modulation: AM radio stations transmit in AM (amplitude modulation), FM radio stations transmit in FM (frequency modulation), which are older analog audio standards, while newer digital radio stations transmit in several digital audio standards: DAB (digital audio broadcasting), HD radio, DRM (Digital Radio Mondiale). Television broadcasting is a separate service which also uses radio frequencies to broadcast television (video) signals.


Contents
1	History
2	Stations
3	Types
3.1	AM
3.1.1	Shortwave, medium wave and long wave
3.2	FM
3.3	Pirate radio
3.4	Terrestrial digital radio
4	Extensions
4.1	Satellite
5	Program formats
6	See also
7	References
8	Further reading
9	External links
History
See also: History of radio Â§ Broadcasting, and History of broadcasting

Advertisement placed in the November 5, 1919 Nieuwe Rotterdamsche Courant announcing PCGG's debut broadcast scheduled for the next evening.[1]
The earliest radio stations were radiotelegraphy systems and did not carry audio. For audio broadcasts to be possible, electronic detection and amplification devices had to be incorporated.

The thermionic valve (a kind of vacuum tube) was invented in 1904 by the English physicist John Ambrose Fleming. He developed a device he called an "oscillation valve" (because it passes current in only one direction). The heated filament, or cathode, was capable of thermionic emission of electrons that would flow to the plate (or anode) when it was at a higher voltage. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons. Later known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector.[2] This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker. However, what was still required was an amplifier.

The triode (mercury-vapor filled with a control grid) was patented on March 4, 1906, by the Austrian Robert von Lieben[3][4][5] independent from that, on October 25, 1906,[6][7] Lee De Forest patented his three-element Audion. It wasn't put to practical use until 1912 when its amplifying ability became recognized by researchers.[8]

By about 1920, valve technology had matured to the point where radio broadcasting was quickly becoming viable.[9][10] However, an early audio transmission that could be termed a broadcast may have occurred on Christmas Eve in 1906 by Reginald Fessenden, although this is disputed.[11] While many early experimenters attempted to create systems similar to radiotelephone devices by which only two parties were meant to communicate, there were others who intended to transmit to larger audiences. Charles Herrold started broadcasting in California in 1909 and was carrying audio by the next year. (Herrold's station eventually became KCBS).

In The Hague, the Netherlands, PCGG started broadcasting on November 6, 1919, making it, arguably the first commercial broadcasting station. In 1916, Frank Conrad, an electrical engineer employed at the Westinghouse Electric Corporation, began broadcasting from his Wilkinsburg, Pennsylvania garage with the call letters 8XK. Later, the station was moved to the top of the Westinghouse factory building in East Pittsburgh, Pennsylvania. Westinghouse relaunched the station as KDKA on November 2, 1920, as the first commercially licensed radio station in America.[12] The commercial broadcasting designation came from the type of broadcast license; advertisements did not air until years later. The first licensed broadcast in the United States came from KDKA itself: the results of the Harding/Cox Presidential Election. The Montreal station that became CFCF began broadcast programming on May 20, 1920, and the Detroit station that became WWJ began program broadcasts beginning on August 20, 1920, although neither held a license at the time.

In 1920, wireless broadcasts for entertainment began in the UK from the Marconi Research Centre 2MT at Writtle near Chelmsford, England. A famous broadcast from Marconi's New Street Works factory in Chelmsford was made by the famous soprano Dame Nellie Melba on June 15, 1920, where she sang two arias and her famous trill. She was the first artist of international renown to participate in direct radio broadcasts. The 2MT station began to broadcast regular entertainment in 1922. The BBC was amalgamated in 1922 and received a Royal Charter in 1926, making it the first national broadcaster in the world,[13][14] followed by Czech Radio and other European broadcasters in 1923.

Radio Argentina began regularly scheduled transmissions from the Teatro Coliseo in Buenos Aires on August 27, 1920, making its own priority claim. The station got its license on November 19, 1923. The delay was due to the lack of official Argentine licensing procedures before that date. This station continued regular broadcasting of entertainment and cultural fare for several decades.[15]

Radio in education soon followed and colleges across the U.S. began adding radio broadcasting courses to their curricula. Curry College in Milton, Massachusetts introduced one of the first broadcasting majors in 1932 when the college teamed up with WLOE in Boston to have students broadcast programs.[16]

Stations
"Radio station" redirects here. For a broader concept, see Radio communication station.
A radio broadcasting station is usually associated with wireless transmission, though in practice broadcasting transmission (sound and television) take place using both wires and radio waves. The point of this is that anyone with the appropriate receiving technology can receive the broadcast.[17]


Use of a sound broadcasting station
In line to ITU Radio Regulations (article1.61) each broadcasting station shall be classified by the service in which it operates permanently or temporarily.

Types

Transmission diagram of sound broadcasting (AM and FM)
Broadcasting by radio takes several forms. These include AM and FM stations. There are several subtypes, namely commercial broadcasting, non-commercial educational (NCE) public broadcasting and non-profit varieties as well as community radio, student-run campus radio stations, and hospital radio stations can be found throughout the world. Many stations broadcast on shortwave bands using AM technology that can be received over thousands of miles (especially at night). For example, the BBC, VOA, VOR, and Deutsche Welle have transmitted via shortwave to Africa and Asia. These broadcasts are very sensitive to atmospheric conditions and solar activity.

Nielsen Audio, formerly known as Arbitron, the United States-based company that reports on radio audiences, defines a "radio station" as a government-licensed AM or FM station; an HD Radio (primary or multicast) station; an internet stream of an existing government-licensed station; one of the satellite radio channels from XM Satellite Radio or Sirius Satellite Radio; or, potentially, a station that is not government licensed.[18]

AM
Main article: AM broadcasting

AM broadcasting stations in 2006
AM stations were the earliest broadcasting stations to be developed. AM refers to amplitude modulation, a mode of broadcasting radio waves by varying the amplitude of the carrier signal in response to the amplitude of the signal to be transmitted. The medium-wave band is used worldwide for AM broadcasting. Europe also uses the long wave band. In response to the growing popularity of FM stereo radio stations in the late 1980s and early 1990s, some North American stations began broadcasting in AM stereo, though this never gained popularity, and very few receivers were ever sold.

The signal is subject to interference from electrical storms (lightning) and other electromagnetic interference (EMI).[19] One advantage of AM radio signal is that it can be detected (turned into sound) with simple equipment. If a signal is strong enough, not even a power source is needed; building an unpowered crystal radio receiver was a common childhood project in the early decades of AM broadcasting.

AM broadcasts occur on North American airwaves in the medium wave frequency range of 525 to 1705 kHz (known as the âstandard broadcast bandâ). The band was expanded in the 1990s by adding nine channels from 1605 to 1705 kHz. Channels are spaced every 10 kHz in the Americas, and generally every 9 kHz everywhere else.

AM transmissions cannot be ionospherically propagated during the day due to strong absorption in the D-layer of the ionosphere. In a crowded channel environment, this means that the power of regional channels which share a frequency must be reduced at night or directionally beamed in order to avoid interference, which reduces the potential nighttime audience. Some stations have frequencies unshared with other stations in North America; these are called clear-channel stations. Many of them can be heard across much of the country at night. During the night, absorption largely disappears and permits signals to travel to much more distant locations via ionospheric reflections. However, fading of the signal can be severe at night.

AM radio transmitters can transmit audio frequencies up to 15 kHz (now limited to 10 kHz in the US due to FCC rules designed to reduce interference), but most receivers are only capable of reproducing frequencies up to 5 kHz or less. At the time that AM broadcasting began in the 1920s, this provided adequate fidelity for existing microphones, 78 rpm recordings, and loudspeakers. The fidelity of sound equipment subsequently improved considerably, but the receivers did not. Reducing the bandwidth of the receivers reduces the cost of manufacturing and makes them less prone to interference. AM stations are never assigned adjacent channels in the same service area. This prevents the sideband power generated by two stations from interfering with each other.[20] Bob Carver created an AM stereo tuner employing notch filtering that demonstrated that an AM broadcast can meet or exceed the 15 kHz baseband bandwidth allotted to FM stations without objectionable interference. After several years, the tuner was discontinued. Bob Carver had left the company and the Carver Corporation later cut the number of models produced before discontinuing production completely.[citation needed]

Shortwave, medium wave and long wave
See shortwave for the differences between shortwave, medium wave, and long wave spectra. Shortwave is used largely for national broadcasters, international propaganda, or religious broadcasting organizations.[21]

FM
Main article: FM broadcasting

FM radio broadcast stations in 2006
FM refers to frequency modulation, and occurs on VHF airwaves in the frequency range of 88 to 108 MHz everywhere except Japan and Russia. Russia, like the former Soviet Union, uses 65.9 to 74 MHz frequencies in addition to the world standard. Japan uses the 76 to 90 MHz frequency band.

Edwin Howard Armstrong invented FM radio to overcome the problem of radio-frequency interference (RFI), which plagued AM radio reception. At the same time, greater fidelity was made possible by spacing stations further apart in the radio frequency spectrum. Instead of 10 kHz apart, as on the AM band in the US, FM channels are 200 kHz (0.2 MHz) apart. In other countries, greater spacing is sometimes mandatory, such as in New Zealand, which uses 700 kHz spacing (previously 800 kHz). The improved fidelity made available was far in advance of the audio equipment of the 1940s, but wide interchannel spacing was chosen to take advantage of the noise-suppressing feature of wideband FM.

Bandwidth of 200 kHz is not needed to accommodate an audio signal â 20 kHz to 30 kHz is all that is necessary for a narrowband FM signal. The 200 kHz bandwidth allowed room for Â±75 kHz signal deviation from the assigned frequency, plus guard bands to reduce or eliminate adjacent channel interference. The larger bandwidth allows for broadcasting a 15 kHz bandwidth audio signal plus a 38 kHz stereo "subcarrier"âa piggyback signal that rides on the main signal. Additional unused capacity is used by some broadcasters to transmit utility functions such as background music for public areas, GPS auxiliary signals, or financial market data.

The AM radio problem of interference at night was addressed in a different way. At the time FM was set up, the available frequencies were far higher in the spectrum than those used for AM radio - by a factor of approximately 100. Using these frequencies meant that even at far higher power, the range of a given FM signal was much shorter; thus its market was more local than for AM radio. The reception range at night is the same as in the daytime. All FM broadcast transmissions are line-of-sight, and ionospheric bounce is not viable. The much larger bandwidths, compared to AM and SSB, are more susceptible to phase dispersion. Propagation speeds (celerities) are fastest in the ionosphere at the lowest sideband frequency. The celerity difference between the highest and lowest sidebands is quite apparent to the listener. Such distortion occurs up to frequencies of approximately 50 MHz. Higher frequencies do not reflect from the ionosphere, nor from storm clouds. Moon reflections have been used in some experiments, but require impractical power levels.

The original FM radio service in the U.S. was the Yankee Network, located in New England.[22][23][24] Regular FM broadcasting began in 1939 but did not pose a significant threat to the AM broadcasting industry. It required purchase of a special receiver. The frequencies used, 42 to 50 MHz, were not those used today. The change to the current frequencies, 88 to 108 MHz, began after the end of World War II and was to some extent imposed by AM broadcasters as an attempt to cripple what was by now realized to be a potentially serious threat.

FM radio on the new band had to begin from the ground floor. As a commercial venture, it remained a little-used audio enthusiasts' medium until the 1960s. The more prosperous AM stations, or their owners, acquired FM licenses and often broadcast the same programming on the FM station as on the AM station ("simulcasting"). The FCC limited this practice in the 1960s. By the 1980s, since almost all new radios included both AM and FM tuners, FM became the dominant medium, especially in cities. Because of its greater range, AM remained more common in rural environments.

Pirate radio
Main article: Pirate radio
Pirate radio is illegal or non-regulated radio transmission. It is most commonly used to describe illegal broadcasting for entertainment or political purposes. Sometimes it is used for illegal two-way radio operation. Its history can be traced back to the unlicensed nature of the transmission, but historically there has been occasional use of sea vesselsâfitting the most common perception of a pirateâas broadcasting bases. Rules and regulations vary largely from country to country, but often the term pirate radio generally describes the unlicensed broadcast of FM radio, AM radio, or shortwave signals over a wide range. In some places, radio stations are legal where the signal is transmitted, but illegal where the signals are receivedâespecially when the signals cross a national boundary. In other cases, a broadcast may be considered "pirate" due to the type of content, its transmission format, or the transmitting power (wattage) of the station, even if the transmission is not technically illegal (such as a webcast or an amateur radio transmission). Pirate radio stations are sometimes referred to as bootleg radio or clandestine stations.

Terrestrial digital radio
Main articles: Digital audio broadcasting, HD radio, ISDB, and Digital Radio Mondiale
Digital radio broadcasting has emerged, first in Europe (the UK in 1995 and Germany in 1999), and later in the United States, France, the Netherlands, South Africa, and many other countries worldwide. The simplest system is named DAB Digital Radio, for Digital Audio Broadcasting, and uses the public domain EUREKA 147 (Band III) system. DAB is used mainly in the UK and South Africa. Germany and the Netherlands use the DAB and DAB+ systems, and France uses the L-Band system of DAB Digital Radio.

The broadcasting regulators of the United States and Canada have chosen to use HD radio, an in-band on-channel system that puts digital broadcasts at frequencies adjacent to the analog broadcast. HD Radio is owned by a consortium of private companies that is called iBiquity. An international non-profit consortium Digital Radio Mondiale (DRM), has introduced the public domain DRM system, which is used by a relatively small number of broadcasters worldwide.

Extensions
Extensions of traditional radio-wave broadcasting for audio broadcasting in general include cable radio, local wire television networks, DTV radio, satellite radio, and internet radio via streaming media on the Internet.

Satellite
[icon]
This section needs expansion. You can help by adding to it. (November 2008)
Main article: Satellite radio
The enormous entry costs of space-based satellite transmitters and restrictions on available radio spectrum licenses has restricted growth of Satellite radio broadcasts. In the US and Canada, just two services, XM Satellite Radio and Sirius Satellite Radio exist. Both XM and Sirius are owned by Sirius XM Satellite Radio, which was formed by the merger of XM and Sirius on July 29, 2008, whereas in Canada, XM Radio Canada and Sirius Canada remained separate companies until 2010. Worldspace in Africa and Asia, and MobaHO! in Japan and the ROK were two unsuccessful satellite radio operators which have gone out of business.

Program formats
Main article: Radio format
Radio program formats differ by country, regulation, and markets. For instance, the U.S. Federal Communications Commission designates the 88â92 megahertz band in the U.S. for non-profit or educational programming, with advertising prohibited.

In addition, formats change in popularity as time passes and technology improves. Early radio equipment only allowed program material to be broadcast in real time, known as live broadcasting. As technology for sound recording improved, an increasing proportion of broadcast programming used pre-recorded material. A current trend is the automation of radio stations. Some stations now operate without direct human intervention by using entirely pre-recorded material sequenced by computer control.

See also
Broadcasting construction permit
Call sign
Disc jockey (DJ)
History of broadcasting
International broadcasting
List of radio topics
Low power radio station
Radio
Radio antenna
Radio network
Radio personality
RF modulation
Sports commentator
Television station
References
 "Vintage Radio Web: Philips" (vintageradio.nl)
 Guarnieri, M. (2012). "The age of vacuum tubes: Early devices and the rise of radio communications". IEEE Ind. Electron. M.: 41â43. doi:10.1109/MIE.2012.2182822.
 Schmidt, Hans-Thomas. "Die LiebenrÃ¶hre". Umleitung zur Homepage von H.-T. Schmidt (in German). Retrieved August 10, 2019. DRP 179807
 Tapan K. Sarkar (ed.) "History of wireless", John Wiley and Sons, 2006. ISBN 0-471-71814-9, p.335
 SÅgo Okamura (ed), History of Electron Tubes, IOS Press, 1994 ISBN 90-5199-145-2 page 20
 "US841387A - Device for amplifying feeble electrical currents". Google Patents. October 25, 1906. Retrieved August 10, 2019.
 "US879532A - Space telegraphy". Google Patents. January 29, 1907. Retrieved August 10, 2019.
 Nebeker, Frederik (2009). Dawn of the Electronic Age: Electrical Technologies in the Shaping of the Modern World, 1914 to 1945. John Wiley & Sons. pp. 14â15. ISBN 978-0470409749.
 "Making the Modern World - Mass consumption". webarchive.nationalarchives.gov.uk. Archived from the original on April 5, 2017.
 Guarnieri, M. (2012). "The age of vacuum tubes: the conquest of analog communications". IEEE Ind. Electron. M.: 52â54. doi:10.1109/MIE.2012.2193274.
 Fessenden â The Next Chapter RWonline.com Archived September 16, 2009, at the Wayback Machine
 Baudino, Joseph E; John M. Kittross (Winter 1977). "Broadcasting's Oldest Stations: An Examination of Four Claimants". Journal of Broadcasting. 21: 61â82. doi:10.1080/08838157709363817. Archived from the original on March 6, 2008. Retrieved January 18, 2013.
 "CARS - Marconi Hall Street, New Street and 2MT callsign". www.g0mwt.org.uk.
 "BBC History â The BBC takes to the Airwaves". BBC News.
 Atgelt, Carlos A. "Early History of Radio Broadcasting in Argentina." The Broadcast Archive (Oldradio.com).
 "Curry College - Home". www.curry.edu. Retrieved July 13, 2018.
 Neira, Bob. "Broadcasting". modestoradiomuseum. modestoradiomuseum.
 "What is a Radio Station?". Radio World. p. 6.
 Based on the "interference" entry of The Concise Oxford English Dictionary, 11th edition, online
 "Types of Technology, FM vs AM". kwarner.bravehost.com. July 13, 2012. Archived from the original on July 13, 2012. Retrieved August 10, 2019.
 Grodkowski, Paul (August 24, 2015). Beginning Shortwave Radio Listening. Booktango. ISBN 9781468964240.
 Halper, Donna L. "John Shepard's FM StationsâAmerica's first FM network." Boston Radio Archives (BostonRadio.org).
 "The Yankee Network in 1936". The Archives @ BostonRadio.org. Retrieved August 10, 2019.
 "FM Broadcasting Chronology". Jeff Miller Pages. June 23, 2017. Retrieved August 10, 2019.
Further reading
Briggs Asa. The History of Broadcasting in the United Kingdom (Oxford University Press, 1961).
Crisell, Andrew. An Introductory History of British Broadcasting (2002) excerpt
Ewbank Henry and Lawton Sherman P. Broadcasting: Radio and Television (Harper & Brothers, 1952).
Fisher, Marc. Something In The Air: Radio, Rock, and the Revolution That Shaped A Generation (Random House, 2007).
Hausman, Carl, Messere, Fritz, Benoit, Philip, and O'Donnell, Lewis, Modern Radio Production, 9th ed., (Cengage, 2013)
Head, Sydney W., Christopher W. Sterling, and Lemuel B. Schofield. Broadcasting in America." (7th ed. 1994).
Lewis, Tom, Empire of the Air: The Men Who Made Radio, 1st ed., New York : E. Burlingame Books, 1991. ISBN 0-06-018215-6. "Empire of the Air: The Men Who Made Radio" (1992) by Ken Burns was a PBS documentary based on the book.
Pilon, Robert, Isabelle Lamoureux, and Gilles Turcotte. Le MarchÃ© de la radio au QuÃ©bec: document de reference. [MontrÃ©al]: Association quÃ©bÃ©coise de l'industrie du dique, du spectacle et de la video, 1991. unpaged. N.B.: Comprises: Robert Pilon's and Isabelle Lamoureux' Profil du marchÃ© de radio au QuÃ©bec: un analyse de MÃ©dia-culture. -- Gilles Turcotte's Analyse comparative de l'Ã©coute des principals stations de MontrÃ©al: prepare par Info Cible.
Ray, William B. FCC: The Ups and Downs of Radio-TV Regulation (Iowa State University Press, 1990).
Russo, Alexan der. Points on the Dial: Golden Age Radio Beyond the Networks (Duke University Press; 2010) 278 pages; discusses regional and local radio as forms that "complicate" the image of the medium as a national unifier from the 1920s to the 1950s.
Scannell, Paddy, and Cardiff, David. A Social History of British Broadcasting, Volume One, 1922-1939 (Basil Blackwell, 1991).
Schramm, Wilbur, ed. The Process and Effects of Mass Communication (1955 and later editions) articles by social scientists
Schramm, Wilbur, ed. Mass Communication (1950, 2nd ed. 1960); more popular essays
Schwoch James. The American Radio Industry and Its Latin American Activities, 1900-1939 (University of Illinois Press, 1990).
Stewart, Sandy. From Coast to Coast: a Personal History of Radio in Canada (Entreprises Radio-Canada, 1985). xi, 191 p., ill., chiefly with b&w photos. ISBN 0-88794-147-8
Stewart, Sandy. A Pictorial History of Radio in Canada (Gage Publishing, 1975). v, [1], 154 p., amply ill. in b&w mostly with photos. SBN 7715-9948-X
White Llewellyn. The American Radio (University of Chicago Press, 1947).
External links
	Look up radio broadcasting in Wiktionary, the free dictionary.
General
Federal Communications Commission website - fcc.gov
DXing.info - Information about radio stations worldwide
Radio-Locator.com- Links to 13,000 radio stations worldwide
BBC reception advice
DXradio.50webs.com "The SWDXER" - with general SWL information and radio antenna tips
RadioStationZone.com - 10.000+ radio stations worldwide with ratings, comments and listen live links
Online-Radio-Stations.org - The Web Radio Tuner has a comprehensive list of over 50.000 radio stations
UnwantedEmissions.com - A general reference to radio spectrum allocations
Radio stanice - Search for radio stations throughout the Europe
Radio Emisoras Latinas - has a directory with thousands of Latin America Radio Stations
MY FM Radio Live - MY FM Radio Live - Internet radio broadcast
vte
Broadcasting
Links to related articles
Categories: Radio broadcasting
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
ÙØ§Ø±Ø³Û
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
à®¤à®®à®¿à®´à¯
TÃ¼rkÃ§e
Ø§Ø±Ø¯Ù
14 more
Edit links
This page was last edited on 26 October 2020, at 19:38 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

FM broadcasting
From Wikipedia, the free encyclopedia
  (Redirected from FM radio)
Jump to navigationJump to search

AM and FM modulated signals for radio. AM (amplitude modulation) and FM (frequency modulation) are types of modulation (coding). The sound of the program material, usually coming from a radio studio, is used to modulate (vary) a carrier wave of a specific frequency, then broadcast.

In AM broadcasting, the amplitude of the carrier wave is modulated to encode the original sound. In FM broadcasting, the frequency of the carrier wave is modulated to encode the sound. A radio receiver extracts the original program sound from the modulated radio signal and reproduces the sound in a loudspeaker.

Position of FM radio in the electromagnetic spectrum

A commercial 35 kW FM radio transmitter built in the late 1980s. It belongs to FM radio station KWNR in Henderson, Nevada and broadcasts at a frequency of 95.5 MHz.
FM broadcasting is a method of radio broadcasting using frequency modulation (FM). Invented in 1933 by American engineer Edwin Armstrong, wide-band FM is used worldwide to provide high fidelity sound over broadcast radio. FM broadcasting is capable of higher fidelityâthat is, more accurate reproduction of the original program soundâthan other broadcasting technologies, such as AM broadcasting. Therefore, FM is used for most broadcasts of music or general audio (in the audio spectrum). FM radio stations use the very high frequency range of radio frequencies.


Contents
1	Broadcast bands
2	Technology
2.1	Modulation
2.2	Pre-emphasis and de-emphasis
2.3	Stereo FM
2.4	Quadraphonic FM
2.5	Noise reduction
2.6	Other subcarrier services
2.7	Transmission power
2.8	Reception distance
3	History
3.1	United States
3.2	Europe
3.2.1	United Kingdom
3.2.2	Italy
3.2.3	Greece
3.3	Australia
3.4	New Zealand
3.5	Trinidad and Tobago
3.6	Turkey
3.7	Other countries
3.8	ITU Conferences about FM
4	FM broadcasting switch-off
5	Small-scale use of the FM broadcast band
5.1	Consumer use of FM transmitters
5.2	Assistive listening
5.3	Microbroadcasting
5.4	Clandestine use of FM transmitters
6	See also
6.1	FM broadcasting by country
6.2	FM broadcasting (technical)
6.3	Lists
6.4	History
6.5	Bands
7	References
8	External links
Broadcast bands
Main article: FM broadcast band
Throughout the world, the FM broadcast band falls within the VHF part of the radio spectrum. Usually 87.5 to 108.0 MHz is used,[1] or some portion thereof, with few exceptions:

In the former Soviet republics, and some former Eastern Bloc countries, the older 65.8â74 MHz band is also used. Assigned frequencies are at intervals of 30 kHz. This band, sometimes referred to as the OIRT band, is slowly being phased out. Where the OIRT band is used, the 87.5â108.0 MHz band is referred to as the CCIR band.
In Japan, the band 76â95 MHz is used.
The frequency of an FM broadcast station (more strictly its assigned nominal center frequency) is usually a multiple of 100 kHz. In most of South Korea, the Americas, the Philippines and the Caribbean, only odd multiples are used. Some other countries follow this plan because of the import of vehicles, principally from the United States, with radios that can only tune to these frequencies. In some parts of Europe, Greenland and Africa, only even multiples are used. In the UK odd or even are used. In Italy, multiples of 50 kHz are used. In most countries the maximum permitted frequency error of the unmodulated carrier is specified, which typically should be within 2000 Hz of the assigned frequency.[2][3]

There are other unusual and obsolete FM broadcasting standards in some countries, with non-standard spacings of 1, 10, 30, 74, 500, and 300 kHz. However, to minimise inter-channel interference, stations operating from the same or geographically close transmitter sites tend to keep to at least a 500 kHz frequency separation even when closer frequency spacing is technically permitted. The ITU publishes Protection Ratio graphs which give the minimum spacing between frequencies based on their relative strengths.[4] Only broadcast stations with large enough geographic separations between their coverage areas can operate on close or the same frequencies.

Technology

FM has better rejection of static (RFI) than AM. This was shown in a dramatic demonstration by General Electric at its New York lab in 1940. The radio had both AM and FM receivers. With a million-volt arc as a source of interference behind it, the AM receiver produced a roar of static, while the FM receiver clearly reproduced a music program from Armstrong's experimental FM transmitter in New Jersey.

Crossed dipole antenna of station KENZ's 94.9 MHz, 48 kW transmitter on Lake Mountain, Utah. It radiates circularly polarized radio waves.
Modulation
Frequency modulation or FM is a form of modulation which conveys information by varying the frequency of a carrier wave; the older amplitude modulation or AM varies the amplitude of the carrier, with its frequency remaining constant. With FM, frequency deviation from the assigned carrier frequency at any instant is directly proportional to the amplitude of the input signal, determining the instantaneous frequency of the transmitted signal. Because transmitted FM signals use more bandwidth than AM signals, this form of modulation is commonly used with the higher (VHF or UHF) frequencies used by TV, the FM broadcast band, and land mobile radio systems.

The maximum frequency deviation of the carrier is usually specified and regulated by the licensing authorities in each country. For a stereo broadcast, the maximum permitted carrier deviation is invariably Â±75 kHz, although a little higher is permitted in the United States when SCA systems are used. For a monophonic broadcast, again the most common permitted maximum deviation is Â±75 kHz. However, some countries specify a lower value for monophonic broadcasts, such as Â±50 kHz.[5]


Armstrong's first prototype FM broadcast transmitter, located in the Empire State Building, New York City, which he used for secret tests of his system between 1934 and 1935. Licensed as experimental station W2XDG, it transmitted on 41 MHz at a power of 2 kW

Instantaneous spectrum and waterfall plot in the FM broadcast band showing three strong local stations; speech and music show different patterns of frequency vs. time. When the transmitted audio is quiet, the 19 kHz stereo pilot tones can be resolved in the spectrum.
Pre-emphasis and de-emphasis
Random noise has a triangular spectral distribution in an FM system, with the effect that noise occurs predominantly at the highest audio frequencies within the baseband. This can be offset, to a limited extent, by boosting the high frequencies before transmission and reducing them by a corresponding amount in the receiver. Reducing the high audio frequencies in the receiver also reduces the high-frequency noise. These processes of boosting and then reducing certain frequencies are known as pre-emphasis and de-emphasis, respectively.

The amount of pre-emphasis and de-emphasis used is defined by the time constant of a simple RC filter circuit. In most of the world a 50 Âµs time constant is used. In the Americas and South Korea, 75 Âµs is used.[6] This applies to both mono and stereo transmissions. For stereo, pre-emphasis is applied to the left and right channels before multiplexing.

The use of pre-emphasis becomes a problem because of the fact that many forms of contemporary music contain more high-frequency energy than the musical styles which prevailed at the birth of FM broadcasting. Pre-emphasizing these high-frequency sounds would cause excessive deviation of the FM carrier. Modulation control (limiter) devices are used to prevent this. Systems more modern than FM broadcasting tend to use either programme-dependent variable pre-emphasis; e.g., dbx in the BTSC TV sound system, or none at all.

Pre-emphasis and de-emphasis was used in the earliest days of FM broadcasting. According to a BBC report from 1946,[7] 100 Âµs was originally considered in the US, but 75 Âµs subsequently adopted.

Stereo FM
Long before FM stereo transmission was considered, FM multiplexing of other types of audio level information was experimented with.[8] Edwin Armstrong who invented FM was the first to experiment with multiplexing, at his experimental 41 MHz station W2XDG located on the 85th floor of the Empire State Building in New York City.

These FM multiplex transmissions started in November 1934 and consisted of the main channel audio program and three subcarriers: a fax program, a synchronizing signal for the fax program and a telegraph "order" channel. These original FM multiplex subcarriers were amplitude modulated.

Two musical programs, consisting of both the Red and Blue Network program feeds of the NBC Radio Network, were simultaneously transmitted using the same system of subcarrier modulation as part of a studio-to-transmitter link system. In April 1935, the AM subcarriers were replaced by FM subcarriers, with much improved results.

The first FM subcarrier transmissions emanating from Major Armstrong's experimental station KE2XCC at Alpine, New Jersey occurred in 1948. These transmissions consisted of two-channel audio programs, binaural audio programs and a fax program. The original subcarrier frequency used at KE2XCC was 27.5 kHz. The IF bandwidth was Â±5 kHz, as the only goal at the time was to relay AM radio-quality audio. This transmission system used 75 Âµs audio pre-emphasis like the main monaural audio and subsequently the multiplexed stereo audio.

In the late 1950s, several systems to add stereo to FM radio were considered by the FCC. Included were systems from 14 proponents including Crosby, Halstead, Electrical and Musical Industries, Ltd (EMI), Zenith, and General Electric. The individual systems were evaluated for their strengths and weaknesses during field tests in Uniontown, Pennsylvania, using KDKA-FM in Pittsburgh as the originating station. The Crosby system was rejected by the FCC because it was incompatible with existing subsidiary communications authorization (SCA) services which used various subcarrier frequencies including 41 and 67 kHz. Many revenue-starved FM stations used SCAs for "storecasting" and other non-broadcast purposes. The Halstead system was rejected due to lack of high frequency stereo separation and reduction in the main channel signal-to-noise ratio. The GE and Zenith systems, so similar that they were considered theoretically identical, were formally approved by the FCC in April 1961 as the standard stereo FM broadcasting method in the United States and later adopted by most other countries.[9][10] It is important that stereo broadcasts be compatible with mono receivers. For this reason, the left (L) and right (R) channels are algebraically encoded into sum (L+R) and difference (LâR) signals. A mono receiver will use just the L+R signal so the listener will hear both channels through the single loudspeaker. A stereo receiver will add the difference signal to the sum signal to recover the left channel, and subtract the difference signal from the sum to recover the right channel.

The (L+R) Main channel signal is transmitted as baseband audio limited to the range of 30 Hz to 15 kHz. The 15 kHz upper limit for the baseband audio is necessary to allow protection of the 19 kHz pilot signal using low-pass filters. The (LâR) signal is amplitude modulated onto a 38 kHz double-sideband suppressed-carrier (DSB-SC) signal occupying the baseband range from 23 kHz to 53 kHz. A 19 kHz Â± 2 Hz[11] pilot tone, at exactly half the 38 kHz sub-carrier frequency and with a precise phase relationship to it, as defined by the formula below, is also generated. This is transmitted at 8â10% of overall modulation level and used by the receiver to identify a stereo transmission and to regenerate the 38 kHz sub-carrier with the correct phase. The final multiplex signal from the stereo generator contains the Main Channel (L+R), the pilot tone, and the sub-channel (LâR). This composite signal, along with any other sub-carriers, modulates the FM transmitter. The terms composite, multiplex and even MPX are used interchangeably to describe this signal.

The instantaneous deviation of the transmitter carrier frequency due to the stereo audio and pilot tone (at 10% modulation) is

{\displaystyle \left[0.9\left[{\frac {A+B}{2}}+{\frac {A-B}{2}}\sin 4\pi f_{p}t\right]+0.1\sin 2\pi f_{p}t\right]\times 75~\mathrm {kHz} }\left[0.9\left[{\frac {A+B}{2}}+{\frac {A-B}{2}}\sin 4\pi f_{p}t\right]+0.1\sin 2\pi f_{p}t\right]\times 75~\mathrm {kHz} [12][13]
where A and B are the pre-emphasized left and right audio signals and {\displaystyle f_{p}}f_{p}=19 kHz is the frequency of the pilot tone. Slight variations in the peak deviation may occur in the presence of other subcarriers or because of local regulations.

Another way to look at the resulting signal is that it alternates between left and right at 38 kHz, with the phase determined by the 19 kHz pilot signal.[14] Most stereo encoders use this switching technique to generate the 38 kHz subcarrier, but practical encoder designs need to incorporate circuitry to deal with the switching harmonics. Converting the multiplex signal back into left and right audio signals is performed by a decoder, built into stereo receivers. Again, the decoder can use a switching technique to recover the left and right channels.

In addition, for a given RF level at the receiver, the signal-to-noise ratio and multipath distortion for the stereo signal will be worse than for the mono receiver.[15] For this reason many stereo FM receivers include a stereo/mono switch to allow listening in mono when reception conditions are less than ideal, and most car radios are arranged to reduce the separation as the signal-to-noise ratio worsens, eventually going to mono while still indicating a stereo signal is being received. As with monaural transmission, it is normal practice to apply pre-emphasis to the left and right channels before encoding and to apply de-emphasis at the receiver after decoding.

In the U.S. around 2010, using single-sideband modulation for the stereo subcarrier was proposed.[16][17] It was theorized to be more spectrum-efficient and to produce a 4dB s/n improvement at the receiver, and it was claimed that multipath distortion would be reduced as well. A handful of radio stations around the country broadcast stereo in this way, under FCC experimental authority. It may not be compatible with very old receivers, but it is claimed that no difference can be heard with most newer receivers. At present, the FCC rules do not allow this mode of stereo operation.[18]

Quadraphonic FM
In 1969, Louis Dorren invented the Quadraplex system of single station, discrete, compatible four-channel FM broadcasting. There are two additional subcarriers in the Quadraplex system, supplementing the single one used in standard stereo FM. The baseband layout is as follows:

50 Hz to 15 kHz main channel (sum of all 4 channels) (LF+LR+RF+RR) signal, for mono FM listening compatibility.
23 to 53 kHz (sine quadrature subcarrier) (LF+LR) â (RF+RR) left minus right difference signal. This signal's modulation in algebraic sum and difference with the main channel is used for 2 channel stereo listener compatibility.
23 to 53 kHz (cosine quadrature 38 kHz subcarrier) (LF+RR) â (LR+RF) Diagonal difference. This signal's modulation in algebraic sum and difference with the main channel and all the other subcarriers is used for the Quadraphonic listener.
61 to 91 kHz (sine quadrature 76 kHz subcarrier) (LF+RF) â (LR+RR) Front-back difference. This signal's modulation in algebraic sum and difference with the main channel and all the other subcarriers is also used for the Quadraphonic listener.
105 kHz SCA subcarrier, phase-locked to 19 kHz pilot, for reading services for the blind, background music, etc.
The normal stereo signal can be considered as switching between left and right channels at 38 kHz, appropriately band-limited. The quadraphonic signal can be considered as cycling through LF, LR, RF, RR, at 76 kHz.[19]

Early efforts to transmit discrete four-channel quadraphonic music required the use of two FM stations; one transmitting the front audio channels, the other the rear channels. A breakthrough came in 1970 when KIOI (K-101) in San Francisco successfully transmitted true quadraphonic sound from a single FM station using the Quadraplex system under Special Temporary Authority from the FCC. Following this experiment, a long-term test period was proposed that would permit one FM station in each of the top 25 U.S. radio markets to transmit in Quadraplex. The test results hopefully would prove to the FCC that the system was compatible with existing two-channel stereo transmission and reception and that it did not interfere with adjacent stations.

There were several variations on this system submitted by GE, Zenith, RCA, and Denon for testing and consideration during the National Quadraphonic Radio Committee field trials for the FCC. The original Dorren Quadraplex System outperformed all the others and was chosen as the national standard for Quadraphonic FM broadcasting in the United States. The first commercial FM station to broadcast quadraphonic program content was WIQB (now called WWWW-FM) in Ann Arbor/Saline, Michigan under the guidance of Chief Engineer Brian Jeffrey Brown.[20]

Noise reduction
Various attempts to add analog noise reduction to FM broadcasting were carried out in the 1970s and 1980s:

A commercially unsuccessful noise reduction system used with FM radio in some countries during the late 1970s, Dolby FM was similar to Dolby B[21] but used a modified 25 Âµs pre-emphasis time constant and a frequency selective companding arrangement to reduce noise. The pre-emphasis change compensates for the excess treble response that otherwise would make listening difficult for those without Dolby decoders.

A similar system named High Com FM was tested in Germany between July 1979 and December 1981 by IRT. It was based on the Telefunken High Com broadband compander system, but was never introduced commercially in FM broadcasting.[22]

Yet another system was the CX-based noise reduction system FMX implemented in some radio broadcasting stations in the United States in the 1980s.

Other subcarrier services

Typical spectrum of composite baseband signal, including DirectBand and a subcarrier on 92 kHz
FM broadcasting has included subsidiary communications authorization (SCA) services capability since its inception, as it was seen as another service which licensees could use to create additional income.[23] Use of SCAs was particularly popular in the US, but much less so elsewhere. Uses for such subcarriers include radio reading services for the blind,[24] which became common and remain so, private data transmission services (for example sending stock market information to stockbrokers or stolen credit card number denial lists to stores,[citation needed]) subscription commercial-free background music services for shops, paging ("beeper") services, non-native-language programming, and providing a program feed for AM transmitters of AM/FM stations. SCA subcarriers are typically 67 kHz and 92 kHz. Initially the users of SCA services were private analog audio channels which could be used internally or leased, for example Muzak-type services. There were experiments with quadraphonic sound. If a station does not broadcast in stereo, everything from 23 kHz on up can be used for other services. The guard band around 19 kHz (Â±4 kHz) must still be maintained, so as not to trigger stereo decoders on receivers. If there is stereo, there will typically be a guard band between the upper limit of the DSBSC stereo signal (53 kHz) and the lower limit of any other subcarrier.

Digital services are now also available. A 57 kHz subcarrier (phase locked to the third harmonic of the stereo pilot tone) is used to carry a low-bandwidth digital Radio Data System signal, providing extra features such as station name, Alternative Frequency (AF), traffic data for commercial GPS receivers[25] and Radio text (RT). This narrowband signal runs at only 1,187.5 bits per second, thus is only suitable for text. A few proprietary systems are used for private communications. A variant of RDS is the North American RBDS or "smart radio" system. In Germany the analog ARI system was used prior to RDS to alert motorists that traffic announcements were being broadcast (without disturbing other listeners). Plans to use ARI for other European countries led to the development of RDS as a more powerful system. RDS is designed to be capable of being used alongside ARI despite using identical subcarrier frequencies.

In the United States and Canada, digital radio services are being deployed within the FM band rather than using Eureka 147 or the Japanese standard ISDB. This in-band on-channel approach, as do all digital radio techniques, makes use of advanced compressed audio. The proprietary iBiquity system, branded as "HD Radio", currently is authorized for "hybrid" mode operation, wherein both the conventional analog FM carrier and digital sideband subcarriers are transmitted. Eventually, presuming widespread deployment of HD Radio receivers, the analog services could theoretically be discontinued and the FM band become all digital.

Transmission power
The output power of a FM broadcasting transmitter is one of the parameters that governs how far a transmission will cover. The other important parameters are the height of the transmitting antenna and the Antenna gain. Transmitter powers should be carefully chosen so that the required area is covered without causing interference to other stations further away. Practical transmitter powers range from a few milliwatts to 80 kW. As transmitter powers increase above a few kilowatts, the operating costs become high and only viable for large stations. The efficiency of larger transmitters is now better than 70% (AC power in to RF power out) for FM only transmission. This compares to 50% before high efficiency switch-mode power supplies and LDMOS amplifiers were used. Efficiency drops dramatically if any digital HD Radio service is added.

Reception distance
VHF Radio waves usually do not travel far beyond the visual horizon, so reception distances for FM stations are typically limited to 30â40 miles (48â64 km). They can also be blocked by hills and to a lesser extent by buildings. Individuals with more-sensitive receivers or specialized antenna systems, or who are located in areas with more favorable topography, may be able to receive useful FM broadcast signals at somewhat greater distances.

The knife edge effect can permit reception where there is no direct line of sight between broadcaster and receiver. The reception can vary considerably depending on the position. One example is the UÄka mountain range, which makes constant reception of Italian signals from Veneto and Marche possible in a good portion of Rijeka, Croatia, despite the distance being over 200 km.[citation needed] Other radio propagation effects such as tropospheric ducting and Sporadic E can occasionally allow distant stations to be intermittently received over very large distances, but cannot be relied on for commercial broadcast purposes. Good reception across the country, is one of the main advantages over DAB/+ radio.

This is still less than the range of AM radio waves, which because of their lower frequency can travel as ground waves or reflect off the ionosphere, so AM radio stations can be received at hundreds (sometimes thousands) of miles. This is a property of the carrier wave's typical frequency (and power), not its mode of modulation.

The range of FM transmission is related to the transmitter's RF power, the antenna gain, and antenna height. Interference from other stations is also a factor in some places. In the U.S, the FCC publishes curves that aid in calculation of this maximum distance as a function of signal strength at the receiving location. Computer modelling is more commonly used for this around the world.

Many FM stations, especially those located in severe multipath areas, use extra audio compression/processing to keep essential sound above the background noise for listeners, occasionally at the expense of overall perceived sound quality. In such instances, however, this technique is often surprisingly effective in increasing the station's useful range.[citation needed].

History
For broader coverage of this topic, see History of radio and History of broadcasting.
See also: AM broadcasting Â§ History

One of the first FM radio stations, Edwin Armstrong's experimental station W2XMN in Alpine, New Jersey, USA. The insets show a part of the transmitter, and a map of FM stations in 1940. The tower still stands today.
United States
FM broadcasting began in the late 1930s, when it was initiated by a handful of early pioneer stations including W1XOJ/WGTR and W1XTG/WSRS, both transmitting from Paxton, Massachusetts (now listed as Worcester, Massachusetts); W1XSL/W1XPW/WDRC-FM, Meriden, Connecticut (now WHCN); W2XMN/KE2XCC/WFMN, Alpine, New Jersey (owned by Edwin Armstrong himself, closed down upon Armstrong's death in 1954); W2XQR/WQXQ/WQXR-FM, New York; W47NV/WSM-FM Nashville, Tennessee (signed off in 1951); W1XER/W39B/WMNE, whose studios were in Boston but whose transmitter was atop the highest mountain in the northeast United States, Mount Washington, New Hampshire (shut down in 1948); W9XAO Milwaukee, Wisconsin (later WTMJ-FM, went off air in 1950, returned in 1959 on another frequency). Also of note are General Electric stations W2XDA Schenectady and W2XOY New Scotland, New Yorkâtwo experimental frequency modulation transmitters on 48.5 MHzâwhich signed on in 1939. The two were merged into one station using the W2XOY call letters on November 20, 1940, with the station taking the WGFM call letters a few years later, and moving to 99.5 MHz when the FM band was relocated to the 88â108 MHz portion of the radio spectrum. General Electric sold the station in the 1980s, and today the station is called WRVE.

WEFM (in the Chicago area) and WGFM (in Schenectady, New York) were reported as the first stereo stations.[26]

The first commercial FM broadcasting stations were in the United States, but initially they were primarily used to simulcast their AM sister stations, to broadcast lush orchestral music for stores and offices, to broadcast classical music to an upmarket listenership in urban areas, or for educational programming.[27] By the late 1960s, FM had been adopted for broadcast of stereo "A.O.R.â'Album Oriented Rock' Format", but it was not until 1978 that listenership to FM stations exceeded that of AM stations in North America. During the 1980s and 1990s, Top 40 music stations and later even country music stations largely abandoned AM for FM. Today AM is mainly the preserve of talk radio, news, sports, religious programming, ethnic (minority language) broadcasting and some types of minority interest music. This shift has transformed AM into the "alternative band" that FM once was. (Some AM stations have begun to simulcast on, or switch to, FM signals to attract younger listeners and aid reception problems in buildings, during thunderstorms, and near high-voltage wires. Some of these stations now emphasize their presence on the FM dial.)

Europe
The medium wave band (known as the AM band because most stations using it employ amplitude modulation) was overcrowded[citation needed] in western Europe, leading to interference problems and, as a result, many MW frequencies are suitable only for speech broadcasting.

Belgium, the Netherlands, Denmark and particularly Germany were among the first countries to adopt FM on a widespread scale. Among the reasons for this were:

The medium wave band in Western Europe became overcrowded after World War II, mainly due to the best available medium wave frequencies being used at high power levels by the Allied Occupation Forces, both for broadcasting entertainment to their troops and for broadcasting Cold War propaganda across the Iron Curtain.
After World War II, broadcasting frequencies were reorganized and reallocated by delegates of the victorious countries in the Copenhagen Frequency Plan. German broadcasters were left with only two remaining AM frequencies and were forced to look to FM for expansion.
Public service broadcasters in Ireland and Australia were far slower at adopting FM radio than those in either North America or continental Europe.

United Kingdom
In the United Kingdom the BBC conducted tests during the 1940s,[7] then began FM broadcasting in 1955, with three national networks: the Light Programme, Third Programme and Home Service. These three networks used the sub-band 88.0â94.6 MHz. The sub-band 94.6â97.6 MHz was later used for BBC and local commercial services.

However, only when commercial broadcasting was introduced to the UK in 1973 did the use of FM pick up in Britain. With the gradual clearance of other users (notably Public Services such as police, fire and ambulance) and the extension of the FM band to 108.0 MHz between 1980 and 1995, FM expanded rapidly throughout the British Isles and effectively took over from LW and MW as the delivery platform of choice for fixed and portable domestic and vehicle-based receivers. In addition, Ofcom (previously the Radio Authority) in the UK issues on demand Restricted Service Licences on FM and also on AM (MW) for short-term local-coverage broadcasting which is open to anyone who does not carry a prohibition and can put up the appropriate licensing and royalty fees. In 2010 around 450 such licences were issued.

When the BBC's radio networks were renamed Radio 2, Radio 3 and Radio 4 respectively in 1967 to coincide with the launch of Radio 1, the new station was the only one of the main four to not have an FM frequency allocated, which was the case for 21 years. Instead, Radio 1 shared airtime with Radio 2 FM, on Saturday afternoons, Sunday evenings, weekday evenings (10pm to midnight) and Bank Holidays, eventually having its own FM frequency starting in London in October 1987 on 104.8 MHz at Crystal Palace. Eventually in 1987 a frequency range of 97.6-99.8 MHz was allocated as police relay transmitters were moved from the 100 MHz frequency, starting in London before being broadly completed by 1989, where Radio 1 in London moved from the latter frequency to 98.8 MHz to the BBC's Wrotham transmitter. This followed the BBC Radio 1 FM frequencies being rolled out to the rest of the UK.[28]

Italy
Italy adopted FM broadcast widely in the early 1970s, but first experiments made by RAI dated back to 1950,[29] when the "movement for free radio", developed by so-called "pirates", forced the recognition of free speech rights also through the use of "free radio media such as Broadcast transmitters", and took the case to the Constitutional Court of Italy. The court finally decided in favor of Free Radio. Just weeks after the court's final decision there was an "FM radio boom" involving small private radio stations across the country. By the mid 1970s, every city in Italy had a crowded FM radio spectrum.

Greece
Greece was another European country where the FM radio spectrum was used at first by the so-called "pirates" (both in Athens and Thessaloniki, the two major Greek cities) in the mid-1970s, before any national stations had started broadcasting on it; there were many AM (MW) stations in use for the purpose. No later than the end of 1977, the national public service broadcasting company EIRT (later also known as ERT) placed in service its first FM transmitter in the capital, Athens. By the end of the 1970s, most of Greek territory was covered by three National FM programs, and every city had many FM "pirates" as well. The adaptation of the FM band for privately owned commercial radio stations came far later, in 1987.

Australia
FM broadcasting started in Australian capital cities in 1947 on an "experimental" basis, using an ABC national network feed, consisting largely of classical music and Parliament, as a programme source. It had a very small audience and was shut down in 1961 ostensibly to clear the television band: TV channel 5 (102.250 video carrier) if allocated would fall within the VHF FM band (98â108 MHz). The official policy on FM at the time was to eventually introduce it on another band, which would have required FM tuners custom-built for Australia. This policy was finally reversed and FM broadcasting was reopened in 1975 using the VHF band, after the few encroaching TV stations had been moved. Subsequently, it developed steadily until in the 1980s many AM stations transferred to FM due to its superior sound quality and lower operating costs. Today, as elsewhere in the developed world, most urban Australian broadcasting is on FM, although AM talk stations are still very popular. Regional broadcasters still commonly operate AM stations due to the additional range the broadcasting method offers. Some stations in major regional centres simulcast on AM and FM bands. Digital radio using the DAB+ standard has been rolled out to capital cities.

New Zealand
Like Australia, New Zealand adopted the FM format relatively late. As was the case with privately owned AM radio in the late 1960s, it took a spate of 'pirate' broadcasters to persuade a control-oriented, technology-averse government to allow FM to be introduced after at least five years of consumer campaigning starting in the mid-1970s, particularly in Auckland. An experimental FM station, FM 90.7, was broadcast in Whakatane in early 1982. Later that year, Victoria University of Wellington's Radio Active began full-time FM transmissions. Commercial FM licences were finally approved in 1983, with Auckland-based 91FM and 89FM being the first to take up the offer.New Zealand Pirates. Broadcasting was deregulated in 1989.

Like many other countries in Africa and Asia that drive on the left, New Zealand imports vehicles from Japan. The standard radios in these vehicles operate on 76-to-90 MHz, which is not compatible with the 88-to-108 MHz range. Imported cars with Japanese radios can have FM expanders installed which down-convert the higher frequencies above 90 MHz. New Zealand has no indigenous car manufacturers.

Trinidad and Tobago
Trinidad and Tobago's first FM Radio station was 95.1FM, now rebranded as 951 Remix, was launched in March 1976 by the TBC Radio Network.

Turkey
In Turkey, FM broadcasting began in the late 1960s, carrying several shows from the One television network which was transferred from the AM frequency (also known as MW in Turkey). In subsequent years, more MW stations were slowly transferred to FM, and by the end of the 1970s, most radio stations that were previously on MW had been moved to FM, though many talk, news and sport, but mostly religious stations, still remain on MW.

Other countries
Most other countries implemented FM broadcasting through 1960s and expanded their use of FM through the 1990s. Because it takes a large number of FM transmitting stations to cover a geographically large country, particularly where there are terrain difficulties, FM is more suited to local broadcasting than for national networks. In such countries, particularly where there are economic or infrastructural problems, "rolling out" a national FM broadcast network to reach the majority of the population can be a slow and expensive process. Despite this, mostly in east European counties, national FM broadcast networks were established in the late 1960s and 1970s. In all Soviet-dependent countries but GDR, the OIRT band was used. First restricted to 68â73 MHz with 100 kHz channel spacing, then in the 1970s eventually expanded to 65.84â74.00 MHz with 30 kHz channel spacing.[30]

The use of FM for domestic radio encouraged listeners to acquire cheap FM-only receivers and so reduced the number able to listen to longer-range AM foreign broadcasters. Similar considerations led to domestic radio in South Africa switching to FM in the 1960s.[citation needed]

ITU Conferences about FM
The frequencies available for FM were decided by some important conferences of ITU. The milestone of those conferences is the Stockholm agreement of 1961 among 38 countries.[31] A 1984 conference in Geneva made some modifications to the original Stockholm agreement particularly in the frequency range above 100 MHz.

FM broadcasting switch-off
See also: Digital audio broadcasting
In 2017, Norway became the first country so far to completely switch to Digital audio broadcasting, the exception being some local stations remaining on FM until 2027. The switchover to DAB+ meant that especially rural areas obtained a far more diverse radio content compared to the FM-only period; several new radio stations had started transmissions on DAB+ in the years before the FM switch-off.

Small-scale use of the FM broadcast band

Belkin TuneCast II FM microtransmitter
Consumer use of FM transmitters
In some countries, small-scale (Part 15 in United States terms) transmitters are available that can transmit a signal from an audio device (usually an MP3 player or similar) to a standard FM radio receiver; such devices range from small units built to carry audio to a car radio with no audio-in capability (often formerly provided by special adapters for audio cassette decks, which are becoming less common on car radio designs) up to full-sized, near-professional-grade broadcasting systems that can be used to transmit audio throughout a property. Most such units transmit in full stereo, though some models designed for beginner hobbyists might not. Similar transmitters are often included in satellite radio receivers and some toys.

Legality of these devices varies by country. The U.S. Federal Communications Commission and Industry Canada allow them. Starting on 1 October 2006, these devices became legal in most countries in the European Union. Devices made to the harmonised European specification became legal in the UK on 8 December 2006.[32]

The FM broadcast band is also used by some inexpensive wireless microphones sold as toys for karaoke or similar purposes, allowing the user to use an FM radio as an output rather than a dedicated amplifier and speaker. Professional-grade wireless microphones generally use bands in the UHF region so they can run on dedicated equipment without broadcast interference.

Some wireless headphones transmit in the FM broadcast band, with the headphones tunable to only a subset of the broadcast band. Higher-quality wireless headphones use infrared transmission or UHF ISM bands such as 315 MHz, 863 MHz, 915 MHz, or 2.4 GHz instead of the FM broadcast band.

Assistive listening
Some assistive listening devices are based on FM radio, mostly using the 72.1 to 75.8 MHz band. Aside from the assisted listening receivers, only certain kinds of FM receivers can tune to this band.[33]

Microbroadcasting
Low-power transmitters such as those mentioned above are also sometimes used for neighborhood or campus radio stations, though campus radio stations are often run over carrier current. This is generally considered a form of microbroadcasting. As a general rule,[vague] enforcement towards low-power FM stations is stricter than with AM stations, due to problems such as the capture effect,[citation needed] and as a result, FM microbroadcasters generally do not reach as far as their AM competitors.

Clandestine use of FM transmitters
FM transmitters have been used to construct miniature wireless microphones for espionage and surveillance purposes (covert listening devices or so-called "bugs"); the advantage to using the FM broadcast band for such operations is that the receiving equipment would not be considered particularly suspect. Common practice is to tune the bug's transmitter off the ends of the broadcast band, into what in the United States would be TV channel 6 (<87.9 MHz) or aviation navigation frequencies (>107.9 MHz); most FM radios with analog tuners have sufficient overcoverage to pick up these slightly-beyond-outermost frequencies, although many digitally tuned radios have not.

Constructing a "bug" is a common early project for electronics hobbyists, and project kits to do so are available from a wide variety of sources. The devices constructed, however, are often too large and poorly shielded for use in clandestine activity.

In addition, much pirate radio activity is broadcast in the FM range, because of the band's greater clarity and listenership, the smaller size and lower cost of equipment.

See also
FM broadcasting by country
FM broadcasting in Australia
FM broadcasting in Canada
FM broadcasting in Egypt
FM broadcasting in India
FM broadcasting in Japan
FM broadcasting in New Zealand
FM broadcasting in Pakistan
FM broadcasting in the UK
FM broadcasting in the United States
FM broadcasting (technical)
AM broadcasting
AM stereo (related technology)
FM broadcast band
FM stereo
Frequency modulation
Long-distance FM reception (FM DX)
Ripping music from FM broadcasts
RDS (Radio Data System)
Lists
List of broadcast station classes
List of FM radio stations in Bangalore
List of Indian-language radio stations
Lists of radio stations in North America
History
History of radio
Oldest radio station
Bands
Band I
Band II
Band III
References
 "Transmission standards for FM sound broadcasting at VHF". ITU Rec. BS.450. International Telecommunications Union. pp. 4â5. Archived from the original on 2012-11-06. Retrieved 2011-01-08.
 "FM BROADCAST STATION SELF - INSPECTION CHECKLIST" (PDF). transition.fcc.gov. FCC. p. 18. Archived (PDF) from the original on 16 February 2017. Retrieved 8 June 2018.
 "Ofcom Site Engineering Code for Analogue Radio Broadcast Transmission Systems" (PDF). Archived (PDF) from the original on July 22, 2019. Retrieved April 8, 2020.
 "Planning standards for terrestrial FM sound broadcasting at VHF" (PDF). International Telecommunication Union. Archived (PDF) from the original on 2018-08-20. Retrieved 2019-08-02.
 "Digital Audio BroadcastingSupplementary Analog SCA Compatibility TestsTest Plan and Procedures" (PDF). ecfsapi.fcc.gov. Advanced Television Technology Center. Archived (PDF) from the original on 18 November 2019. Retrieved 30 October 2019.
 admin (2013-10-10). "The 75 Microsecond Pre-Emphasis Curve". Cornelius' home on the web!. Archived from the original on 2019-10-30. Retrieved 2019-10-30.
 "Report 1946-04 â Frequency Modulation". BBC Research & Development. Archived from the original on 2020-01-03. Retrieved 2020-01-03.
 RWO. "How FM Stereo Came to Life". Archived from the original on 2016-10-18. Retrieved 2016-03-06.
 ThÃ©berge, Paul; Devine, Kyle; Everrett, Tom (2015-01-29). Living Stereo: Histories and Cultures of Multichannel Sound. Bloomsbury Publishing USA. p. 189. ISBN 9781623566876.
 van Duyne, John P. (Fall 1961). "The Notebook:A Modulator for the New FM Stereo System". Boontown Radio Corp. CiteSeerX 10.1.1.309.3861.
 "73-319" (PDF). www.govinfo.gov. FCC. Archived (PDF) from the original on 22 January 2019. Retrieved 22 January 2019.
 "Stereophonic Broadcasting: Technical Details of Pilot-tone System", Information Sheet 1604(4), BBC Engineering Information Service, June 1970
 "Subsidiary communications multiplex operation: engineering standards" (PDF). www.fcc.gov. US Federal Communications Commission. Archived (PDF) from the original on 3 February 2017. Retrieved 12 January 2017.
 "FM Stereo demodulation circuit". USPTO. Archived from the original on 24 June 2017. Retrieved 6 December 2015.
 "FM Reception Guide: FM Propagation". WGBH. Archived from the original on 8 July 2007. Retrieved 9 May 2010.
Includes tips for multipath & fringe problems.
 "SSBSC: A Win-Win for FM Radio?". Radio World. April 2, 2012. Archived from the original on September 13, 2018. Retrieved May 30, 2018.
 "Archived copy" (PDF). Archived (PDF) from the original on 2018-09-13. Retrieved 2018-05-30.
 "FCC CFR 47 Part 73.322" (PDF). FCC Rules. Retrieved 23 May 2020.
 "Compatible four channel FM system". pdfpiw.uspto.gov. USPTO. Archived from the original on 2 February 2017. Retrieved 19 October 2016.
 Ann Arbor News, Ann Arbor, Michigan, January 3, 1973
 Mielke, E.-JÃ¼rgen (1977). "EinfluÃ des Dolby-B-Verfahrens auf die ÃbertragungsqualitÃ¤t im UKW-HÃ¶rrundfunk". Rundfunktechnische Mitteilungen (in German). Institut fÃ¼r Rundfunktechnik (IRT). 21: 222â228.
 "PrÃ¼fung eines modifizierten HIGHâCOM-Kompanders fÃ¼r den Einsatz bei der RF-Ãbertragung im UKW-HÃ¶rfunk" (in German). Institut fÃ¼r Rundfunktechnik (IRT). 1981-12-30. Technical Report 55/81.
 "Full text of "Radio Electronics (August 1987)"". archive.org. Archived from the original on 2016-04-02. Retrieved 2015-04-10.
 "CRIS Radio". Archived from the original on November 29, 2019. Retrieved April 8, 2020.
 comments, Crutchfield's Matt Freeman 9. "Live traffic information services for GPS systems". Crutchfield.
 ""Stereophonic FM Broadcast Begun by WEFM", Chicago Tribune, June 2, 1961, p. B-10". Archived from the original on February 3, 2004. Retrieved Apr 8, 2020.
 no byline (January 1, 1954). "MUSIC FOR BUSES URGED / F. C. C. Proposes Wide Field Also for FM News Reports". The New York Times. Retrieved February 19, 2019.
 "Radio 1 History - Transmitters". Radio Rewind. Archived from the original on 27 June 2013. Retrieved 11 August 2013.
 "[IT] Radio FM in Italia". Archived from the original on 5 March 2016. Retrieved 22 September 2015.
 "OIRT Tuner". Archived from the original on 2016-06-04. Retrieved 2016-10-25.
 "ITU Publications". ITU. Archived from the original on 2007-06-26. Retrieved 2007-03-26.
 "Change to the law to allow the use of low power FM transmitters for MP3 players". Ofcom. 23 November 2006. Archived from the original on 12 September 2015. Retrieved 8 August 2015.
 Clem, Richard P. (2014). "Inexpensive Options for Assistive Listening Device Receivers". W0IS.com. Archived from the original on 27 September 2019. Retrieved 13 July 2019.
External links
Library resources about
FM broadcasting
Resources in your library
Resources in other libraries
Related technical content
U.S. Patent 1,941,066
U.S. Patent 3,708,623 Compatible Four Channel FM System
Introduction to FM MPX
Frequency Modulation (FM) Tutorial
Stereo Multiplexing for Dummies Graphs that show waveforms at different points in the FM Multiplex process
Factbook list of stations worldwide
Invention History â The Father of FM
Audio Engineering Society
FM Broadcast and TV Broadcast Aural Subcarriers - Clifton Laboratories
vte
Analog and digital audio broadcasting
vte
Telecommunications
Categories: Radio communicationsBroadcast engineering
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Ð ÑÑÑÐºÐ¸Ð¹
Ø§Ø±Ø¯Ù
Tiáº¿ng Viá»t
ä¸­æ
21 more
Edit links
This page was last edited on 25 October 2020, at 14:36 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

High fidelity
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For other uses, see High fidelity (disambiguation).

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "High fidelity" â news Â· newspapers Â· books Â· scholar Â· JSTOR (January 2016) (Learn how and when to remove this template message)

Hi-fi speakers are a key component of quality audio reproduction.
High fidelity (often shortened to hi-fi or hifi) is a term used by listeners, audiophiles and home audio enthusiasts to refer to high-quality reproduction of sound.[1] This is in contrast to the lower quality sound produced by inexpensive audio equipment, AM radio, or the inferior quality of sound reproduction that can be heard in recordings made until the late 1940s.

Ideally, high-fidelity equipment has inaudible noise and distortion, and a flat (neutral, uncolored) frequency response within the human hearing range.[2]


Contents
1	History
2	Listening tests
3	Semblance of realism
4	Modularity
5	Modern equipment
6	See also
7	References
8	Further reading
9	External links
History
Bell Laboratories began experimenting with a range of recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the Bell labs in New Jersey. Some multitrack recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and Twentieth Century Fox Film Corporation (as early as 1941). RCA Victor began recording performances by several orchestras using optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs. During the 1930s, Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestraâthat would achieve high fidelity to the original sound. After World War II, Harry F. Olson conducted an experiment whereby test subjects listened to a live orchestra through a hidden variable acoustic filter. The results proved that listeners preferred high-fidelity reproduction, once the noise and distortion introduced by early sound equipment was removed.[citation needed]

Beginning in 1948, several innovations created the conditions that made major improvements of home-audio quality possible:

Reel-to-reel audio tape recording, based on technology taken from Germany after WWII, helped musical artists such as Bing Crosby make and distribute recordings with better fidelity.
The advent of the 33â rpm Long Play (LP) microgroove vinyl record, with lower surface noise and quantitatively specified equalization curves as well as noise-reduction and dynamic range systems. Classical music fans, who were opinion leaders in the audio market, quickly adopted LPs because, unlike with older records, most classical works would fit on a single LP.
FM radio, with wider audio bandwidth and less susceptibility to signal interference and fading than AM radio.
Better amplifier designs, with more attention to frequency response and much higher power output capability, reproducing audio without perceptible distortion.[3]
New loudspeaker designs, including acoustic suspension, developed by Edgar Villchur and Henry Kloss with improved bass frequency response.
In the 1950s, audio manufacturers employed the phrase high fidelity as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted high fidelity as fancy and expensive equipment, many found the difference in quality compared to the then-standard AM radios and 78-rpm records readily apparent and bought high-fidelity phonographs and 33â LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system). Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts even assembled their own loudspeaker systems. In the 1950s, hi-fi became a generic term for home sound equipment, to some extent displacing phonograph and record player.

In the late 1950s and early 1960s, the development of stereophonic equipment and recodings led to the next wave of home-audio improvement, and in common parlance stereo displaced hi-fi. Records were now played on a stereo. In the world of the audiophile, however, the concept of high fidelity continued to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is regarded as the "Golden Age of Hi-Fi", when vacuum tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state (transistorized) equipment was introduced to the market, subsequently replacing tube equipment as the mainstream technology.

The metal-oxide-semiconductor field-effect transistor (MOSFET) was adapted into a power MOSFET for audio by Jun-ichi Nishizawa at Tohoku University in 1974. Power MOSFETs were soon manufactured by Yamaha for their hi-fi audio amplifiers. JVC, Pioneer Corporation, Sony and Toshiba also began manufacturing amplifiers with power MOSFETs in 1974.[4] In 1977, Hitachi introduced the LDMOS (lateral diffused MOS), a type of power MOSFET. Hitachi was the only LDMOS manufacturer between 1977 and 1983, during which time LDMOS was used in audio power amplifiers from manufacturers such as HH Electronics (V-series) and Ashly Audio, and were used for music and public address systems.[4] Class-D amplifiers became successful in the mid-1980s when low-cost, fast-switching MOSFETs were made available.[5] Many transistor amps use MOSFET devices in their power sections, because their distortion curve is more tube-like.[6]

A popular type of system for reproducing music beginning in the 1970s was the integrated music centreâwhich combined a phonograph turntable, AM-FM radio tuner, tape player, preamplifier, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components or be familiar with impedance and power ratings. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction.

Audiophiles in the 1970s and 1980s preferred to buy each component separately. That way, they could choose models of each component with the specifications that they desired. In the 1980s, a number of audiophile magazines became available, offering reviews of components and articles on how to choose and test speakers, amplifiers and other components.

Listening tests
See also: Codec listening test
Listening tests are used by hi-fi manufacturers, audiophile magazines and audio engineering researchers and scientists. If a listening test is done in such a way that the listener who is assessing the sound quality of a component or recording can see the components that are being used for the test (e.g., the same musical piece listened to through a tube power amplifier and a solid-state amplifier), then it is possible that the listener's pre-existing biases towards or against certain components or brands could affect their judgment. To respond to this issue, researchers began to use blind tests, in which listeners cannot see the components being tested. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample A, the reference, and sample B, an alternative), and one unknown sample X, for three samples total. X is randomly selected from A and B, and the subject identifies X as being either A or B. Although there is no way to prove that a certain methodology is transparent,[7] a properly conducted double-blind test can prove that a method is not transparent.

Blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these blind tests is not accepted by some audiophile magazines such as Stereophile and The Absolute Sound in their evaluations of audio equipment. John Atkinson, current editor of Stereophile, stated that he once purchased a solid-state amplifier, the Quad 405, in 1978 after seeing the results from blind tests, but came to realize months later that "the magic was gone" until he replaced it with a tube amp.[8] Robert Harley of The Absolute Sound wrote, in 2008, that: "...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon."[9]

Doug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009.[10][11] He stated: "Blind tests are at the core of the decades' worth of research into loudspeaker design done at Canada's National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so." Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Audio professional Dr. Sean Olive of Harman International shares this view.[12]

Semblance of realism
Stereophonic sound provided a partial solution to the problem of creating the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became popular, and many consumers were willing to tolerate the six to eight channels required in a home theater.

In addition to spatial realism, the playback of music must be subjectively free from noise, such as hiss or hum, to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range,[13] which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall.[14] Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz. [15] Most adults can't hear higher than 15 kHz.[13] CDs are capable of reproducing frequencies as low as 0 Hz and as high as 22.05 kHz, making them adequate for reproducing the frequency range that most humans can hear.[13] The equipment must also provide no noticeable distortion of the signal or emphasis or de-emphasis of any frequency in this frequency range.

Files such as mp3s include lossy compression, and therefore result in reduced sound quality.

Modularity

Modular components made by Samsung and Harman Kardon
Integrated, mini, or lifestyle systems (also known by the older terms music centre or midi system[16][17]) contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from separates (or components), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.

For slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an integrated amplifier; with a tuner, it is a receiver. A monophonic power amplifier, which is called a monoblock, is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.

This modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system. Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Disadvantages include the complexity of having multiple components with cabling and connectivity, and different remote controls for each unit.

Modern equipment
In the 2000s, modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support. Another modern component is the music server consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system. There is now a push from certain streaming services to offer hi-fi services. Streaming services typically have a modified dynamic range and possibly bit rates lower than audiophiles would be happy with. Tidal (service) has launched a hi-fi tier which includes access to FLAC and Master Quality Authenticated studio masters for many tracks through the desktop version of the player. This integration is also available for high-end audio systems.

See also
Audio system measurements
Comparison of analog and digital recording
DIY audio
Edwin Howard Armstrong
Entertainment center
FM broadcasting
High-end audio
Lo-fi music
Radio Data System (RDS)
Wife acceptance factor
WiFi a wireless term derived from HiFi
References
 Hartley, H. A. (1958). "High fidelity". Audio Design Handbook (PDF). New York, New York: Gernsback Library. p. 7, 200. Library of Congress Catalog Card No. 57-9007. Archived from the original (PDF) on 2009-01-27. Retrieved 2009-08-08. I invented the phrase 'high fidelity' in 1927 to denote a type of sound reproduction that might be taken rather seriously by a music lover. In those days the average radio or phonograph equipment sounded pretty horrible but, as I was really interested in music, it occurred to me that something might be done about it.
 "Frequency response". www.hi-fiworld.co.uk.
 David Lander (JuneâJuly 2006). "The Buyable Past: Classic Hi-Fi Components". American Heritage. Archived from the original on 2007-02-23.
 Duncan, Ben (1996). High Performance Audio Power Amplifiers. Elsevier. pp. 177-8, 406. ISBN 9780080508047.
 Duncan, Ben (1996). High Performance Audio Power Amplifiers. Newnes. pp. 147â148. ISBN 9780750626293.
 Fliegler, Ritchie; Eiche, Jon F. (1993). Amps! The Other Half of Rock 'n' Roll. Hal Leonard Corporation. ISBN 9780793524112.
 Spanos, Aris (1999). Probability Theory and Statistical Inference. Cambridge University Press. p. 699. ISBN 0-521-42408-9.
 John Atkinson (2005-07-17). "Blind Tests & Bus Stops".
 Robert Harley (2008-05-28). "Blind Listening Tests are Flawed: An Editorial". The Absolute Sound. Archived from the original on 2011-09-30. Retrieved 2011-09-29.
 Doug Schneider (2009-05-01). "The Misinformed Misleading the Uninformed â A Bit About Blind Listening Tests". GoodSound!. Retrieved 2011-09-29.
 Doug Schneider (2009-06-01). "A Bit More About Blind Listening Tests (6/2009)". GoodSound!. Retrieved 2011-09-29.
 Dr. Sean Olive (2009-04-09). "The Dishonesty of Sighted Listening Tests". Retrieved 2011-09-29.[self-published source?]
 Fries, Bruce; Marty Fries (2005). Digital Audio Essentials. O'Reilly Media. pp. 144â147. ISBN 0-596-00856-2. Digital audio at 16-bit resolution has a theoretical dynamic range of 96 dB, but the actual dynamic range is usually lower because of overhead from filters that are built into most audio systems." ... "Audio CDs achieve about a 90-dB signal-to-noise ratio." "Most adults can't hear frequencies higher than 15 kHz, so the 44.1 kHz sampling rate of CD audio is more than adequate to reproduce the highest frequencies most people can hear.
 Eargle, John (2005). Handbook of Recording Engineering. Springer. p. 4. ISBN 0-387-28470-2.
 D'Ambrose, Chris (2003). "Frequency Range of Human Hearing". The Physics Factbook. Retrieved October 11, 2009.
 Argos Catalogue Autumn/Winter 1986. Argos. 1986. pp. 258â259. Archived from the original on 2020-05-27. "Midi Systems [..] Scheider 2500R Remote Control Midi System [..] Amstrad MS-45 Midi System [..] Toshiba S103K Midi System [etc] Alt URL
 "Matsui MIDI 47".
Further reading
Pier Paolo Ferrari (2016). The Hi-Fi's Golden Age (1st ed.). Bergamo, Italy: Sandit. ISBN 978-88-6928-171-6.
Janet Borgerson; Jonathan Schroeder (2017). Designed for Hi-Fi Living: The Vinyl LP in Midcentury America. Cambridge, MA: MIT Press. ISBN 9780262036238.
Pier Paolo Ferrari (2017). High-Fidelity (1st ed.). Bergamo, Italy: Sandit Libri. ISBN 978-88-6928-254-6.
External links
HiFi at Curlie
A Dictionary of Home Entertainment Terms
vte
High-definition (HD)
vte
Audio players
Categories: SoundConsumer electronicsSound recordingAudio engineering
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
PortuguÃªs
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
25 more
Edit links
This page was last edited on 20 October 2020, at 12:04 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Audiophile
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
An audiophile is a person who is enthusiastic about high-fidelity sound reproduction.[1] An audiophile seeks to reproduce the sound of a live musical performance, typically in a room with good acoustics. It is widely agreed that reaching this goal is very difficult and that even the best-regarded recording and playback systems rarely, if ever, achieve it.[2][3]

Audiophile values may be applied at all stages of music reproduction: the initial audio recording, the production process, and the playback, which is usually in a home setting. In general, the values of an audiophile are seen to be antithetical to the growing popularity of more convenient but lower quality music, especially lossy digital file types like MP3, lower definition streaming services, and inexpensive headphones.[4]

The term high-end audio refers to playback equipment used by audiophiles, which may be bought at specialist shops and websites.[5] High-end components include turntables, digital-to-analog converters, equalization devices, preamplifiers and amplifiers (both solid-state and vacuum tube), horn and electrostatic speakers, power conditioners, subwoofers, headphones, and acoustic room treatment in addition to room correction devices.[6][7]


Contents
1	Audio playback components
1.1	Sound sources
1.2	Amplifiers
1.3	Loudspeakers
1.4	Accessories
1.5	Headphones
1.6	Design variety
2	Controversies
3	See also
4	References
5	External links
Audio playback components
An audio system typically consists of one or more source components, one or more amplification components, and (for stereo) two or more loudspeakers.[8]

Signal cables (analog audio, speaker, digital audio etc.) are used to link these components. There are also a variety of accessories, including equipment racks, power conditioners, devices to reduce or control vibration, record cleaners, anti-static devices, phonograph needle cleaners, reverberation reducing devices such as speaker pads and stands, sound absorbent foam, and soundproofing.

The interaction between the loudspeakers and the room (room acoustics) plays an important part in sound quality. Sound vibrations are reflected from walls, floor and ceiling, and are affected by the contents of the room. Room dimensions can create standing waves at particular (usually low) frequencies. There are devices and materials for room treatment that affect sound quality. Soft materials, such as draperies and carpets, can absorb higher frequencies, whereas hard walls and floors can cause excess reverberation.


Modern turntable.

Top-loading CD player and external D-to-A converter.

Quad II, an early monoblock valve (vacuum tube) amplifier.
Sound sources
Audiophiles play music from a variety of sources including phonograph records, compact discs (CDs), and digital audio files that are either uncompressed or are losslessly compressed, such as FLAC, DSD, Windows Media Audio 9 Lossless and Apple Lossless (ALAC), in contrast to lossy compression, such as in MP3 encoding. From the early 1990s, CDs were the most common source of high-quality music. Nevertheless, turntables, tonearms, and magnetic cartridges are still used, despite the difficulties of keeping records free from dust and the delicate set-up associated with turntables.

The 44.1 kHz sampling rate of the CD format, in theory, restricts CD information losses to above the theoretical upper-frequency limit of human hearing â 20 kHz, see Nyquist limit. Nonetheless, newer formats such as FLAC, ALAC, DVD-Audio and Super Audio Compact Disc (SACD) have sampling rates of 88.2 kHz, 96 kHz or even 192 kHz.

CD audio signals are encoded in 16-bit values. Some higher-definition consumer formats such as HDCD-encoded CDs, DVD-Audio, and SA-CD contain 20-bit, 24-bit and even 32-bit audio streams. With more bits more dynamic range is possible; 20 bit dynamic range is theoretically 120 dBâthe limit of most consumer electronic playback equipment.[9]

SACDs and DVD-Audio have up to 5.1 to 6.1 surround sound. Although both high-res optical formats have failed, there has been a resurgence in high-res digital files. SACD can be stored as a DSD file, and DVD-Audio can be stored as a FLAC or ALAC file. FLAC is the most widely used digital format for high-res with up to 8 channels and a maximum depth of 32 bit, and 65,535 Hz sampling rate. Uncompressed formats such as WAV and AIFF files can store audio CDs with no compression.

Amplifiers
A preamplifier selects among several audio inputs, amplifies source-level signals (such as those from a turntable), and allows the listener to adjust the sound with volume and tone controls. Many audiophile-oriented preamplifiers lack tone controls. A power amplifier takes the "line-level" audio signal from the preamplifier and drives the loudspeakers. An integrated amplifier combines the functions of power amplification with input switching and volume and tone control. Both pre/power combinations and integrated amplifiers are widely used by audiophiles.

Audiophile amplifiers are available based on solid-state (semiconductor) technology, vacuum-tube (valve) technology, or hybrid technologyâsemiconductors and vacuum tubes.

Dedicated amplifiers are also commonly used by audiophiles to drive headphones, especially those with high impedance and/or low sensitivity, or electrostatic headphones.

Loudspeakers
The cabinet of the loudspeaker is known as the enclosure. There are a variety of loudspeaker enclosure designs, including sealed cabinets (acoustic suspension), ported cabinets (bass-reflex), transmission line, infinite baffle, and horn loaded. The enclosure plays a major role in the sound of the loudspeaker.

The drivers that produce the sound are referred to as tweeters, midranges, and woofers. Driver designs include dynamic, electrostatic, plasma, ribbon, planar, ionic, and servo-actuated. Drivers are made from a variety of materials including paper pulp, polypropylene, kevlar, aluminum, magnesium, beryllium, and vapor-deposited diamond.

The direction and intensity of the output of a loudspeaker, called dispersion or polar response, has a large effect on its sound.[10] Various methods are employed to control the dispersion. These methods include monopolar, bipolar, dipolar, 360-degree, horn, waveguide, and line source. These terms refer to the configuration and arrangement of the various drivers in the enclosure.

The positioning of loudspeakers in the room has a strong influence on the sound experience.[11][12] Loudspeaker output is influenced by interaction with room boundaries, particularly bass response, and high frequency transducers are directional, or "beaming".

Accessories
Audiophiles use a wide variety of accessories and fine-tuning techniques, sometimes referred to as "tweaks", to improve the sound of their systems. These include power conditioner filters to "clean" the electricity,[13] equipment racks to isolate components from floor vibrations, specialty power and audio cables, loudspeaker stands (and footers to isolate speakers from stands), and room treatments.

There are several types of room treatment. Sound-absorbing materials may be placed strategically within a listening room to reduce the amplitude of early reflections, and to deal with resonance modes. Other treatments are designed to produce diffusion, reflection of sound in a scattered fashion. Room treatments can be expensive and difficult to optimize.

Headphones
Headphones are regularly used by audiophiles. These products can be remarkably expensive, some over $10,000,[14] but in general are much cheaper than comparable speaker systems. They have the advantage of not requiring room treatment, and being usable without requiring others to listen at the same time. Newer canalphones can be driven by the less powerful outputs found on portable music players.

Design variety
For music storage, digital formats offer an absence of clicks, pops, wow, flutter, acoustic feedback, and rumble, compared to vinyl records. Depending on the format, digital can also have a higher signal-to-noise ratio, a wider dynamic range, less total harmonic distortion, and a flatter and more extended frequency response.[15][16] Despite this, vinyl records remain popular, and discussion about the relative merits of analog and digital sound continues (see Analog sound vs. digital sound). (Note that vinyl records may be mastered differently from their digital versions.)

In the amplification stage, vacuum-tube electronics remain popular, despite most other applications having since abandoned tubes for solid state amplifiers. Also vacuum-tube amplifiers often have higher total harmonic distortion, require rebiasing, are less reliable, generate more heat, are less powerful, and cost more.[17] There is also continuing debate about the proper use of negative feedback in amplifier design.[18][19]

Controversies
There is substantial controversy on the subject of audiophile components; many have asserted that the occasionally high cost produces no measurable improvement in audio reproduction.[20] For example, skeptic James Randi, through his foundation One Million Dollar Paranormal Challenge, has offered a prize of $1 million to anyone who can demonstrate that $7,250 audio cables "are any better than ordinary audio cables".[21] In 2008, audio reviewer Michael Fremer attempted to claim the prize, and said that Randi declined the challenge.[22] Randi said that the cable manufacturer Pear Cables was the one who withdrew.[23]

Criticisms usually focus on claims around so-called "tweaks" and accessories beyond the core source, amplification, and speaker products. Examples of these accessories include speaker cables, component interconnects, stones, cones, CD markers, and power cables or conditioners.[24][25]

There is disagreement on how equipment testing should be conducted and as to its utility. Audiophile publications frequently describe differences in quality which are not detected by standard audio system measurements and double blind testing, claiming that they perceive differences in audio quality which cannot be measured by current instrumentation,[26] and cannot be detected by listeners if listening conditions are controlled,[27] but without providing an explanation for those claims.

See also
Broadcast quality
Professional audio
Videophile
Audiophile publications
The Absolute Sound
Stereophile
What Hi-Fi? Sound and Vision
References
 "Audiophile". Dictionary.reference.com. 2011. Retrieved 6 December 2011.
 Lichte, Erick (2 July 2012). "Audio Research Reference 150 power amplifier". Stereophile.
 Doris, Frank (1993). "Hi Fi in the Arena: The Concert Sound of the Grateful Dead". The Absolute Sound.
 Kurutz, Steven (24 July 2013). "The new audio geeks". The New York Times.
 Perlman, M. (2004). "Golden ears and meter readers: The contest for epistemic authority in Audiophilia". Social Studies of Science. 34 (5): 783. doi:10.1177/0306312704047613.
 van der Veen, M. (2005). "Universal system and output transformer for valve amplifiers" (PDF). 118th AES Convention, Barcelona, Spain.
 O'Connell, Joseph (January 1992). "The Fine-Tuning of a Golden Ear: High-End Audio and the Evolutionary Model of Technology". Technology and Culture. Society for the History of Technology. 33 (1): 1â37. doi:10.2307/3105807. ISSN 0040-165X. JSTOR 3105807.
 "Sound Systems". Media college.com. Retrieved 3 July 2012.
 Huber, David Miles; Robert E. Runstein (2005). Modern Recording Techniques, Sixth Edition. Focal Press. p. 130. ISBN 0-240-80625-5.
 "The 'Best' Loudspeaker: Revisiting Dispersion Issues". EnjoyTheMusic.com. May 1999. Retrieved 3 July 2012.
 Hoffman, David (8 October 2000). "Loudspeaker Room Placement -- Part 1". onhifi.com. Archived from the original on 21 December 2016. Retrieved 3 July 2012.
 Atkinson, John (3 October 2008). "Getting the Best from Your Loudspeakers". Stereophile.
 "Power Conditioner Reviews". Audioholics.
 Woollard, Deidre (14 December 2005). "Sennheiser HE 90 Headphones". Luxist. Archived from the original on 28 September 2007. Retrieved 30 June 2007.
 Ian G. Masters (1 January 2003). "The Decline of Vinyl and Its Timely Death". mastersonaudio.com. Archived from the original on 6 September 2005.
 Ian G. Masters (15 April 2005). "Vinyl Hooey". mastersonaudio.com. Archived from the original on 20 March 2006.
 Ian G. Masters (1 September 2002). "The Ongoing Debate about Amplifier "Sound"". mastersonaudio.com. Archived from the original on 30 August 2007.
 Martin Colloms (January 1998). "A Future Without Feedback?" (PDF). Stereophile. Archived from the original (PDF) on 19 June 2013. Retrieved 9 May 2007.
 Bruno Putzeys (February 2011). "The 'F' word, or why there is no such thing as too much feedback" (PDF). Linear Audio. Archived from the original (PDF) on 11 July 2013. Retrieved 19 March 2013.
 "Lost in music: the world of obsessive audiophilia". The Guardian.
 "James Randi Offers $1 Million If Audiophiles Can Prove $7250 Speaker Cables Are Better". Gizmodo. 2005. Archived from the original on 2 February 2009. Retrieved 6 January 2009.
 "The Swift Boating of Audiophiles". Stereophile. 2008. Retrieved 4 January 2011.
 "Blake Withdraws from PEAR Cable Challenge". JREF. 2007. Retrieved 8 January 2011.
 Kinch, Bruce (2000). "Cheap Tweaks That Sound Like a Million Bucks!!!". Enjoy The Music. Archived from the original on 30 July 2012. Retrieved 7 November 2007.
 Russell, Roger (1999â2007). "Speaker Wire - A History". Roger Russell. Archived from the original on 5 November 2007. Retrieved 7 November 2007.
 "God is in the Nuances". Retrieved 25 March 2007.
 "The Highs and Lows of Double Blind Testing". Retrieved 23 May 2013.
External links
Audiophilia: The Online Journal for the Serious Audiophile
Why We Need Audiophiles (Gizmodo)
High end Audio and Audiophile Pages
Enjoy the Music.com: Equipment reviews, industry news, shows reports, etc.
Portuguese High end Audio benchmark reviews and reports website from JosÃ© Victor Henriques.
Audio Societies
Audiophile Society of New South Wales
Bay Area Audiophile Society
Boston Audio Society
Chicago Audio Society
Colorado Audio Society
Los Angeles and Orange County Audio Society
Pacific Northwest Audio Society
Categories: HobbiesAudio engineering
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
EspaÃ±ol
FranÃ§ais
Italiano
Nederlands
æ¥æ¬èª
PortuguÃªs
Ð ÑÑÑÐºÐ¸Ð¹
TÃ¼rkÃ§e
Tiáº¿ng Viá»t
8 more
Edit links
This page was last edited on 7 September 2020, at 11:36 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Vacuum tube
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
This article is about the electronic device. For experiments in an evacuated pipe, see Free fall. For the transport system, see Pneumatic tube. For Blood sampling, see Venipuncture.

Later thermionic vacuum tubes, mostly miniature style, some with top cap connections for higher voltages
A vacuum tube, an electron tube,[1][2][3] valve (British usage) or tube (North America),[4] is a device that controls electric current flow in a high vacuum between electrodes to which an electric potential difference has been applied.

The type known as a thermionic tube or thermionic valve uses the phenomenon of thermionic emission of electrons from a hot cathode and is used for a number of fundamental electronic functions such as signal amplification and current rectification. Non-thermionic types, such as a vacuum phototube however, achieve electron emission through the photoelectric effect, and are used for such purposes as the detection of light intensities. In both types, the electrons are accelerated from the cathode to the anode by the electric field in the tube.

The simplest vacuum tube, the diode, invented in 1904 by John Ambrose Fleming, contains only a heated electron-emitting cathode and an anode. Electrons can only flow in one direction through the deviceâfrom the cathode to the anode. Adding one or more control grids within the tube allows the current between the cathode and anode to be controlled by the voltage on the grids.[5]

These devices became a key component of electronic circuits for the first half of the twentieth century. They were crucial to the development of radio, television, radar, sound recording and reproduction, long-distance telephone networks, and analog and early digital computers. Although some applications had used earlier technologies such as the spark gap transmitter for radio or mechanical computers for computing, it was the invention of the thermionic vacuum tube that made these technologies widespread and practical, and created the discipline of electronics.[6]

In the 1940s, the invention of semiconductor devices made it possible to produce solid-state devices, which are smaller, more efficient, reliable, durable, safer, and more economical than thermionic tubes. Beginning in the mid-1960s, thermionic tubes were being replaced by the transistor. However, the cathode-ray tube (CRT) remained the basis for television monitors and oscilloscopes until the early 21st century. Thermionic tubes are still used in some applications, such as the magnetron used in microwave ovens, certain high-frequency amplifiers, and amplifiers that audio enthusiasts prefer for their "warmer" tube sound.

Not all electronic circuit valves/electron tubes are vacuum tubes. Gas-filled tubes are similar devices, but containing a gas, typically at low pressure, which exploit phenomena related to electric discharge in gases, usually without a heater.


Contents
1	Classifications
2	Description
3	History and development
3.1	Diodes
3.2	Triodes
3.3	Tetrodes and pentodes
3.4	Multifunction and multisection tubes
3.5	Beam power tubes
3.6	Gas-filled tubes
3.7	Miniature tubes
3.7.1	Sub-miniature tubes
3.8	Improvements in construction and performance
3.9	Indirectly heated cathodes
3.10	Use in electronic computers
3.10.1	Colossus
3.10.2	Whirlwind and "special-quality" tubes
4	Heat generation and cooling
5	Tube packages
6	Names
7	Special-purpose tubes
8	Powering the tube
8.1	Batteries
8.2	AC power
9	Reliability
9.1	Vacuum
9.2	Transmitting tubes
9.3	Receiving tubes
9.4	Failure modes
9.4.1	Catastrophic failures
9.4.2	Degenerative failures
9.4.3	Other failures
10	Testing
11	Other vacuum tube devices
11.1	Cathode ray tubes
11.2	Electron multipliers
12	Vacuum tubes in the 21st century
12.1	Niche applications
12.1.1	Audiophiles
12.2	Displays
12.2.1	Cathode ray tube
12.2.2	Vacuum fluorescent display
12.3	Vacuum tubes using field electron emitters
13	Characteristics
13.1	Space charge of a vacuum tube
13.2	V-I characteristic of vacuum tube
13.3	Size of electrostatic field
14	Patents
15	See also
16	References
17	Notes
18	Further reading
19	External links
Classifications

Audio vacuum tubes in radio
One classification of thermionic vacuum tubes is by the number of active electrodes. A device with two active elements is a diode, usually used for rectification. Devices with three elements are triodes used for amplification and switching. Additional electrodes create tetrodes, pentodes, and so forth, which have multiple additional functions made possible by the additional controllable electrodes.


Tube amplifier
Other classifications are:

by frequency range (audio, radio, VHF, UHF, microwave)
by power rating (small-signal, audio power, high-power radio transmitting)
by cathode/filament type (indirectly heated, directly heated) and Warm-up time (including "bright-emitter" or "dull-emitter")
by characteristic curves design (e.g., sharp- versus remote-cutoff in some pentodes)
by application (receiving tubes, transmitting tubes, amplifying or switching, rectification, mixing)
specialized parameters (long life, very low microphonic sensitivity and low-noise audio amplification, rugged/military versions)
specialized functions (light or radiation detectors, video imaging tubes)
tubes used to display information (Nixie tubes, "magic eye" tubes, vacuum fluorescent displays, CRTs)
Tubes have different functions, such as cathode ray tubes which create a beam of electrons for display purposes (such as the television picture tube) in addition to more specialized functions such as electron microscopy and electron beam lithography. X-ray tubes are also vacuum tubes. Phototubes and photomultipliers rely on electron flow through a vacuum, though in those cases electron emission from the cathode depends on energy from photons rather than thermionic emission. Since these sorts of "vacuum tubes" have functions other than electronic amplification and rectification they are described in their own articles.

Description

Diode: electrons from the hot cathode flow towards the positive anode, but not vice versa

Triode: voltage applied to the grid controls plate (anode) current.
A vacuum tube consists of two or more electrodes in a vacuum inside an airtight envelope. Most tubes have glass envelopes with a glass-to-metal seal based on kovar sealable borosilicate glasses, though ceramic and metal envelopes (atop insulating bases) have been used. The electrodes are attached to leads which pass through the envelope via an airtight seal. Most vacuum tubes have a limited lifetime, due to the filament or heater burning out or other failure modes, so they are made as replaceable units; the electrode leads connect to pins on the tube's base which plug into a tube socket. Tubes were a frequent cause of failure in electronic equipment, and consumers were expected to be able to replace tubes themselves. In addition to the base terminals, some tubes had an electrode terminating at a top cap. The principal reason for doing this was to avoid leakage resistance through the tube base, particularly for the high impedance grid input.[7]:580[8] The bases were commonly made with phenolic insulation which performs poorly as an insulator in humid conditions. Other reasons for using a top cap include improving stability by reducing grid-to-anode capacitance,[9] improved high-frequency performance, keeping a very high plate voltage away from lower voltages, and accommodating one more electrode than allowed by the base. There was even an occasional design that had two top cap connections.

The earliest vacuum tubes evolved from incandescent light bulbs, containing a filament sealed in an evacuated glass envelope. When hot, the filament releases electrons into the vacuum, a process called thermionic emission, originally known as the Edison effect. A second electrode, the anode or plate, will attract those electrons if it is at a more positive voltage. The result is a net flow of electrons from the filament to plate. However, electrons cannot flow in the reverse direction because the plate is not heated and does not emit electrons. The filament (cathode) has a dual function: it emits electrons when heated; and, together with the plate, it creates an electric field due to the potential difference between them. Such a tube with only two electrodes is termed a diode, and is used for rectification. Since current can only pass in one direction, such a diode (or rectifier) will convert alternating current (AC) to pulsating DC. Diodes can therefore be used in a DC power supply, as a demodulator of amplitude modulated (AM) radio signals and for similar functions.

Early tubes used the filament as the cathode; this is called a "directly heated" tube. Most modern tubes are "indirectly heated" by a "heater" element inside a metal tube that is the cathode. The heater is electrically isolated from the surrounding cathode and simply serves to heat the cathode sufficiently for thermionic emission of electrons. The electrical isolation allows all the tubes' heaters to be supplied from a common circuit (which can be AC without inducing hum) while allowing the cathodes in different tubes to operate at different voltages. H. J. Round invented the indirectly heated tube around 1913.[10]

The filaments require constant and often considerable power, even when amplifying signals at the microwatt level. Power is also dissipated when the electrons from the cathode slam into the anode (plate) and heat it; this can occur even in an idle amplifier due to quiescent currents necessary to ensure linearity and low distortion. In a power amplifier, this heating can be considerable and can destroy the tube if driven beyond its safe limits. Since the tube contains a vacuum, the anodes in most small and medium power tubes are cooled by radiation through the glass envelope. In some special high power applications, the anode forms part of the vacuum envelope to conduct heat to an external heat sink, usually cooled by a blower, or water-jacket.

Klystrons and magnetrons often operate their anodes (called collectors in klystrons) at ground potential to facilitate cooling, particularly with water, without high-voltage insulation. These tubes instead operate with high negative voltages on the filament and cathode.

Except for diodes, additional electrodes are positioned between the cathode and the plate (anode). These electrodes are referred to as grids as they are not solid electrodes but sparse elements through which electrons can pass on their way to the plate. The vacuum tube is then known as a triode, tetrode, pentode, etc., depending on the number of grids. A triode has three electrodes: the anode, cathode, and one grid, and so on. The first grid, known as the control grid, (and sometimes other grids) transforms the diode into a voltage-controlled device: the voltage applied to the control grid affects the current between the cathode and the plate. When held negative with respect to the cathode, the control grid creates an electric field that repels electrons emitted by the cathode, thus reducing or even stopping the current between cathode and anode. As long as the control grid is negative relative to the cathode, essentially no current flows into it, yet a change of several volts on the control grid is sufficient to make a large difference in the plate current, possibly changing the output by hundreds of volts (depending on the circuit). The solid-state device which operates most like the pentode tube is the junction field-effect transistor (JFET), although vacuum tubes typically operate at over a hundred volts, unlike most semiconductors in most applications.

History and development

One of Edison's experimental bulbs
The 19th century saw increasing research with evacuated tubes, such as the Geissler and Crookes tubes. The many scientists and inventors who experimented with such tubes include Thomas Edison, Eugen Goldstein, Nikola Tesla, and Johann Wilhelm Hittorf. With the exception of early light bulbs, such tubes were only used in scientific research or as novelties. The groundwork laid by these scientists and inventors, however, was critical to the development of subsequent vacuum tube technology.

Although thermionic emission was originally reported in 1873 by Frederick Guthrie,[11] it was Thomas Edison's apparently independent discovery of the phenomenon in 1883 that became well known. Although Edison was aware of the unidirectional property of current flow between the filament and the anode, his interest (and patent[12]) concentrated on the sensitivity of the anode current to the current through the filament (and thus filament temperature). Little practical use was ever made of this property (however early radios often implemented volume controls through varying the filament current of amplifying tubes). It was only years later that John Ambrose Fleming utilized the rectifying property of the diode tube to detect (demodulate) radio signals, a substantial improvement on the early cat's-whisker detector already used for rectification.

Amplification by vacuum tube became practical only with Lee De Forest's 1907 invention of the three-terminal "audion" tube, a crude form of what was to become the triode.[13] Being essentially the first electronic amplifier,[14] such tubes were instrumental in long-distance telephony (such as the first coast-to-coast telephone line in the US) and public address systems, and introduced a far superior and versatile technology for use in radio transmitters and receivers. The electronics revolution of the 20th century arguably began with the invention of the triode vacuum tube.

Diodes
Main article: Diode

Fleming's first diodes
The English physicist John Ambrose Fleming worked as an engineering consultant for firms including Edison Swan,[15] Edison Telephone and the Marconi Company. In 1904, as a result of experiments conducted on Edison effect bulbs imported from the United States, he developed a device he called an "oscillation valve" (because it passes current in only one direction). The heated filament, was capable of thermionic emission of electrons that would flow to the plate (or anode) when it was at a positive voltage with respect to the heated cathode. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons.

Later known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector. This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker, an adjustable point contact. Unlike modern semiconductors, such a diode required painstaking adjustment of the contact to the crystal in order for it to rectify.

The tube was relatively immune to vibration, and thus vastly superior on shipboard duty, particularly for navy ships with the shock of weapon fire commonly knocking the sensitive but delicate galena off its sensitive point (the tube was in general no more sensitive as a radio detector, but was adjustment free). The diode tube was a reliable alternative for detecting radio signals.

As electronic engineering advanced, notably during World War II, this function of a diode came to be considered as one type of demodulation. While firmly established by history, the term "detector" is not of itself descriptive and should be considered outdated.

Higher-power diode tubes or power rectifiers found their way into power supply applications until they were eventually replaced first by selenium, and later, by silicon rectifiers in the 1960s.

Triodes
Main article: Triode

The first triode, the De Forest Audion, invented in 1906

Triodes as they evolved over 40 years of tube manufacture, from the RE16 in 1918 to a 1960s era miniature tube

Triode symbol. From top to bottom: plate (anode), control grid, cathode, heater (filament)
Originally, the only use for tubes in radio circuits was for rectification, not amplification. In 1906, Robert von Lieben filed for a patent[16] for a cathode ray tube which included magnetic deflection. This could be used for amplifying audio signals and was intended for use in telephony equipment. He would later help refine the triode vacuum tube.

However, Lee De Forest is credited with inventing the triode tube in 1907 while experimenting to improve his original (diode) Audion. By placing an additional electrode between the filament (cathode) and plate (anode), he discovered the ability of the resulting device to amplify signals. As the voltage applied to the control grid (or simply "grid") was lowered from the cathode's voltage to somewhat more negative voltages, the amount of current from the filament to the plate would be reduced.

The negative electrostatic field created by the grid in the vicinity of the cathode would inhibit the passage of emitted electrons and reduce the current to the plate. Thus, a few volt difference at the grid would make a large change in the plate current and could lead to a much larger voltage change at the plate; the result was voltage and power amplification. In 1908, De Forest was granted a patent (U.S. Patent 879,532 ) for such a three-electrode version of his original Audion for use as an electronic amplifier in radio communications. This eventually became known as the triode.


General Electric Company Pliotron, Science History Institute
De Forest's original device was made with conventional vacuum technology. The vacuum was not a "hard vacuum" but rather left a very small amount of residual gas. The physics behind the device's operation was also not settled. The residual gas would cause a blue glow (visible ionization) when the plate voltage was high (above about 60 volts). In 1912, De Forest brought the Audion to Harold Arnold in AT&T's engineering department. Arnold recommended that AT&T purchase the patent, and AT&T followed his recommendation. Arnold developed high-vacuum tubes which were tested in the summer of 1913 on AT&T's long-distance network.[17] The high-vacuum tubes could operate at high plate voltages without a blue glow.

Finnish inventor Eric Tigerstedt significantly improved on the original triode design in 1914, while working on his sound-on-film process in Berlin, Germany. Tigerstedt's innovation was to make the electrodes concentric cylinders with the cathode at the centre, thus greatly increasing the collection of emitted electrons at the anode.[18]

Irving Langmuir at the General Electric research laboratory (Schenectady, New York) had improved Wolfgang Gaede's high-vacuum diffusion pump and used it to settle the question of thermionic emission and conduction in a vacuum. Consequently, General Electric started producing hard vacuum triodes (which were branded Pliotrons) in 1915.[19] Langmuir patented the hard vacuum triode, but De Forest and AT&T successfully asserted priority and invalidated the patent.

Pliotrons were closely followed by the French type 'TM' and later the English type 'R' which were in widespread use by the allied military by 1916. Historically, vacuum levels in production vacuum tubes typically ranged from 10 ÂµPa down to 10 nPa.[20]

The triode and its derivatives (tetrodes and pentodes) are transconductance devices, in which the controlling signal applied to the grid is a voltage, and the resulting amplified signal appearing at the anode is a current. Compare this to the behavior of the bipolar junction transistor, in which the controlling signal is a current and the output is also a current.

For vacuum tubes, transconductance or mutual conductance (gm) is defined as the change in the plate(anode)/cathode current divided by the corresponding change in the grid to cathode voltage, with a constant plate(anode) to cathode voltage. Typical values of gm for a small-signal vacuum tube are 1 to 10 millisiemens. It is one of the three 'constants' of a vacuum tube, the other two being its gain Î¼ and plate resistance Rp or Ra. The Van der Bijl equation defines their relationship as follows: {\displaystyle g_{m}={\mu \over R_{p}}}{\displaystyle g_{m}={\mu  \over R_{p}}}

The non-linear operating characteristic of the triode caused early tube audio amplifiers to exhibit harmonic distortion at low volumes. Plotting plate current as a function of applied grid voltage, it was seen that there was a range of grid voltages for which the transfer characteristics were approximately linear.

To use this range, a negative bias voltage had to be applied to the grid to position the DC operating point in the linear region. This was called the idle condition, and the plate current at this point the "idle current". The controlling voltage was superimposed onto the bias voltage, resulting in a linear variation of plate current in response to positive and negative variation of the input voltage around that point.

This concept is called grid bias. Many early radio sets had a third battery called the "C battery" (unrelated to the present-day C cell, for which the letter denotes its size and shape). The C battery's positive terminal was connected to the cathode of the tubes (or "ground" in most circuits) and whose negative terminal supplied this bias voltage to the grids of the tubes.

Later circuits, after tubes were made with heaters isolated from their cathodes, used cathode biasing, avoiding the need for a separate negative power supply. For cathode biasing, a relatively low-value resistor is connected between the cathode and ground. This makes the cathode positive with respect to the grid, which is at ground potential for DC.

However C batteries continued to be included in some equipment even when the "A" and "B" batteries had been replaced by power from the AC mains. That was possible because there was essentially no current draw on these batteries; they could thus last for many years (often longer than all the tubes) without requiring replacement.

When triodes were first used in radio transmitters and receivers, it was found that tuned amplification stages had a tendency to oscillate unless their gain was very limited. This was due to the parasitic capacitance between the plate (the amplifier's output) and the control grid (the amplifier's input), known as the Miller capacitance.

Eventually the technique of neutralization was developed whereby the RF transformer connected to the plate (anode) would include an additional winding in the opposite phase. This winding would be connected back to the grid through a small capacitor, and when properly adjusted would cancel the Miller capacitance. This technique was employed and led to the success of the Neutrodyne radio during the 1920s. However, neutralization required careful adjustment and proved unsatisfactory when used over a wide range of frequencies.

Tetrodes and pentodes
Main articles: Tetrode and Pentode

Tetrode symbol. From top to bottom: plate (anode), screen grid, control grid, cathode, heater (filament).
To combat the stability problems and limited voltage gain due to the Miller effect, the physicist Walter H. Schottky invented the tetrode tube in 1919.[21] He showed that the addition of a second grid, located between the control grid and the plate (anode), known as the screen grid, could solve these problems. ("Screen" in this case refers to electrical "screening" or shielding, not physical construction: all "grid" electrodes in between the cathode and plate are "screens" of some sort rather than solid electrodes since they must allow for the passage of electrons directly from the cathode to the plate). A positive voltage slightly lower than the plate (anode) voltage was applied to it, and was bypassed (for high frequencies) to ground with a capacitor. This arrangement decoupled the anode and the control grid, essentially eliminating the Miller capacitance and its associated problems. The screen's constant voltage also reduced the anode voltage's influence on the space charge. Where the ratio of plate voltage control of plate current to grid control of the plate current (amplification factor) commonly ranges from below ten to perhaps 100, tetrode amplification factors readily exceeded 500. Consequently, higher voltage gains from a single tube became possible, reducing the number of tubes required in many circuits. This two-grid tube is called a tetrode, meaning four active electrodes, and was common by 1926.


At certain values of plate voltage and current, the tetrode characteristic curves are kinked due to secondary emission
However, the tetrode had one new problem. In any tube, electrons strike the anode with sufficient energy to cause the emission of electrons from its surface. In a triode this so-called secondary emission of electrons is not important since they are simply re-captured by the more positive anode (plate). But in a tetrode they can be captured by the screen grid (thus also acting as an anode) since it is also at a high voltage, thus robbing them from the plate current and reducing the amplification of the device. Since secondary electrons can outnumber the primary electrons, in the worst case, particularly as the plate voltage dips below the screen voltage, the plate current can decrease with increasing plate voltage. This is the so-called "tetrode kink" and is an example of negative resistance which can itself cause instability.[22] The otherwise undesirable negative resistance was exploited to produce a simple oscillator circuit only requiring connection of the plate to a resonant LC circuit to oscillate; this was effective over a wide frequency range. The so-called dynatron oscillator thus operated on the same principle of negative resistance as the tunnel diode oscillator many years later. Another undesirable consequence of secondary emission is that in extreme cases enough charge can flow to the screen grid to overheat and destroy it. Later tetrodes had anodes treated to reduce secondary emission; earlier ones such as the type 77 sharp-cutoff pentode connected as a tetrode made better dynatrons.

The solution was to add another grid between the screen grid and the main anode, called the suppressor grid (since it suppressed secondary emission current toward the screen grid). This grid was held at the cathode (or "ground") voltage and its negative voltage (relative to the anode) electrostatically repelled secondary electrons so that they would be collected by the anode after all. This three-grid tube is called a pentode, meaning five electrodes. The pentode was invented in 1926 by Bernard D. H. Tellegen[23] and became generally favored over the simple tetrode. Pentodes are made in two classes: those with the suppressor grid wired internally to the cathode (e.g. EL84/6BQ5) and those with the suppressor grid wired to a separate pin for user access (e.g. 803, 837). An alternative solution for power applications is the beam tetrode or "beam power tube", discussed below.

Multifunction and multisection tubes

The pentagrid converter contains five grids between the cathode and the plate (anode)
Superheterodyne receivers require a local oscillator and mixer, combined in the function of a single pentagrid converter tube. Various alternatives such as using a combination of a triode with a hexode and even an octode have been used for this purpose. The additional grids include control grids (at a low potential) and screen grids (at a high voltage). Many designs use such a screen grid as an additional anode to provide feedback for the oscillator function, whose current adds to that of the incoming radio frequency signal. The pentagrid converter thus became widely used in AM receivers, including the miniature tube version of the "All American Five". Octodes, such as the 7A8, were rarely used in the United States, but much more common in Europe, particularly in battery operated radios where the lower power consumption was an advantage.

To further reduce the cost and complexity of radio equipment, two separate structures (triode and pentode for instance) can be combined in the bulb of a single multisection tube. An early example is the Loewe 3NF. This 1920s device has three triodes in a single glass envelope together with all the fixed capacitors and resistors required to make a complete radio receiver. As the Loewe set had only one tube socket, it was able to substantially undercut the competition, since, in Germany, state tax was levied by the number of sockets. However, reliability was compromised, and production costs for the tube were much greater. In a sense, these were akin to integrated circuits. In the United States, Cleartron briefly produced the "Multivalve" triple triode for use in the Emerson Baby Grand receiver. This Emerson set also has a single tube socket, but because it uses a four-pin base, the additional element connections are made on a "mezzanine" platform at the top of the tube base.

By 1940 multisection tubes had become commonplace. There were constraints, however, due to patents and other licensing considerations (see British Valve Association). Constraints due to the number of external pins (leads) often forced the functions to share some of those external connections such as their cathode connections (in addition to the heater connection). The RCA Type 55 is a double diode triode used as a detector, automatic gain control rectifier and audio preamplifier in early AC powered radios. These sets often include the 53 Dual Triode Audio Output. Another early type of multi-section tube, the 6SN7, is a "dual triode" which performs the functions of two triode tubes while taking up half as much space and costing less. The 12AX7 is a dual "high mu" (high voltage gain[24][25][26]) triode in a miniature enclosure, and became widely used in audio signal amplifiers, instruments, and guitar amplifiers.

The introduction of the miniature tube base (see below) which can have 9 pins, more than previously available, allowed other multi-section tubes to be introduced, such as the 6GH8/ECF82 triode-pentode, quite popular in television receivers. The desire to include even more functions in one envelope resulted in the General Electric Compactron which has 12 pins. A typical example, the 6AG11, contains two triodes and two diodes.

Some otherwise conventional tubes do not fall into standard categories; the 6AR8, 6JH8 and 6ME8 have several common grids, followed by a pair of beam deflection electrodes which deflected the current towards either of two anodes. They were sometimes known as the 'sheet beam' tubes and used in some color TV sets for color demodulation. The similar 7360 was popular as a balanced SSB (de)modulator.

Beam power tubes
Main article: Beam tetrode

6L6 tubes in glass envelopes
The beam power tube is usually a tetrode with the addition of beam-forming electrodes, which take the place of the suppressor grid. These angled plates (not to be confused with the anode) focus the electron stream onto certain spots on the anode which can withstand the heat generated by the impact of massive numbers of electrons, while also providing pentode behavior. The positioning of the elements in a beam power tube uses a design called "critical-distance geometry", which minimizes the "tetrode kink", plate to control grid capacitance, screen grid current, and secondary emission from the anode, thus increasing power conversion efficiency. The control grid and screen grid are also wound with the same pitch, or number of wires per inch. The windings of the control and screen grid wires are aligned such that the screen grid is in the "shadow" of the control grid. The two grids are positioned so that the control grid creates "sheets" of electrons that pass between the screen-grid wires.

Aligning the grid wires also helps to reduce screen current, which represents wasted energy. This design helps to overcome some of the practical barriers to designing high-power, high-efficiency power tubes. EMI engineers Cabot Bull and Sidney Rodda developed the design which became the 6L6, the first popular beam power tube, introduced by RCA in 1936 and later corresponding tubes in Europe the KT66, KT77 and KT88 made by the Marconi-Osram Valve subsidiary of GEC (the KT standing for "Kinkless Tetrode").

"Pentode operation" of beam power tubes is often described in manufacturers' handbooks and data sheets, resulting in some confusion in terminology. While they are not strictly pentodes, their overall electrical behavior is similar.

Variations of the 6L6 design are still widely used in tube guitar amplifiers, making it one of the longest-lived electronic device families in history. Similar design strategies are used in the construction of large ceramic power tetrodes used in radio transmitters.

Beam power tubes can be connected as triodes for improved audio tonal quality but in triode mode deliver significantly reduced power output.

Gas-filled tubes
Gas-filled tubes such as discharge tubes and cold cathode tubes are not hard vacuum tubes, though are always filled with gas at less than sea-level atmospheric pressure. Types such as the voltage-regulator tube and thyratron resemble hard vacuum tubes and fit in sockets designed for vacuum tubes. Their distinctive orange, red, or purple glow during operation indicates the presence of gas; electrons flowing in a vacuum do not produce light within that region. These types may still be referred to as "electron tubes" as they do perform electronic functions. High-power rectifiers use mercury vapor to achieve a lower forward voltage drop than high-vacuum tubes.

Miniature tubes

Miniature tube (right) compared to the older octal style. Not including pins, the larger tube, a 5U4GB, is 93 mm high with a 35 mm diameter base, while the smaller, a 9-pin 12AX7, is 45 mm high, and 20.4 mm in diameter.

Subminiature CV4501 tube (SQ version of EF72), 35 mm long x 10 mm diameter (excluding leads)
Early tubes used a metal or glass envelope atop an insulating bakelite base. In 1938 a technique was developed to use an all-glass construction[27] with the pins fused in the glass base of the envelope. This was used in the design of a much smaller tube outline, known as the miniature tube, having seven or nine pins. Making tubes smaller reduced the voltage where they could safely operate, and also reduced the power dissipation of the filament. Miniature tubes became predominant in consumer applications such as radio receivers and hi-fi amplifiers. However, the larger older styles continued to be used especially as higher-power rectifiers, in higher-power audio output stages and as transmitting tubes.


RCA 6DS4 "Nuvistor" triode, circa 20 mm high by 11 mm diameter
Sub-miniature tubes
Sub-miniature tubes with a size roughly that of half a cigarette were used in one of the very earliest general-purpose digital computers, the Jaincomp-B, produced by the Jacobs Instrument Company,[28][a] and consumer applications as hearing-aid amplifiers. These tubes did not have pins plugging into a socket but were soldered in place. The "acorn tube" (named due to its shape) was also very small, as was the metal-cased RCA nuvistor from 1959, about the size of a thimble. The nuvistor was developed to compete with the early transistors and operated at higher frequencies than those early transistors could. The small size supported especially high-frequency operation; nuvistors were used in aircraft radio transceivers, UHF television tuners, and some HiFi FM radio tuners (Sansui 500A) until replaced by high-frequency capable transistors.

Improvements in construction and performance

Commercial packaging for vacuum tubes used in the latter half of the 20th century including boxes for individual tubes (bottom right), sleeves for rows of the boxes (left), and bags that smaller tubes would be put in by a store upon purchase (top right)
The earliest vacuum tubes strongly resembled incandescent light bulbs and were made by lamp manufacturers, who had the equipment needed to manufacture glass envelopes and the vacuum pumps required to evacuate the enclosures. De Forest used Heinrich Geissler's mercury displacement pump, which left behind a partial vacuum. The development of the diffusion pump in 1915 and improvement by Irving Langmuir led to the development of high-vacuum tubes. After World War I, specialized manufacturers using more economical construction methods were set up to fill the growing demand for broadcast receivers. Bare tungsten filaments operated at a temperature of around 2200 Â°C. The development of oxide-coated filaments in the mid-1920s reduced filament operating temperature to a dull red heat (around 700 Â°C), which in turn reduced thermal distortion of the tube structure and allowed closer spacing of tube elements. This in turn improved tube gain, since the gain of a triode is inversely proportional to the spacing between grid and cathode. Bare tungsten filaments remain in use in small transmitting tubes but are brittle and tend to fracture if handled roughlyâe.g. in the postal services. These tubes are best suited to stationary equipment where impact and vibration is not present. Over time vacuum tubes became much smaller.

Indirectly heated cathodes
The desire to power electronic equipment using AC mains power faced a difficulty with respect to the powering of the tubes' filaments, as these were also the cathode of each tube. Powering the filaments directly from a power transformer introduced mains-frequency (50 or 60 Hz) hum into audio stages. The invention of the "equipotential cathode" reduced this problem, with the filaments being powered by a balanced AC power transformer winding having a grounded center tap.

A superior solution, and one which allowed each cathode to "float" at a different voltage, was that of the indirectly heated cathode: a cylinder of oxide-coated nickel acted as an electron-emitting cathode and was electrically isolated from the filament inside it. Indirectly heated cathodes enable the cathode circuit to be separated from the heater circuit. The filament, no longer electrically connected to the tube's electrodes, became simply known as a "heater", and could as well be powered by AC without any introduction of hum.[29] In the 1930s, indirectly heated cathode tubes became widespread in equipment using AC power. Directly heated cathode tubes continued to be widely used in battery-powered equipment as their filaments required considerably less power than the heaters required with indirectly heated cathodes.

Tubes designed for high gain audio applications may have twisted heater wires to cancel out stray electric fields, fields that could induce objectionable hum into the program material.

Heaters may be energized with either alternating current (AC) or direct current (DC). DC is often used where low hum is required.

Use in electronic computers
See also: List of vacuum tube computers

The 1946 ENIAC computer used 17,468 vacuum tubes and consumed 150 kW of power
Vacuum tubes used as switches made electronic computing possible for the first time, but the cost and relatively short mean time to failure of tubes were limiting factors.[30] "The common wisdom was that valvesâwhich, like light bulbs, contained a hot glowing filamentâcould never be used satisfactorily in large numbers, for they were unreliable, and in a large installation too many would fail in too short a time".[31] Tommy Flowers, who later designed Colossus, "discovered that, so long as valves were switched on and left on, they could operate reliably for very long periods, especially if their 'heaters' were run on a reduced current".[31] In 1934 Flowers built a successful experimental installation using over 3,000 tubes in small independent modules; when a tube failed, it was possible to switch off one module and keep the others going, thereby reducing the risk of another tube failure being caused; this installation was accepted by the Post Office (who operated telephone exchanges). Flowers was also a pioneer of using tubes as very fast (compared to electromechanical devices) electronic switches. Later work confirmed that tube unreliability was not as serious an issue as generally believed; the 1946 ENIAC, with over 17,000 tubes, had a tube failure (which took 15 minutes to locate) on average every two days. The quality of the tubes was a factor, and the diversion of skilled people during the Second World War lowered the general quality of tubes.[32] During the war Colossus was instrumental in breaking German codes. After the war, development continued with tube-based computers including, military computers ENIAC and Whirlwind, the Ferranti Mark 1 (one of the first commercially available electronic computers), and UNIVAC I, also available commercially.

Advances using subminiature tubes included the Jaincomp series of machines produced by the Jacobs Instrument Company of Bethesda, Maryland. Models such as its Jaincomp-B employed just 300 such tubes in a desktop-sized unit that offered performance to rival many of the then room-sized machines.[28]

Colossus
Main article: Colossus computer

Vacuum tubes seen on end in recreation of World War II-era Colossus computer at Bletchley Park, England
Flowers's Colossus and its successor Colossus Mk2 were built by the British during World War II to substantially speed up the task of breaking the German high level Lorenz encryption. Using about 1,500 vacuum tubes (2,400 for Mk2), Colossus replaced an earlier machine based on relay and switch logic (the Heath Robinson). Colossus was able to break in a matter of hours messages that had previously taken several weeks; it was also much more reliable.[31] Colossus was the first use of vacuum tubes working in concert on such a large scale for a single machine.[31]

Once Colossus was built and installed, it ran continuously, powered by dual redundant diesel generators, the wartime mains supply being considered too unreliable. The only time it was switched off was for conversion to Mk2, which added more tubes. Another nine Colossus Mk2s were built. Each Mk2 consumed 15 kilowatts; most of the power was for the tube heaters.

A Colossus reconstruction was switched on in 1996; it was upgraded to Mk2 configuration in 2004; it found the key for a wartime German ciphertext in 2007.[33]

Whirlwind and "special-quality" tubes
Main article: Whirlwind (computer)

Circuitry from core memory unit of Whirlwind
To meet the reliability requirements of the 1951 US digital computer Whirlwind, "special-quality" tubes with extended life, and a long-lasting cathode in particular, were produced. The problem of short lifetime was traced largely to evaporation of silicon, used in the tungsten alloy to make the heater wire easier to draw. The silicon forms barium orthosilicate at the interface between the nickel sleeve and the cathode barium oxide coating.[7]:301 This "cathode interface" is a high-resistance layer (with some parallel capacitance) which greatly reduces the cathode current when the tube is switched into conduction mode.[34]:224 Elimination of silicon from the heater wire alloy (and more frequent replacement of the wire drawing dies) allowed the production of tubes that were reliable enough for the Whirlwind project. High-purity nickel tubing and cathode coatings free of materials such as silicates and aluminum that can reduce emissivity also contribute to long cathode life.

The first such "computer tube" was Sylvania's 7AK7 pentode of 1948 (these replaced the 7AD7, which was supposed to be better quality than the standard 6AG7 but proved too unreliable).[35]:59 Computers were the first tube devices to run tubes at cutoff (enough negative grid voltage to make them cease conduction) for quite-extended periods of time. Running in cutoff with the heater on accelerates cathode poisoning and the output current of the tube will be greatly reduced when switched into conduction mode.[34]:224 The 7AK7 tubes improved the cathode poisoning problem, but that alone was insufficient to achieve the required reliability.[35]:60 Further measures included switching off the heater voltage when the tubes were not required to conduct for extended periods, turning on and off the heater voltage with a slow ramp to avoid thermal shock on the heater element,[34]:226 and stress testing the tubes during offline maintenance periods to bring on early failure of weak units.[35]:60â61

The tubes developed for Whirlwind were later used in the giant SAGE air-defense computer system. By the late 1950s, it was routine for special-quality small-signal tubes to last for hundreds of thousands of hours if operated conservatively. This increased reliability also made mid-cable amplifiers in submarine cables possible.

Heat generation and cooling

The anode (plate) of this transmitting triode has been designed to dissipate up to 500 W of heat
A considerable amount of heat is produced when tubes operate, from both the filament (heater) and the stream of electrons bombarding the plate. In power amplifiers, this source of heat is greater than cathode heating. A few types of tube permit operation with the anodes at a dull red heat; in other types, red heat indicates severe overload.

The requirements for heat removal can significantly change the appearance of high-power vacuum tubes. High power audio amplifiers and rectifiers required larger envelopes to dissipate heat. Transmitting tubes could be much larger still.

Heat escapes the device by black-body radiation from the anode (plate) as infrared radiation, and by convection of air over the tube envelope.[36] Convection is not possible inside most tubes since the anode is surrounded by vacuum.

Tubes which generate relatively little heat, such as the 1.4-volt filament directly heated tubes designed for use in battery-powered equipment, often have shiny metal anodes. 1T4, 1R5 and 1A7 are examples. Gas-filled tubes such as thyratrons may also use a shiny metal anode since the gas present inside the tube allows for heat convection from the anode to the glass enclosure.

The anode is often treated to make its surface emit more infrared energy. High-power amplifier tubes are designed with external anodes that can be cooled by convection, forced air or circulating water. The water-cooled 80 kg, 1.25 MW 8974 is among the largest commercial tubes available today.

In a water-cooled tube, the anode voltage appears directly on the cooling water surface, thus requiring the water to be an electrical insulator to prevent high voltage leakage through the cooling water to the radiator system. Water as usually supplied has ions that conduct electricity; deionized water, a good insulator, is required. Such systems usually have a built-in water-conductance monitor which will shut down the high-tension supply if the conductance becomes too high.

The screen grid may also generate considerable heat. Limits to screen grid dissipation, in addition to plate dissipation, are listed for power devices. If these are exceeded then tube failure is likely.

Tube packages

Metal-cased tubes with octal bases

High power GS-9B triode transmitting tube with heat sink at bottom
Most modern tubes have glass envelopes, but metal, fused quartz (silica) and ceramic have also been used. A first version of the 6L6 used a metal envelope sealed with glass beads, while a glass disk fused to the metal was used in later versions. Metal and ceramic are used almost exclusively for power tubes above 2 kW dissipation. The nuvistor was a modern receiving tube using a very small metal and ceramic package.

The internal elements of tubes have always been connected to external circuitry via pins at their base which plug into a socket. Subminiature tubes were produced using wire leads rather than sockets, however, these were restricted to rather specialized applications. In addition to the connections at the base of the tube, many early triodes connected the grid using a metal cap at the top of the tube; this reduces stray capacitance between the grid and the plate leads. Tube caps were also used for the plate (anode) connection, particularly in transmitting tubes and tubes using a very high plate voltage.

High-power tubes such as transmitting tubes have packages designed more to enhance heat transfer. In some tubes, the metal envelope is also the anode. The 4CX1000A is an external anode tube of this sort. Air is blown through an array of fins attached to the anode, thus cooling it. Power tubes using this cooling scheme are available up to 150 kW dissipation. Above that level, water or water-vapor cooling are used. The highest-power tube currently available is the Eimac 4CM2500KG, a forced water-cooled power tetrode capable of dissipating 2.5 megawatts.[37] By comparison, the largest power transistor can only dissipate about 1 kilowatt.

Names
The generic name "[thermionic] valve" used in the UK derives from the unidirectional current flow allowed by the earliest device, the thermionic diode emitting electrons from a heated filament, by analogy with a non-return valve in a water pipe.[38] The US names "vacuum tube", "electron tube", and "thermionic tube" all simply describe a tubular envelope which has been evacuated ("vacuum"), has a heater and controls electron flow.

In many cases, manufacturers and the military gave tubes designations that said nothing about their purpose (e.g., 1614). In the early days some manufacturers used proprietary names which might convey some information, but only about their products; the KT66 and KT88 were "kinkless tetrodes". Later, consumer tubes were given names that conveyed some information, with the same name often used generically by several manufacturers. In the US, Radio Electronics Television Manufacturers' Association (RETMA) designations comprise a number, followed by one or two letters, and a number. The first number is the (rounded) heater voltage; the letters designate a particular tube but say nothing about its structure; and the final number is the total number of electrodes (without distinguishing between, say, a tube with many electrodes, or two sets of electrodes in a single envelopeâa double triode, for example). For example, the 12AX7 is a double triode (two sets of three electrodes plus heater) with a 12.6V heater (which, as it happens, can also be connected to run from 6.3V). The "AX" has no meaning other than to designate this particular tube according to its characteristics. Similar, but not identical, tubes are the 12AD7, 12AE7...12AT7, 12AU7, 12AV7, 12AW7 (rare!), 12AY7, and the 12AZ7.

A system widely used in Europe known as the MullardâPhilips tube designation, also extended to transistors, uses a letter, followed by one or more further letters, and a number. The type designator specifies the heater voltage or current (one letter), the functions of all sections of the tube (one letter per section), the socket type (first digit), and the particular tube (remaining digits). For example, the ECC83 (equivalent to the 12AX7) is a 6.3V (E) double triode (CC) with a miniature base (8). In this system special-quality tubes (e.g., for long-life computer use) are indicated by moving the number immediately after the first letter: the E83CC is a special-quality equivalent of the ECC83, the E55L a power pentode with no consumer equivalent.

Special-purpose tubes

Voltage-regulator tube in operation. Low-pressure gas within tube glows due to current flow.
Some special-purpose tubes are constructed with particular gases in the envelope. For instance, voltage-regulator tubes contain various inert gases such as argon, helium or neon, which will ionize at predictable voltages. The thyratron is a special-purpose tube filled with low-pressure gas or mercury vapor. Like vacuum tubes, it contains a hot cathode and an anode, but also a control electrode which behaves somewhat like the grid of a triode. When the control electrode starts conduction, the gas ionizes, after which the control electrode can no longer stop the current; the tube "latches" into conduction. Removing anode (plate) voltage lets the gas de-ionize, restoring its non-conductive state.

Some thyratrons can carry large currents for their physical size. One example is the miniature type 2D21, often seen in 1950s jukeboxes as control switches for relays. A cold-cathode version of the thyratron, which uses a pool of mercury for its cathode, is called an ignitron; some can switch thousands of amperes. Thyratrons containing hydrogen have a very consistent time delay between their turn-on pulse and full conduction; they behave much like modern silicon-controlled rectifiers, also called thyristors due to their functional similarity to thyratrons. Hydrogen thyratrons have long been used in radar transmitters.

A specialized tube is the krytron, which is used for rapid high-voltage switching. Krytrons are used to initiate the detonations used to set off a nuclear weapon; krytrons are heavily controlled at an international level.

X-ray tubes are used in medical imaging among other uses. X-ray tubes used for continuous-duty operation in fluoroscopy and CT imaging equipment may use a focused cathode and a rotating anode to dissipate the large amounts of heat thereby generated. These are housed in an oil-filled aluminum housing to provide cooling.

The photomultiplier tube is an extremely sensitive detector of light, which uses the photoelectric effect and secondary emission, rather than thermionic emission, to generate and amplify electrical signals. Nuclear medicine imaging equipment and liquid scintillation counters use photomultiplier tube arrays to detect low-intensity scintillation due to ionizing radiation.

The Ignatron tube was used in resistance welding equipment in the early 1970s. The Ignatron had a cathode, anode and an igniter. The tube base was filled with mercury and the tube was used as a very high current switch. A large current potential was placed between the anode and cathode of the tube but was only permitted to conduct when the igniter in contact with the mercury had enough current to vaporize the mercury and complete the circuit. Because this was used in resistance welding there were two Ignatrons for the two phases of an AC circuit. Because of the mercury at the bottom of the tube they were extremely difficult to ship. These tubes were eventually replaced by SCRs (Silicon Controlled Rectifiers).

Powering the tube
Batteries
Main article: Battery (vacuum tube)
Batteries provided the voltages required by tubes in early radio sets. Three different voltages were generally required, using three different batteries designated as the A, B, and C battery. The "A" battery or LT (low-tension) battery provided the filament voltage. Tube heaters were designed for single, double or triple-cell lead-acid batteries, giving nominal heater voltages of 2 V, 4 V or 6 V. In portable radios, dry batteries were sometimes used with 1.5 or 1 V heaters. Reducing filament consumption improved the life span of batteries. By 1955 towards the end of the tube era, tubes using only 50 mA down to as little as 10 mA for the heaters had been developed.[39]

The high voltage applied to the anode (plate) was provided by the "B" battery or the HT (high-tension) supply or battery. These were generally of dry cell construction and typically came in 22.5-, 45-, 67.5-, 90-, 120- or 135-volt versions. After the use of B-batteries was phased out and rectified line-power was employed to produce the high voltage needed by tubes' plates, the term "B+" persisted in the US when referring to the high voltage source, most of the rest of the English speaking world refers to this supply as just HT (high tension).


Batteries for a vacuum tube circuit. The C battery is highlighted.
Early sets used a grid bias battery or "C" battery which was connected to provide a negative voltage. Since no current flows through a tube's grid connection, these batteries had no current drain and lasted the longest, usually limited by their own shelf life. The supply from the grid bias battery was rarely, if ever, disconnected when the radio was otherwise switched off. Even after AC power supplies became commonplace, some radio sets continued to be built with C batteries, as they would almost never need replacing. However more modern circuits were designed using cathode biasing, eliminating the need for a third power supply voltage; this became practical with tubes using indirect heating of the cathode along with the development of resistor/capacitor coupling which replaced earlier interstage transformers.

The "C battery" for bias is a designation having no relation to the "C cell" battery size.

AC power
"Cheater cord" redirects here. For the three-prong to two-prong mains plug adapter, see Cheater plug.
Battery replacement was a major operating cost for early radio receiver users. The development of the battery eliminator, and, in 1925, batteryless receivers operated by household power, reduced operating costs and contributed to the growing popularity of radio. A power supply using a transformer with several windings, one or more rectifiers (which may themselves be vacuum tubes), and large filter capacitors provided the required direct current voltages from the alternating current source.

As a cost reduction measure, especially in high-volume consumer receivers, all the tube heaters could be connected in series across the AC supply using heaters requiring the same current and with a similar warm-up time. In one such design, a tap on the tube heater string supplied the 6 volts needed for the dial light. By deriving the high voltage from a half-wave rectifier directly connected to the AC mains, the heavy and costly power transformer was eliminated. This also allowed such receivers to operate on direct current, a so-called AC/DC receiver design. Many different US consumer AM radio manufacturers of the era used a virtually identical circuit, given the nickname All American Five.

Where the mains voltage was in the 100â120 V range, this limited voltage proved suitable only for low-power receivers. Television receivers either required a transformer or could use a voltage doubling circuit. Where 230 V nominal mains voltage was used, television receivers as well could dispense with a power transformer.

Transformer-less power supplies required safety precautions in their design to limit the shock hazard to users, such as electrically insulated cabinets and an interlock tying the power cord to the cabinet back, so the line cord was necessarily disconnected if the user or service person opened the cabinet. A cheater cord was a power cord ending in the special socket used by the safety interlock; servicers could then power the device with the hazardous voltages exposed.

To avoid the warm-up delay, "instant on" television receivers passed a small heating current through their tubes even when the set was nominally off. At switch on, full heating current was provided and the set would play almost immediately.

Reliability

Tube tester manufactured in 1930. Despite how it is displayed, it could only test one tube at a time.
One reliability problem of tubes with oxide cathodes is the possibility that the cathode may slowly become "poisoned" by gas molecules from other elements in the tube, which reduce its ability to emit electrons. Trapped gases or slow gas leaks can also damage the cathode or cause plate (anode) current runaway due to ionization of free gas molecules. Vacuum hardness and proper selection of construction materials are the major influences on tube lifetime. Depending on the material, temperature and construction, the surface material of the cathode may also diffuse onto other elements. The resistive heaters that heat the cathodes may break in a manner similar to incandescent lamp filaments, but rarely do, since they operate at much lower temperatures than lamps.

The heater's failure mode is typically a stress-related fracture of the tungsten wire or at a weld point and generally occurs after accruing many thermal (power on-off) cycles. Tungsten wire has a very low resistance when at room temperature. A negative temperature coefficient device, such as a thermistor, may be incorporated in the equipment's heater supply or a ramp-up circuit may be employed to allow the heater or filaments to reach operating temperature more gradually than if powered-up in a step-function. Low-cost radios had tubes with heaters connected in series, with a total voltage equal to that of the line (mains). Some receivers made before World War II had series-string heaters with total voltage less than that of the mains. Some had a resistance wire running the length of the power cord to drop the voltage to the tubes. Others had series resistors made like regular tubes; they were called ballast tubes.

Following World War II, tubes intended to be used in series heater strings were redesigned to all have the same ("controlled") warm-up time. Earlier designs had quite-different thermal time constants. The audio output stage, for instance, had a larger cathode and warmed up more slowly than lower-powered tubes. The result was that heaters that warmed up faster also temporarily had higher resistance, because of their positive temperature coefficient. This disproportionate resistance caused them to temporarily operate with heater voltages well above their ratings, and shortened their life.

Another important reliability problem is caused by air leakage into the tube. Usually oxygen in the air reacts chemically with the hot filament or cathode, quickly ruining it. Designers developed tube designs that sealed reliably. This was why most tubes were constructed of glass. Metal alloys (such as Cunife and Fernico) and glasses had been developed for light bulbs that expanded and contracted in similar amounts, as temperature changed. These made it easy to construct an insulating envelope of glass, while passing connection wires through the glass to the electrodes.

When a vacuum tube is overloaded or operated past its design dissipation, its anode (plate) may glow red. In consumer equipment, a glowing plate is universally a sign of an overloaded tube. However, some large transmitting tubes are designed to operate with their anodes at red, orange, or in rare cases, white heat.

"Special quality" versions of standard tubes were often made, designed for improved performance in some respect, such as a longer life cathode, low noise construction, mechanical ruggedness via ruggedized filaments, low microphony, for applications where the tube will spend much of its time cut off, etc. The only way to know the particular features of a special quality part is by reading the datasheet. Names may reflect the standard name (12AU7==>12AU7A, its equivalent ECC82==>E82CC, etc.), or be absolutely anything (standard and special-quality equivalents of the same tube include 12AU7, ECC82, B329, CV491, E2163, E812CC, M8136, CV4003, 6067, VX7058, 5814A and 12AU7A).[40]

The longest recorded valve life was earned by a Mazda AC/P pentode valve (serial No. 4418) in operation at the BBC's main Northern Ireland transmitter at Lisnagarvey. The valve was in service from 1935 until 1961 and had a recorded life of 232,592 hours. The BBC maintained meticulous records of their valves' lives with periodic returns to their central valve stores.[41][42]

Vacuum

Getter in opened tube; silvery deposit from getter

Dead vacuum fluorescent display (air has leaked in and the getter spot has become white)
A vacuum tube needs an extremely good ("hard") vacuum to avoid the consequences of generating positive ions within the tube. With a small amount of residual gas, some of those atoms may ionize when struck by an electron and create fields that adversely affect the tube characteristics. Larger amounts of residual gas can create a self-sustaining visible glow discharge between the tube elements.[citation needed] To avoid these effects, the residual pressure within the tube must be low enough that the mean free path of an electron is much longer than the size of the tube (so an electron is unlikely to strike a residual atom and very few ionized atoms will be present). Commercial vacuum tubes are evacuated at manufacture to about 0.000001 mmHg (1.0Ã10â6 Torr; 130 Î¼Pa; 1.3Ã10â6 mbar; 1.3Ã10â9 atm).[43]

To prevent gases from compromising the tube's vacuum, modern tubes are constructed with "getters", which are usually small, circular troughs filled with metals that oxidize quickly, barium being the most common. While the tube envelope is being evacuated, the internal parts except the getter are heated by RF induction heating to evolve any remaining gas from the metal parts. The tube is then sealed and the getter is heated to a high temperature, again by radio frequency induction heating, which causes the getter material to vaporize and react with any residual gas. The vapor is deposited on the inside of the glass envelope, leaving a silver-colored metallic patch that continues to absorb small amounts of gas that may leak into the tube during its working life. Great care is taken with the valve design to ensure this material is not deposited on any of the working electrodes. If a tube develops a serious leak in the envelope, this deposit turns a white color as it reacts with atmospheric oxygen. Large transmitting and specialized tubes often use more exotic getter materials, such as zirconium. Early gettered tubes used phosphorus-based getters, and these tubes are easily identifiable, as the phosphorus leaves a characteristic orange or rainbow deposit on the glass. The use of phosphorus was short-lived and was quickly replaced by the superior barium getters. Unlike the barium getters, the phosphorus did not absorb any further gases once it had fired.

Getters act by chemically combining with residual or infiltrating gases, but are unable to counteract (non-reactive) inert gases. A known problem, mostly affecting valves with large envelopes such as cathode ray tubes and camera tubes such as iconoscopes, orthicons, and image orthicons, comes from helium infiltration.[citation needed] The effect appears as impaired or absent functioning, and as a diffuse glow along the electron stream inside the tube. This effect cannot be rectified (short of re-evacuation and resealing), and is responsible for working examples of such tubes becoming rarer and rarer. Unused ("New Old Stock") tubes can also exhibit inert gas infiltration, so there is no long-term guarantee of these tube types surviving into the future.

Transmitting tubes
Large transmitting tubes have carbonized tungsten filaments containing a small trace (1% to 2%) of thorium. An extremely thin (molecular) layer of thorium atoms forms on the outside of the wire's carbonized layer and, when heated, serve as an efficient source of electrons. The thorium slowly evaporates from the wire surface, while new thorium atoms diffuse to the surface to replace them. Such thoriated tungsten cathodes usually deliver lifetimes in the tens of thousands of hours. The end-of-life scenario for a thoriated-tungsten filament is when the carbonized layer has mostly been converted back into another form of tungsten carbide and emission begins to drop off rapidly; a complete loss of thorium has never been found to be a factor in the end-of-life in a tube with this type of emitter. WAAY-TV in Huntsville, Alabama achieved 163,000 hours (18.6 years) of service from an Eimac external cavity klystron in the visual circuit of its transmitter; this is the highest documented service life for this type of tube.[44] It has been said[who?] that transmitters with vacuum tubes are better able to survive lightning strikes than transistor transmitters do. While it was commonly believed that at RF power levels above approximately 20 kilowatts, vacuum tubes were more efficient than solid-state circuits, this is no longer the case, especially in medium wave (AM broadcast) service where solid-state transmitters at nearly all power levels have measurably higher efficiency. FM broadcast transmitters with solid-state power amplifiers up to approximately 15 kW also show better overall power efficiency than tube-based power amplifiers.

Receiving tubes
Cathodes in small "receiving" tubes are coated with a mixture of barium oxide and strontium oxide, sometimes with addition of calcium oxide or aluminium oxide. An electric heater is inserted into the cathode sleeve and insulated from it electrically by a coating of aluminum oxide. This complex construction causes barium and strontium atoms to diffuse to the surface of the cathode and emit electrons when heated to about 780 degrees Celsius.

Failure modes
Catastrophic failures
A catastrophic failure is one that suddenly makes the vacuum tube unusable. A crack in the glass envelope will allow air into the tube and destroy it. Cracks may result from stress in the glass, bent pins or impacts; tube sockets must allow for thermal expansion, to prevent stress in the glass at the pins. Stress may accumulate if a metal shield or other object presses on the tube envelope and causes differential heating of the glass. Glass may also be damaged by high-voltage arcing.

Tube heaters may also fail without warning, especially if exposed to over voltage or as a result of manufacturing defects. Tube heaters do not normally fail by evaporation like lamp filaments since they operate at much lower temperature. The surge of inrush current when the heater is first energized causes stress in the heater and can be avoided by slowly warming the heaters, gradually increasing current with a NTC thermistor included in the circuit. Tubes intended for series-string operation of the heaters across the supply have a specified controlled warm-up time to avoid excess voltage on some heaters as others warm up. Directly heated filament-type cathodes as used in battery-operated tubes or some rectifiers may fail if the filament sags, causing internal arcing. Excess heater-to-cathode voltage in indirectly heated cathodes can break down the insulation between elements and destroy the heater.

Arcing between tube elements can destroy the tube. An arc can be caused by applying voltage to the anode (plate) before the cathode has come up to operating temperature, or by drawing excess current through a rectifier, which damages the emission coating. Arcs can also be initiated by any loose material inside the tube, or by excess screen voltage. An arc inside the tube allows gas to evolve from the tube materials, and may deposit conductive material on internal insulating spacers.[45]

Tube rectifiers have limited current capability and exceeding ratings will eventually destroy a tube.

Degenerative failures
Degenerative failures are those caused by the slow deterioration of performance over time.

Overheating of internal parts, such as control grids or mica spacer insulators, can result in trapped gas escaping into the tube; this can reduce performance. A getter is used to absorb gases evolved during tube operation but has only a limited ability to combine with gas. Control of the envelope temperature prevents some types of gassing. A tube with an unusually high level of internal gas may exhibit a visible blue glow when plate voltage is applied. The getter (being a highly reactive metal) is effective against many atmospheric gases but has no (or very limited) chemical reactivity to inert gases such as helium. One progressive type of failure, especially with physically large envelopes such as those used by camera tubes and cathode-ray tubes, comes from helium infiltration. The exact mechanism is not clear: the metal-to-glass lead-in seals are one possible infiltration site.

Gas and ions within the tube contribute to grid current which can disturb operation of a vacuum tube circuit. Another effect of overheating is the slow deposit of metallic vapors on internal spacers, resulting in inter-element leakage.

Tubes on standby for long periods, with heater voltage applied, may develop high cathode interface resistance and display poor emission characteristics. This effect occurred especially in pulse and digital circuits, where tubes had no plate current flowing for extended times. Tubes designed specifically for this mode of operation were made.

Cathode depletion is the loss of emission after thousands of hours of normal use. Sometimes emission can be restored for a time by raising heater voltage, either for a short time or a permanent increase of a few percent. Cathode depletion was uncommon in signal tubes but was a frequent cause of failure of monochrome television cathode-ray tubes.[46] Usable life of this expensive component was sometimes extended by fitting a boost transformer to increase heater voltage.

Other failures
Vacuum tubes may develop defects in operation that make an individual tube unsuitable in a given device, although it may perform satisfactorily in another application. Microphonics refers to internal vibrations of tube elements which modulate the tube's signal in an undesirable way; sound or vibration pick-up may affect the signals, or even cause uncontrolled howling if a feedback path (with greater than unity gain) develops between a microphonic tube and, for example, a loudspeaker. Leakage current between AC heaters and the cathode may couple into the circuit, or electrons emitted directly from the ends of the heater may also inject hum into the signal. Leakage current due to internal contamination may also inject noise.[47] Some of these effects make tubes unsuitable for small-signal audio use, although unobjectionable for other purposes. Selecting the best of a batch of nominally identical tubes for critical applications can produce better results.

Tube pins can develop non-conducting or high resistance surface films due to heat or dirt. Pins can be cleaned to restore conductance.

Testing
Main article: Tube tester

Universal vacuum tube tester
Vacuum tubes can be tested outside of their circuitry using a vacuum tube tester.

Other vacuum tube devices
Most small signal vacuum tube devices have been superseded by semiconductors, but some vacuum tube electronic devices are still in common use. The magnetron is the type of tube used in all microwave ovens. In spite of the advancing state of the art in power semiconductor technology, the vacuum tube still has reliability and cost advantages for high-frequency RF power generation.

Some tubes, such as magnetrons, traveling-wave tubes, carcinotrons, and klystrons, combine magnetic and electrostatic effects. These are efficient (usually narrow-band) RF generators and still find use in radar, microwave ovens and industrial heating. Traveling-wave tubes (TWTs) are very good amplifiers and are even used in some communications satellites. High-powered klystron amplifier tubes can provide hundreds of kilowatts in the UHF range.

Cathode ray tubes
Main article: Cathode ray tube

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Vacuum tube" â news Â· newspapers Â· books Â· scholar Â· JSTOR (May 2018) (Learn how and when to remove this template message)
The cathode ray tube (CRT) is a vacuum tube used particularly for display purposes. Although there are still many televisions and computer monitors using cathode ray tubes, they are rapidly being replaced by flat panel displays whose quality has greatly improved even as their prices drop. This is also true of digital oscilloscopes (based on internal computers and analog-to-digital converters), although traditional analog scopes (dependent upon CRTs) continue to be produced, are economical, and preferred by many technicians.[citation needed] At one time many radios used "magic eye tubes", a specialized sort of CRT used in place of a meter movement to indicate signal strength or input level in a tape recorder. A modern indicator device, the vacuum fluorescent display (VFD) is also a sort of cathode ray tube.

The X-ray tube is a type of cathode ray tube that generates X-rays when high voltage electrons hit the anode.

Gyrotrons or vacuum masers, used to generate high-power millimeter band waves, are magnetic vacuum tubes in which a small relativistic effect, due to the high voltage, is used for bunching the electrons. Gyrotrons can generate very high powers (hundreds of kilowatts). Free-electron lasers, used to generate high-power coherent light and even X-rays, are highly relativistic vacuum tubes driven by high-energy particle accelerators. Thus, these are sorts of cathode ray tubes.

Electron multipliers
A photomultiplier is a phototube whose sensitivity is greatly increased through the use of electron multiplication. This works on the principle of secondary emission, whereby a single electron emitted by the photocathode strikes a special sort of anode known as a dynode causing more electrons to be released from that dynode. Those electrons are accelerated toward another dynode at a higher voltage, releasing more secondary electrons; as many as 15 such stages provide a huge amplification. Despite great advances in solid-state photodetectors, the single-photon detection capability of photomultiplier tubes makes this vacuum tube device excel in certain applications. Such a tube can also be used for detection of ionizing radiation as an alternative to the GeigerâMÃ¼ller tube (itself not an actual vacuum tube). Historically, the image orthicon TV camera tube widely used in television studios prior to the development of modern CCD arrays also used multistage electron multiplication.

For decades, electron-tube designers tried to augment amplifying tubes with electron multipliers in order to increase gain, but these suffered from short life because the material used for the dynodes "poisoned" the tube's hot cathode. (For instance, the interesting RCA 1630 secondary-emission tube was marketed, but did not last.) However, eventually, Philips of the Netherlands developed the EFP60 tube that had a satisfactory lifetime and was used in at least one product, a laboratory pulse generator. By that time, however, transistors were rapidly improving, making such developments superfluous.

One variant called a "channel electron multiplier" does not use individual dynodes but consists of a curved tube, such as a helix, coated on the inside with material with good secondary emission. One type had a funnel of sorts to capture the secondary electrons. The continuous dynode was resistive, and its ends were connected to enough voltage to create repeated cascades of electrons. The microchannel plate consists of an array of single stage electron multipliers over an image plane; several of these can then be stacked. This can be used, for instance, as an image intensifier in which the discrete channels substitute for focussing.

Tektronix made a high-performance wideband oscilloscope CRT with a channel electron multiplier plate behind the phosphor layer. This plate was a bundled array of a huge number of short individual c.e.m. tubes that accepted a low-current beam and intensified it to provide a display of practical brightness. (The electron optics of the wideband electron gun could not provide enough current to directly excite the phosphor.)

Vacuum tubes in the 21st century
Niche applications
Although vacuum tubes have been largely replaced by solid-state devices in most amplifying, switching, and rectifying applications, there are certain exceptions. In addition to the special functions noted above, tubes still have some niche applications.

In general, vacuum tubes are much less susceptible than corresponding solid-state components to transient overvoltages, such as mains voltage surges or lightning, the electromagnetic pulse effect of nuclear explosions,[48] or geomagnetic storms produced by giant solar flares.[49] This property kept them in use for certain military applications long after more practical and less expensive solid-state technology was available for the same applications, as for example with the MiG-25.[48] In that aircraft, output power of the radar is about one kilowatt and it can burn through a channel under interference.[citation needed]

Vacuum tubes are still[when?] practical alternatives to solid-state devices in generating high power at radio frequencies in applications such as industrial radio frequency heating, particle accelerators, and broadcast transmitters. This is particularly true at microwave frequencies where such devices as the klystron and traveling-wave tube provide amplification at power levels unattainable using current semiconductor devices. The household microwave oven uses a magnetron tube to efficiently generate hundreds of watts of microwave power. Solid-state devices such as gallium nitride are promising replacements, but are very expensive and still[when?] in development.

In military applications, a high-power vacuum tube can generate a 10â100 megawatt signal that can burn out an unprotected receiver's frontend. Such devices are considered non-nuclear electromagnetic weapons; they were introduced in the late 1990s by both the U.S. and Russia.[citation needed]

Audiophiles
Main article: Tube sound

70-watt tube-hybrid audio amplifier selling for US$2,680[50] in 2011, about 10 times the price of a comparable model using transistors.[51]
Enough people prefer tube sound to make tube amplifiers commercially viable in three areas: musical instrument (e.g., guitar) amplifiers, devices used in recording studios, and audiophile equipment.[52]

Many guitarists prefer using valve amplifiers to solid-state models, often due to the way they tend to distort when overdriven.[53] Any amplifier can only accurately amplify a signal to a certain volume; past this limit, the amplifier will begin to distort the signal. Different circuits will distort the signal in different ways; some guitarists prefer the distortion characteristics of vacuum tubes. Most popular vintage models use vacuum tubes.[citation needed]

Displays
Cathode ray tube
The cathode ray tube was the dominant display technology for televisions and computer monitors at the start of the 21st century. However, rapid advances and falling prices of LCD flat panel technology soon took the place of CRTs in these devices.[54] By 2010, most CRT production had ended.[55]

Vacuum fluorescent display
Main article: Vacuum fluorescent display

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2018) (Learn how and when to remove this template message)

Typical VFD used in a videocassette recorder
A modern display technology using a variation of cathode ray tube is often used in videocassette recorders, DVD players and recorders, microwave oven control panels, and automotive dashboards. Rather than raster scanning, these vacuum fluorescent displays (VFD) switch control grids and anode voltages on and off, for instance, to display discrete characters. The VFD uses phosphor-coated anodes as in other display cathode ray tubes. Because the filaments are in view, they must be operated at temperatures where the filament does not glow visibly. This is possible using more recent cathode technology, and these tubes also operate with quite low anode voltages (often less than 50 volts) unlike cathode ray tubes. Their high brightness allows reading the display in bright daylight. VFD tubes are flat and rectangular, as well as relatively thin. Typical VFD phosphors emit a broad spectrum of greenish-white light, permitting use of color filters, though different phosphors can give other colors even within the same display. The design of these tubes provides a bright glow despite the low energy of the incident electrons. This is because the distance between the cathode and anode is relatively small. (This technology is distinct from fluorescent lighting, which uses a discharge tube.)

Vacuum tubes using field electron emitters
Main article: Nanoscale vacuum-channel transistor

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Vacuum tube" â news Â· newspapers Â· books Â· scholar Â· JSTOR (May 2018) (Learn how and when to remove this template message)
In the early years of the 21st century there has been renewed interest in vacuum tubes, this time with the electron emitter formed on a flat silicon substrate, as in integrated circuit technology. This subject is now called vacuum nanoelectronics.[56] The most common design uses a cold cathode in the form of a large-area field electron source (for example a field emitter array). With these devices, electrons are field-emitted from a large number of closely spaced individual emission sites.

Such integrated microtubes may find application in microwave devices including mobile phones, for Bluetooth and Wi-Fi transmission, and in radar and satellite communication.[citation needed] As of 2012, they were being studied for possible applications in field emission display technology, but there were significant production problems.[citation needed]

As of 2014, NASA's Ames Research Center was reported to be working on vacuum-channel transistors produced using CMOS techniques.[57]

Characteristics

Characteristics of the pentode
Space charge of a vacuum tube
The space between the Cathode and the Anode form a cloud which is known as the "space charge".

V-I characteristic of vacuum tube
The V-I characteristic depends upon the size and material of the plate and cathode.[58] Express the ratio between voltage plate and plate current.[59]

V-I curve (Voltage across filaments, plate current)
Plate current,plate voltage characteristics
DC plate resistance of the plate - resistance of the path between anode and cathode of direct current
AC plate resistance of the plate - resistance of the path between anode and cathode of alternating current
Size of electrostatic field
Size of electrostatic field is the size between two or more plates in the tube.

Patents
U.S. Patent 803,684 âInstrument for converting alternating electric currents into continuous currents (Fleming valve patent)
U.S. Patent 841,387 âDevice for amplifying feeble electrical currents
U.S. Patent 879,532 âDe Forest's Audion
See also
icon	Electronics portal
Bogey valueâclose to manufacturer's stated parameter values
Fetronâa solid-state, plug-compatible, replacement for vacuum tubes
List of vacuum tubesâa list of type numbers.
List of vacuum tube computers
MullardâPhilips tube designation
Nixie tubeâa gas-filled display device sometimes misidentified as a vacuum tube
RETMA tube designation
RMA tube designation
Russian tube designations
Tube caddy
Tube tester
Valve amplifier
Zetatron
References
 Reich, Herbert J. (13 April 2013). Principles of Electron Tubes (PDF). Literary Licensing, LLC. ISBN 978-1258664060. Archived (PDF) from the original on 2 April 2017.
 Fundamental Amplifier Techniques with Electron Tubes: Theory and Practice with Design Methods for Self Construction. Elektor Electronics. 1 January 2011. ISBN 978-0905705934.
 "RCA Electron Tube 6BN6/6KS6". Retrieved 13 April 2015.
 John Algeo, "Types of English heteronyms", p. 23 in, Edgar Werner Schneider (ed), Englishes Around the World: General studies, British Isles, North America, John Benjamins Publishing, 1997 ISBN 9027248761.
 Hoddeson, L. "The Vacuum Tube". PBS. Archived from the original on 15 April 2012. Retrieved 6 May 2012.
 Macksey, Kenneth; Woodhouse, William (1991). "Electronics". The Penguin Encyclopedia of Modern Warfare: 1850 to the present day. Viking. p. 110. ISBN 978-0-670-82698-8. The electronics age may be said to have been ushered in with the invention of the vacuum diode valve in 1902 by the Briton John Fleming (himself coining the word 'electronics'), the immediate application being in the field of radio.
 Morgan Jones, Valve Amplifiers, Elsevier, 2012 ISBN 0080966403.
 Olsen, George Henry (2013). Electronics: A General Introduction for the Non-Specialist. Springer. p. 391. ISBN 978-1489965356.
 Rogers, D. C. (1951). "Triode amplifiers in the frequency range 100 Mc/s to 420 Mc/s". Journal of the British Institution of Radio Engineers. 11 (12): 569â575. doi:10.1049/jbire.1951.0074., p.571
 Bray, John (2002). Innovation and the Communications Revolution: From the Victorian Pioneers to Broadband Internet. IET. ISBN 9780852962183. Archived from the original on 3 December 2016.
 Guthrie, Frederick (1876). Magnetism and Electricity. London and Glasgow: William Collins, Sons, & Company. p. 1.[page needed]
 Thomas A. Edison U.S. Patent 307,031 "Electrical Indicator", Issue date: 1884
 Guarnieri, M. (2012). "The age of vacuum tubes: Early devices and the rise of radio communications". IEEE Ind. Electron. M. 6 (1): 41â43. doi:10.1109/MIE.2012.2182822. S2CID 23351454.
 White, Thomas, United States Early Radio History, archived from the original on 18 August 2012
 "Mazda Valves". Archived from the original on 28 June 2013. Retrieved 12 January 2017.
 "Robert von Lieben â Patent Nr 179807 Dated November 19, 1906" (PDF). Kaiserliches Patentamt. 19 November 1906. Archived (PDF) from the original on 28 May 2008. Retrieved 30 March 2008.
 "Archived copy". Archived from the original on 5 October 2013. Retrieved 21 August 2013.
 RÃ¤isÃ¤nen, Antti V.; Lehto, Arto (2003). Radio Engineering for Wireless Communication and Sensor Applications. Artech House. p. 7. ISBN 978-1580536691.
 Edison Tech Center (2015). "General Electric Research Lab History". edisontechcenter.org. Retrieved 12 November 2018.
 J.Jenkins and W.H.Jarvis, "Basic Principles of Electronics, Volume 1 Thermionics", Pergamon Press (1966), Ch.1.10 p.9
 Guarnieri, M. (2012). "The age of vacuum tubes: the conquest of analog communications". IEEE Ind. Electron. M. 6 (2): 52â54. doi:10.1109/MIE.2012.2193274. S2CID 42357863.
 Introduction to Thermionic Valves (Vacuum Tubes) Archived 28 May 2007 at the Wayback Machine, Colin J. Seymour
 "Philips Historical Products: Philips Vacuum Tubes". Archived from the original on 6 November 2013. Retrieved 3 November 2013.
 Baker, Bonnie (2008). Analog circuits. Newnes. p. 391. ISBN 978-0-7506-8627-3.
 Modjeski, Roger A. "Mu, Gm and Rp and how Tubes are matched". VÃ¤lljud AB. Archived from the original on 21 March 2012. Retrieved 22 April 2011.
 Ballou, Glen (1987). Handbook for Sound Engineers: The New Audio Cyclopedia (1st ed.). Howard W. Sams Co. p. 250. ISBN 978-0-672-21983-2. Amplification factor or voltage gain is the amount the signal at the control grid is increased in amplitude after passing through the tube, which is also referred to as the Greek letter Î¼ (mu) or voltage gain (Vg) of the tube.
 C H Gardner (1965) The Story of the Valve Archived 23 December 2015 at the Wayback Machine, Radio Constructor (See particularly the section "Glass Base Construction")
 Pentagon symposium: Commercially Available General Purpose Electronic Digital Computers of Moderate Price, Washington, D.C., 14 MAY 1952
 L.W. Turner (ed.) Electronics Engineer's Reference Book, 4th ed. Newnes-Butterworth, London 1976 ISBN 0-408-00168-2 pages 7â2 through 7-6
 Guarnieri, M. (2012). "The age of Vacuum Tubes: Merging with Digital Computing". IEEE Ind. Electron. M. 6 (3): 52â55. doi:10.1109/MIE.2012.2207830. S2CID 41800914.
 From part of Copeland's "Colossus" available online Archived 23 March 2012 at the Wayback Machine
 Randall, Alexander 5th (14 February 2006). "A lost interview with ENIAC co-inventor J. Presper Eckert". Computer World. Archived from the original on 2 April 2009. Retrieved 25 April 2011.
 The National Museum of ComputingâRebuilding Colossus
The National Museum of ComputingâThe Colossus Gallery
 E.S. Rich, N.H. Taylor, "Component failure analysis in computers", Proceedings of Symposium on Improved Quality Electronic Components, vol. 1, pp. 222â233, Radio-Television Manufacturers Association, 1950.
 Bernd Ulmann, AN/FSQ-7: The Computer that Shaped the Cold War, Walter de Gruyter GmbH, 2014 ISBN 3486856707.
 RCA "Transmitting Tubes Manual" TT-5 1962, p. 10
 "MULTI-PHASE COOLED POWER TETRODE 4CM2500KG" (PDF). Archived (PDF) from the original on 11 October 2016. The maximum anode dissipation rating is 2500 kilowatts.
 The Oxford Companion to the History of Modern Science, J. L. Heilbron, Oxford University Press 2003, 9780195112290, "valve, thermionic"
 Okamura, SÅgo (1994). History of electron tubes. IOS Press. pp. 133â. ISBN 978-90-5199-145-1. Archived from the original on 22 June 2013. Retrieved 9 May 2011.
 National Valve Museum: audio double triodes ECC81, 2, and 3 Archived 7 January 2011 at the Wayback Machine
 Certified by BBC central valve stores, Motspur Park
 Mazda Data Booklet 1968 Page 112.
 C. Robert Meissner (ed.), Vacuum Technology Transactions: Proceedings of the Sixth National Symposium, Elsevier, 2016,ISBN 1483223558 page 96
 31 Alumni. "The Klystron & Cactus". Archived from the original on 20 August 2013. Retrieved 29 December 2013.
 Tomer, Robert B. (1960), Getting the most out of vacuum tubes, Indianapolis, Indiana, USA: Howard W. Sams, LCCN 60-13843. available on the Internet Archive. Chapter 1
 Tomer 1960, 60, chapter 2
 Tomer 1960, 60, chapter 3
 Broad, William J. "Nuclear Pulse (I): Awakening to the Chaos Factor", Science. 29 May 1981 212: 1009â1012
 Y Butt, The Space Review, 2011 Archived 22 April 2012 at the Wayback Machine "... geomagnetic storms, on occasion, can induce more powerful pulses than the E3 pulse from even megaton type nuclear weapons."
 Price of $4,680 for the "super enhanced version". Includes 90-day warranty on tubes "under normal operation conditions". See Model no: SE-300B-70W Archived 12 January 2012 at the Wayback Machine
 Rolls RA200 100 W RMS/Channel @ 4 Ohms Power Amplifier Archived 12 January 2012 at the Wayback Machine. Full Compass. Retrieved on 2011-05-09.
 Barbour, E. (1998). "The cool sound of tubesâvacuum tube musical applications". IEEE Spectrum. 35 (8). IEEE. pp. 24â35. Archived from the original on 4 January 2012.
 Keeports, David (9 February 2017). "The warm, rich sound of valve guitar amplifiers". Physics Education. 52 (2): 025010. doi:10.1088/1361-6552/aa57b7.
 Wong, May (22 October 2006). "Flat Panels Drive Old TVs From Market". AP via USA Today. Retrieved 8 October 2006.
 "The Standard TV" (PDF). Veritas et Visus. Retrieved 12 June 2008.
 Ackerman, Evan. "Vacuum tubes could be the future of computing". Dvice. Dvice. Archived from the original on 25 March 2013. Retrieved 8 February 2013.
 Anthony, Sebastian. "The vacuum tube strikes back: NASA's tiny 460GHz vacuum transistor that could one day replace silicon FETs". ExtremeTech. Archived from the original on 17 November 2015.
 indiastudychannel.com/
 Basic theory and application of Electron tubes Department of the army and air force, AGO 2244-Jan
Notes
 The Jaincomp-B was just 8-1/2 x 21-1/4 x 30" and weighed only 110 lbs, but contained 300 subminiature vacuum tubes and offered performance on a par with then-common building-sized digital computers.[28]
Further reading
Basic Electronics: Volumes 1â5; Van Valkenburgh, Nooger, Neville; John F. Rider Publisher; 1955.
Spangenberg, Karl R. (1948). Vacuum Tubes. McGraw-Hill. OCLC 567981. LCC TK7872.V3.
Millman, J. & Seely, S. Electronics, 2nd ed. McGraw-Hill, 1951.
Shiers, George, "The First Electron Tube", Scientific American, March 1969, p. 104.
Tyne, Gerald, Saga of the Vacuum Tube, Ziff Publishing, 1943, (reprint 1994 Prompt Publications), pp. 30â83.
Stokes, John, 70 Years of Radio Tubes and Valves, Vestal Press, New York, 1982, pp. 3â9.
Thrower, Keith, History of the British Radio Valve to 1940, MMA International, 1982, pp 9â13.
Eastman, Austin V., Fundamentals of Vacuum Tubes, McGraw-Hill, 1949
Philips Technical Library. Books published in the UK in the 1940s and 1950s by Cleaver Hume Press on design and application of vacuum tubes.
RCA Radiotron Designer's Handbook, 1953 (4th Edition). Contains chapters on the design and application of receiving tubes.
Wireless World. Radio Designer's Handbook. UK reprint of the above.
RCA. Receiving Tube Manual, RC15, RC26 (1947, 1968) Issued every two years, contains details of the technical specs of the tubes that RCA sold.
External links
	Wikimedia Commons has media related to Vacuum tubes.
How to build a vacuum tube tester
"The Thermionic Detector"âHJ van der Bijl (October 1919), Popular Science Monthly
How vacuum tubes really workâThermionic emission and vacuum tube theory, using introductory college-level mathematics.
The Vacuum Tube FAQâFAQ from rec.audio
The invention of the thermionic valve. Fleming discovers the thermionic (or oscillation) valve, or 'diode'.
"Tubes Vs. Transistors: Is There an Audible Difference?"â1972 AES paper on audible differences in sound quality between vacuum tubes and transistors.
The Virtual Valve Museum
The cathode ray tube site
O'Neill's Electronic museumâvacuum tube museum
Vacuum tubes for beginnersâJapanese Version
NJ7P Tube DatabaseâData manual for tubes used in North America.
Vacuum tube data sheet locator
Characteristics and datasheets
Video of amateur radio operator making his own vacuum tube triodes
Tuning eye tubes
Archive film of Mullard factory Blackburn
Western Electric specifications sheets for 1940s and 1950s electron and vacuum tubes
vte
Electronic components
vte
Thermionic valves
Authority control Edit this at Wikidata
BNF: cb119777227 (data)GND: 4014330-2LCCN: sh85141746NDL: 00571066
Categories: Vacuum tubes1904 in science1904 in technologyElectrical componentsEnglish inventionsGlass applicationsTelecommunications-related introductions in 1904Vacuum
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Ð¡ÑÐ¿ÑÐºÐ¸ / srpski
Tiáº¿ng Viá»t
ä¸­æ
56 more
Edit links
This page was last edited on 27 October 2020, at 17:37 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Thermionic emission
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

Closeup of the filament in a low pressure mercury gas-discharge lamp showing white thermionic emission mix coating on the central portion of the coil. Typically made of a mixture of barium, strontium and calcium oxides, the coating is sputtered away through normal use, often eventually resulting in lamp failure.

One of the bulbs with which Edison discovered thermionic emission. It consists of an evacuated glass light bulb containing a carbon filament (hairpin shape), with an additional metal plate attached to wires emerging from the base. Electrons released by the filament were attracted to the plate when it had a positive voltage.
Thermionic emission is the liberation of electrons from an electrode by virtue of its temperature (releasing of energy supplied by heat). This occurs because the thermal energy given to the charge carrier overcomes the work function of the material. The charge carriers can be electrons or ions, and in older literature are sometimes referred to as thermions. After emission, a charge that is equal in magnitude and opposite in sign to the total charge emitted is initially left behind in the emitting region. But if the emitter is connected to a battery, the charge left behind is neutralized by charge supplied by the battery as the emitted charge carriers move away from the emitter, and finally the emitter will be in the same state as it was before emission.

The classical example of thermionic emission is that of electrons from a hot cathode into a vacuum (also known as thermal electron emission or the Edison effect) in a vacuum tube. The hot cathode can be a metal filament, a coated metal filament, or a separate structure of metal or carbides or borides of transition metals. Vacuum emission from metals tends to become significant only for temperatures over 1,000 K (730 Â°C; 1,340 Â°F).

The term 'thermionic emission' is now also used to refer to any thermally-excited charge emission process, even when the charge is emitted from one solid-state region into another. This process is crucially important in the operation of a variety of electronic devices and can be used for electricity generation (such as thermionic converters and electrodynamic tethers) or cooling. The magnitude of the charge flow increases dramatically with increasing temperature.


Contents
1	History
2	Richardson's law
3	Schottky emission
4	Photon-enhanced thermionic emission
5	References
6	External links
History

The Edison effect in a diode tube. A diode tube is connected in two configurations; one has a flow of electrons and the other does not. Note that the arrows represent electron current, not conventional current.
Because the electron was not identified as a separate physical particle until the work of J. J. Thomson in 1897, the word "electron" was not used when discussing experiments that took place before this date.

The phenomenon was initially reported in 1853 by Edmond Becquerel.[1][2] It was rediscovered in 1873 by Frederick Guthrie in Britain.[3] While doing work on charged objects, Guthrie discovered that a red-hot iron sphere with a negative charge would lose its charge (by somehow discharging it into air). He also found that this did not happen if the sphere had a positive charge.[4] Other early contributors included Johann Wilhelm Hittorf (1869â1883),[5] Eugen Goldstein (1885),[6] and Julius Elster and Hans Friedrich Geitel (1882â1889).[7]

The effect was rediscovered again by Thomas Edison on February 13, 1880, while he was trying to discover the reason for breakage of lamp filaments and uneven blackening (darkest near the positive terminal of the filament) of the bulbs in his incandescent lamps.

Edison built several experimental lamp bulbs with an extra wire, metal plate, or foil inside the bulb that was separate from the filament and thus could serve as an electrode. He connected a galvanometer, a device used to measure current (the flow of charge), to the output of the extra metal electrode. If the foil was put at a negative potential relative to the filament, there was no measurable current between the filament and the foil. When the foil was raised to a positive potential relative to the filament, there could be a significant current between the filament through the vacuum to the foil if the filament was heated sufficiently (by its own external power source).

We now know that the filament was emitting electrons, which were attracted to a positively charged foil, but not a negatively charged one. This one-way current was called the Edison effect (although the term is occasionally used to refer to thermionic emission itself). He found that the current emitted by the hot filament increased rapidly with increasing voltage, and filed a patent application for a voltage-regulating device using the effect on November 15, 1883 (U.S. patent 307,031,[8] the first US patent for an electronic device). He found that sufficient current would pass through the device to operate a telegraph sounder. This was exhibited at the International Electrical Exposition in Philadelphia in September 1884. William Preece, a British scientist, took back with him several of the Edison effect bulbs. He presented a paper on them in 1885, where he referred to thermionic emission as the "Edison Effect."[9][10] The British physicist John Ambrose Fleming, working for the British "Wireless Telegraphy" Company, discovered that the Edison Effect could be used to detect radio waves. Fleming went on to develop the two-element vacuum tube known as the diode, which he patented on November 16, 1904.[11]

The thermionic diode can also be configured as a device that converts a heat difference to electric power directly without moving parts (a thermionic converter, a type of heat engine).

Richardson's law
Following J. J. Thomson's identification of the electron in 1897, the British physicist Owen Willans Richardson began work on the topic that he later called "thermionic emission". He received a Nobel Prize in Physics in 1928 "for his work on the thermionic phenomenon and especially for the discovery of the law named after him".

From band theory, there are one or two electrons per atom in a solid that are free to move from atom to atom. This is sometimes collectively referred to as a "sea of electrons". Their velocities follow a statistical distribution, rather than being uniform, and occasionally an electron will have enough velocity to exit the metal without being pulled back in. The minimum amount of energy needed for an electron to leave a surface is called the work function. The work function is characteristic of the material and for most metals is on the order of several electronvolts. Thermionic currents can be increased by decreasing the work function. This often-desired goal can be achieved by applying various oxide coatings to the wire.

In 1901 Richardson published the results of his experiments: the current from a heated wire seemed to depend exponentially on the temperature of the wire with a mathematical form similar to the Arrhenius equation.[12] Later, he proposed that the emission law should have the mathematical form[13]

{\displaystyle J=A_{\mathrm {G} }T^{2}\mathrm {e} ^{-W \over kT}}J=A_{{{\mathrm  {G}}}}T^{2}{\mathrm  {e}}^{{-W \over kT}}
where J is the emission current density, T is the temperature of the metal, W is the work function of the metal, k is the Boltzmann constant, and AG is a parameter discussed next.

In the period 1911 to 1930, as physical understanding of the behaviour of electrons in metals increased, various theoretical expressions (based on different physical assumptions) were put forward for AG, by Richardson, Saul Dushman, Ralph H. Fowler, Arnold Sommerfeld and Lothar Wolfgang Nordheim. Over 60 years later, there is still no consensus amongst interested theoreticians as to the exact expression of AG, but there is agreement that AG must be written in the form

{\displaystyle A_{\mathrm {G} }=\;\lambda _{\mathrm {R} }A_{0}}A_{{{\mathrm  {G}}}}=\;\lambda _{{{\mathrm  {R}}}}A_{0}
where Î»R is a material-specific correction factor that is typically of order 0.5, and A0 is a universal constant given by[13]

{\displaystyle A_{0}={4\pi mk^{2}q_{e} \over h^{3}}=1.20173\times 10^{6}\,\mathrm {A\,m^{-2}\,K^{-2}} }{\displaystyle A_{0}={4\pi mk^{2}q_{e} \over h^{3}}=1.20173\times 10^{6}\,\mathrm {A\,m^{-2}\,K^{-2}} }
where m and {\displaystyle -q_{e}}{\displaystyle -q_{e}} are the mass and charge of an electron, and h is Planck's constant.

In fact, by about 1930 there was agreement that, due to the wave-like nature of electrons, some proportion rav of the outgoing electrons would be reflected as they reached the emitter surface, so the emission current density would be reduced, and Î»R would have the value (1-rav). Thus, one sometimes sees the thermionic emission equation written in the form

{\displaystyle J=(1-r_{\mathrm {av} })\lambda _{B}A_{0}T^{2}\mathrm {e} ^{-W \over kT}}{\displaystyle J=(1-r_{\mathrm {av} })\lambda _{B}A_{0}T^{2}\mathrm {e} ^{-W \over kT}}.
However, a modern theoretical treatment by Modinos assumes that the band-structure of the emitting material must also be taken into account. This would introduce a second correction factor Î»B into Î»R, giving {\displaystyle A_{\mathrm {G} }=\lambda _{\mathrm {B} }(1-r_{\mathrm {av} })A_{0}}A_{{{\mathrm  {G}}}}=\lambda _{{{\mathrm  {B}}}}(1-r_{{{\mathrm  {av}}}})A_{0}. Experimental values for the "generalized" coefficient AG are generally of the order of magnitude of A0, but do differ significantly as between different emitting materials, and can differ as between different crystallographic faces of the same material. At least qualitatively, these experimental differences can be explained as due to differences in the value of Î»R.

Considerable confusion exists in the literature of this area because: (1) many sources do not distinguish between AG and A0, but just use the symbol A (and sometimes the name "Richardson constant") indiscriminately; (2) equations with and without the correction factor here denoted by Î»R are both given the same name; and (3) a variety of names exist for these equations, including "Richardson equation", "Dushman's equation", "RichardsonâDushman equation" and "RichardsonâLaueâDushman equation". In the literature, the elementary equation is sometimes given in circumstances where the generalized equation would be more appropriate, and this in itself can cause confusion. To avoid misunderstandings, the meaning of any "A-like" symbol should always be explicitly defined in terms of the more fundamental quantities involved.

Because of the exponential function, the current increases rapidly with temperature when kT is less than W. (For essentially every material, melting occurs well before kT = W.)

Schottky emission
Accuracy dispute
This article appears to contradict the article Schottky effect. Please see discussion on the linked talk page. (March 2013) (Learn how and when to remove this template message)
Main article: Schottky effect
In electron emission devices, especially electron guns, the thermionic electron emitter will be biased negative relative to its surroundings. This creates an electric field of magnitude F at the emitter surface. Without the field, the surface barrier seen by an escaping Fermi-level electron has height W equal to the local work-function. The electric field lowers the surface barrier by an amount ÎW, and increases the emission current. This is known as the Schottky effect (named for Walter H. Schottky) or field enhanced thermionic emission. It can be modeled by a simple modification of the Richardson equation, by replacing W by (W â ÎW). This gives the equation[14][15]

{\displaystyle J(F,T,W)=A_{\mathrm {G} }T^{2}e^{-(W-\Delta W) \over kT}}J (F,T,W) = A_{\mathrm{G}} T^2 e^{ - (W - \Delta W) \over k T}
{\displaystyle \Delta W={\sqrt {{q_{e}}^{3}F \over 4\pi \epsilon _{0}}},}{\displaystyle \Delta W={\sqrt {{q_{e}}^{3}F \over 4\pi \epsilon _{0}}},}
where Îµ0 is the electric constant (also, formerly, called the vacuum permittivity).

Electron emission that takes place in the field-and-temperature-regime where this modified equation applies is often called Schottky emission. This equation is relatively accurate for electric field strengths lower than about 108 V  mâ1. For electric field strengths higher than 108 V mâ1, so-called Fowler-Nordheim (FN) tunneling begins to contribute significant emission current. In this regime, the combined effects of field-enhanced thermionic and field emission can be modeled by the Murphy-Good equation for thermo-field (T-F) emission.[16] At even higher fields, FN tunneling becomes the dominant electron emission mechanism, and the emitter operates in the so-called "cold field electron emission (CFE)" regime.

Thermionic emission can also be enhanced by interaction with other forms of excitation such as light.[17] For example, excited Cs-vapours in thermionic converters form clusters of Cs-Rydberg matter which yield a decrease of collector emitting work function from 1.5 eV to 1.0â0.7 eV. Due to long-lived nature of Rydberg matter this low work function remains low which essentially increases the low-temperature converter's efficiency.[18]

Photon-enhanced thermionic emission
Photon-enhanced thermionic emission (PETE) is a process developed by scientists at Stanford University that harnesses both the light and heat of the sun to generate electricity and increases the efficiency of solar power production by more than twice the current levels. The device developed for the process reaches peak efficiency above 200 Â°C, while most silicon solar cells become inert after reaching 100 Â°C. Such devices work best in parabolic dish collectors, which reach temperatures up to 800 Â°C. Although the team used a gallium nitride semiconductor in its proof-of-concept device, it claims that the use of gallium arsenide can increase the device's efficiency to 55â60 percent, nearly triple that of existing systems,[19][20] and 12â17 percent more than existing 43 percent multi-junction solar cells.[21][22]

References
 Paxton, William. "THERMIONIC ELECTRONEMISSION PROPERTIES OF NITROGEN-INCORPORATED POLYCRYSTALLINE DIAMOND FILMS" (PDF). Archived (PDF) from the original on 2016-11-23. Retrieved 2016-11-22.
 "Thermionic power converter". Encyclopedia Britannica. Archived from the original on 2016-11-23. Retrieved 2016-11-22.
 See:
Guthrie, Frederick (October 1873). "On a relation between heat and static electricity". The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science. 4th. 46 (306): 257â266. doi:10.1080/14786447308640935. Archived from the original on 2018-01-13.
Guthrie, Frederick (February 13, 1873). "On a new relation between heat and electricity". Proceedings of the Royal Society of London. 21 (139â147): 168â169. doi:10.1098/rspl.1872.0037. Archived from the original on January 13, 2018.
 Richardson, O. W. (2003). Thermionic Emission from Hot Bodies. Wexford College Press. p. 196. ISBN 978-1-929148-10-3. Archived from the original on 2013-12-31.
 See:
Hittorf, W. (1869). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie. 2nd series (in German). 136 (1): 1â31. doi:10.1002/andp.18692120102.
Hittorf, W. (1869). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie. 2nd series (in German). 136 (2): 197â234. doi:10.1002/andp.18692120203.
Hittorf, W. (1874). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie (in German). Jubalband (anniversary volume): 430â445. Archived from the original on 2018-01-13.
Hittorf, W. (1879). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie. 3rd series (in German). 7 (8): 553â631. doi:10.1002/andp.18792430804.
Hittorf, W. (1883). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie. 3rd series (in German). 20 (12): 705â755. doi:10.1002/andp.18832561214.
Hittorf, W. (1884). "Ueber die ElectricitÃ¤tsleitung der Gase" [On electrical conduction of gases]. Annalen der Physik und Chemie. 3rd series (in German). 21: 90â139. doi:10.1002/andp.18842570105.
 E. Goldstein (1885) "Ueber electrische Leitung in Vacuum" Archived 2018-01-13 at the Wayback Machine (On electric conduction in vacuum) Annalen der Physik und Chemie, 3rd series, 24 : 79-92.
 See:
Elster and Geitel (1882) "Ueber die ElectricitÃ¤t der Flamme" (On the electricity of flames), Annalen der Physik und Chemie, 3rd series, 16 : 193-222.
Elster and Geitel (1883) "Ueber ElectricitÃ¤tserregung beim Contact von Gasen und glÃ¼henden KÃ¶rpern" (On the generation of electricity by the contact of gases and incandescent bodies), Annalen der Physik und Chemie, 3rd series, 19 : 588-624.
Elster and Geitel (1885) "Ueber die unipolare Leitung erhitzter Gase" (On the unipolar conductivity of heated gases") Annalen der Physik und Chemie, 3rd series, 26 : 1-9.
Elster and Geitel (1887) "Ueber die Electrisirung der Gase durch glÃ¼hende KÃ¶rper" (On the electrification of gases by incandescent bodies") Annalen der Physik und Chemie, 3rd series, 31 : 109-127.
Elster and Geitel (1889) "Ueber die ElectricitÃ¤tserregung beim Contact verdÃ¼nnter Gase mit galvanisch glÃ¼henden DrÃ¤hten" (On the generation of electricity by contact of rarefied gas with electrically heated wires) Annalen der Physik und Chemie, 3rd series, 37 : 315-329.
 US 307031, Edison, Thomas A., "Electrical indicator", published November 15, 1883, issued October 21, 1884
 Preece, William Henry (1885). "On a peculiar behaviour of glow lamps when raised to high incandescence". Proceedings of the Royal Society of London. 38 (235â238): 219â230. doi:10.1098/rspl.1884.0093. Archived from the original on 2014-06-26. Preece coins the term the "Edison effect" on page 229.
 Josephson, M. (1959). Edison. McGraw-Hill. ISBN 978-0-07-033046-7.
 See:
Provisional specification for a thermionic valve was lodged on November 16, 1904. In this document, Fleming coined the British term "valve" for what in North America is called a "vacuum tube": "The means I employ for this purpose consists in the insertion in the circuit of the alternating current of an appliance which permits only the passage of electric current in one direction and constitutes therefore an electrical valve."
GB 190424850, Fleming, John Ambrose, "Improvements in instruments for detecting and measuring alternating electric currents", published August 15, 1905, issued September 21, 1905
US 803684, Fleming, John Ambrose, "Instrument for converting alternating electric currents into continuous currents", published April 29, 1905, issued November 7, 1905
 O. W. Richardson (1901) "On the negative radiation from hot platinum," Philosophical of the Cambridge Philosophical Society, 11 : 286-295.
 Crowell, C. R. (1965). "The Richardson constant for thermionic emission in Schottky barrier diodes". Solid-State Electronics. 8 (4): 395â399. Bibcode:1965SSEle...8..395C. doi:10.1016/0038-1101(65)90116-4.
 Kiziroglou, M. E.; Li, X.; Zhukov, A. A.; De Groot, P. A. J.; De Groot, C. H. (2008). "Thermionic field emission at electrodeposited Ni-Si Schottky barriers" (PDF). Solid-State Electronics. 52 (7): 1032â1038. Bibcode:2008SSEle..52.1032K. doi:10.1016/j.sse.2008.03.002.
 Orloff, J. (2008). "Schottky emission". Handbook of Charged Particle Optics (2nd ed.). CRC Press. pp. 5â6. ISBN 978-1-4200-4554-3. Archived from the original on 2017-01-17.
 Murphy, E. L.; Good, G. H. (1956). "Thermionic Emission, Field Emission, and the Transition Region". Physical Review. 102 (6): 1464â1473. Bibcode:1956PhRv..102.1464M. doi:10.1103/PhysRev.102.1464.
 Mal'Shukov, A. G.; Chao, K. A. (2001). "Opto-Thermionic Refrigeration in Semiconductor Heterostructures". Physical Review Letters. 86 (24): 5570â5573. Bibcode:2001PhRvL..86.5570M. doi:10.1103/PhysRevLett.86.5570. PMID 11415303.
 Svensson, R.; Holmlid, L. (1992). "Very low work function surfaces from condensed excited states: Rydber matter of cesium". Surface Science. 269/270: 695â699. Bibcode:1992SurSc.269..695S. doi:10.1016/0039-6028(92)91335-9.
 Bergeron, L. (2 August 2010). "New solar energy conversion process discovered by Stanford engineers could revamp solar power production". Stanford Report. Archived from the original on 11 April 2011. Retrieved 2010-08-04.
 Schwede, J. W.; et al. (2010). "Photon-enhanced thermionic emission for solar concentrator systems". Nature Materials. 9 (9): 762â767. Bibcode:2010NatMa...9..762S. doi:10.1038/nmat2814. PMID 20676086.
 Green, M. A.; Emery, K.; Hishikawa, Y.; Warta, W. (2011). "Solar cell efficiency tables (version 37)". Progress in Photovoltaics: Research and Applications. 19 (1): 84. doi:10.1002/pip.1088.
 Ang, Yee Sin; Ang, L. K. (2016). "Current-Temperature Scaling for a Schottky Interface with Nonparabolic Energy Dispersion". Physical Review Applied. 6 (3): 034013. arXiv:1609.00460. Bibcode:2016PhRvP...6c4013A. doi:10.1103/PhysRevApplied.6.034013. S2CID 119221695.
External links
How vacuum tubes really work with a section on thermionic emission, with equations, john-a-harper.com.
Owen Richardson's Nobel lecture on thermionics, nobel.se, December 12, 1929. (PDF)
Derivations of thermionic emission equations from an undergraduate lab, csbsju.edu.
vte
Thermionic valves
vte
Thomas Edison
Categories: Atomic physicsElectricityEnergy conversionVacuum tubesThomas Edison
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
ä¸­æ
31 more
Edit links
This page was last edited on 4 October 2020, at 06:53 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Work function
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
In solid-state physics, the work function (sometimes spelled workfunction) is the minimum thermodynamic work (i.e., energy) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface. Here "immediately" means that the final electron position is far from the surface on the atomic scale, but still too close to the solid to be influenced by ambient electric fields in the vacuum. The work function is not a characteristic of a bulk material, but rather a property of the surface of the material (depending on crystal face and contamination).


Contents
1	Definition
2	Applications
3	Measurement
3.1	Methods based on thermionic emission
3.1.1	Work function of cold electron collector
3.2	Methods based on photoemission
3.3	Kelvin probe method
4	Work functions of elements
5	Physical factors that determine the work function
5.1	Surface dipole
5.2	Doping and electric field effect (semiconductors)
5.3	Theoretical models of metal work functions
6	References
7	Further reading
8	External links
Definition
The work function W for a given surface is defined by the difference[1]

{\displaystyle W=-e\phi -E_{\rm {F}},}W = -e\phi - E_{\rm F},
where âe is the charge of an electron, Ï is the electrostatic potential in the vacuum nearby the surface, and EF is the Fermi level (electrochemical potential of electrons) inside the material. The term âeÏ is the energy of an electron at rest in the vacuum nearby the surface.


Plot of electron energy levels against position, in a gold-vacuum-aluminium system. The two metals depicted here are in complete thermodynamic equilibrium. However, the vacuum electrostatic potential Ï is not flat due to a difference in work function.
In practice, one directly controls EF by the voltage applied to the material through electrodes, and the work function is generally a fixed characteristic of the surface material. Consequently, this means that when a voltage is applied to a material, the electrostatic potential Ï produced in the vacuum will be somewhat lower than the applied voltage, the difference depending on the work function of the material surface. Rearranging the above equation, one has

{\displaystyle \phi =V-{\frac {W}{e}}}\phi = V - \frac{W}{e}
where V = âEF/e is the voltage of the material (as measured by a voltmeter, through an attached electrode), relative to an electrical ground that is defined as having zero Fermi level. The fact that Ï depends on the material surface means that the space between two dissimilar conductors will have a built-in electric field, when those conductors are in total equilibrium with each other (electrically shorted to each other, and with equal temperatures). An example of this situation is depicted in the adjacent figure. As described in the next section, these built-in vacuum electric fields can have important consequences in some cases.

Applications
Thermionic emission
In thermionic electron guns, the work function and temperature of the hot cathode are critical parameters in determining the amount of current that can be emitted. Tungsten, the common choice for vacuum tube filaments, can survive to high temperatures but its emission is somewhat limited due to its relatively high work function (approximately 4.5 eV). By coating the tungsten with a substance of lower work function (e.g., thorium or barium oxide), the emission can be greatly increased. This prolongs the lifetime of the filament by allowing operation at lower temperatures (for more information, see hot cathode).
Band bending models in solid-state electronics
The behavior of a solid-state device is strongly dependent on the size of various Schottky barriers and band offsets in the junctions of differing materials, such as metals, semiconductors, and insulators. Some commonly used heuristic approaches to predict the band alignment between materials, such as Anderson's rule and the Schottky-Mott rule, are based on the thought experiment of two materials coming together in vacuum, such that the surfaces charge up and adjust their work functions to become equal just before contact. In reality these work function heuristics are inaccurate due to their neglect of numerous microscopic effects. However, they provide a convenient estimate until the true value can be determined by experiment.[2][3]
Equilibrium electric fields in vacuum chambers
Variation in work function between different surfaces causes a non-uniform electrostatic potential in the vacuum. Even on an ostensibly uniform surface, variations in W known as patch potentials are always present due to microscopic inhomogeneities. Patch potentials have disrupted sensitive apparatus that rely on a perfectly uniform vacuum, such as Casimir force experiments[4] and the Gravity Probe B experiment.[5] Critical apparatus may have surfaces covered with molybdenum, which shows low variations in work function between different crystal faces.[6]
Contact electrification
If two conducting surfaces are moved relative to each other, and there is potential difference in the space between them, then an electric current will be driven. This is because the surface charge on a conductor depends on the magnitude of the electric field, which in turn depends on the distance between the surfaces. The externally observed electrical effects are largest when the conductors are separated by the smallest distance without touching (once brought into contact, the charge will instead flow internally through the junction between the conductors). Since two conductors in equilibrium can have a built-in potential difference due to work function differences, this means that bringing dissimilar conductors into contact, or pulling them apart, will drive electric currents. These contact currents can damage sensitive microelectronic circuitry and occur even when the conductors would be grounded in the absence of motion.[7]
Measurement
Certain physical phenomena are highly sensitive to the value of the work function. The observed data from these effects can be fitted to simplified theoretical models, allowing one to extract a value of the work function. These phenomenologically extracted work functions may be slightly different from the thermodynamic definition given above. For inhomogeneous surfaces, the work function varies from place to place, and different methods will yield different values of the typical "work function" as they average or select differently among the microscopic work functions.[8]

Many techniques have been developed based on different physical effects to measure the electronic work function of a sample. One may distinguish between two groups of experimental methods for work function measurements: absolute and relative.

Absolute methods employ electron emission from the sample induced by photon absorption (photoemission), by high temperature (thermionic emission), due to an electric field (field electron emission), or using electron tunnelling.
Relative methods make use of the contact potential difference between the sample and a reference electrode. Experimentally, either an anode current of a diode is used or the displacement current between the sample and reference, created by an artificial change in the capacitance between the two, is measured (the Kelvin Probe method, Kelvin probe force microscope). However, absolute work function values can be obtained if the tip is first calibrated against a reference sample.[9]
Methods based on thermionic emission
The work function is important in the theory of thermionic emission, where thermal fluctuations provide enough energy to "evaporate" electrons out of a hot material (called the 'emitter') into the vacuum. If these electrons are absorbed by another, cooler material (called the collector) then a measurable electric current will be observed. Thermionic emission can be used to measure the work function of both the hot emitter and cold collector. Generally, these measurements involve fitting to Richardson's law, and so they must be carried out in a low temperature and low current regime where space charge effects are absent.


Energy level diagrams for thermionic diode in forward bias configuration, used to extract all hot electrons coming out from the emitter's surface. The barrier is the vacuum near emitter surface.
In order to move from the hot emitter to the vacuum, an electron's energy must exceed the emitter Fermi level by an amount

{\displaystyle E_{\rm {barrier}}=W_{\rm {e}}}E_{\rm barrier} = W_{\rm e}
determined simply by the thermionic work function of the emitter. If an electric field is applied towards the surface of the emitter, then all of the escaping electrons will be accelerated away from the emitter and absorbed into whichever material is applying the electric field. According to Richardson's law the emitted current density (per unit area of emitter), Je (A/m2), is related to the absolute temperature Te of the emitter by the equation:

{\displaystyle J_{\rm {e}}=-A_{\rm {e}}T_{\rm {e}}^{2}e^{-E_{\rm {barrier}}/kT_{\rm {e}}}}J_{\rm e} = -A_{\rm e} T_{\rm e}^2 e^{-E_{\rm barrier} / k T_{\rm e}}
where k is the Boltzmann constant and the proportionality constant Ae is the Richardson's constant of the emitter. In this case, the dependence of Je on Te can be fitted to yield We.

Work function of cold electron collector

Energy level diagrams for thermionic diode in retarding potential configuration. The barrier is the vacuum near collector surface.
The same setup can be used to instead measure the work function in the collector, simply by adjusting the applied voltage. If an electric field is applied away from the emitter instead, then most of the electrons coming from the emitter will simply be reflected back to the emitter. Only the highest energy electrons will have enough energy to reach the collector, and the height of the potential barrier in this case depends on the collector's work function, rather than the emitter's.

The current is still governed by Richardson's law. However, in this case the barrier height does not depend on We. The barrier height now depends on the work function of the collector, as well as any additional applied voltages:[10]

{\displaystyle E_{\rm {barrier}}=W_{\rm {c}}-e(\Delta V_{\rm {ce}}-\Delta V_{\rm {S}})}E_{\rm barrier} = W_{\rm c} - e (\Delta V_{\rm ce} - \Delta V_{\rm S})
where Wc is the collector's thermionic work function, ÎVce is the applied collectorâemitter voltage, and ÎVS is the Seebeck voltage in the hot emitter (the influence of ÎVS is often omitted, as it is a small contribution of order 10 mV). The resulting current density Jc through the collector (per unit of collector area) is again given by Richardson's Law, except now

{\displaystyle J_{\rm {c}}=AT_{\rm {e}}^{2}e^{-E_{\rm {barrier}}/kT_{\rm {e}}}}J_{\rm c} = A T_{\rm e}^2 e^{-E_{\rm barrier}/kT_{\rm e}}
where A is a Richardson-type constant that depends on the collector material but may also depend on the emitter material, and the diode geometry. In this case, the dependence of Jc on Te, or on ÎVce, can be fitted to yield Wc.

This retarding potential method is one of the simplest and oldest methods of measuring work functions, and is advantageous since the measured material (collector) is not required to survive high temperatures.

Methods based on photoemission

Photoelectric diode in forward bias configuration, used for measuring the work function We of the illuminated emitter.
The photoelectric work function is the minimum photon energy required to liberate an electron from a substance, in the photoelectric effect. If the photon's energy is greater than the substance's work function, photoelectric emission occurs and the electron is liberated from the surface. Similar to the thermionic case described above, the liberated electrons can be extracted into a collector and produce a detectable current, if an electric field is applied into the surface of the emitter. Excess photon energy results in a liberated electron with non-zero kinetic energy. It is expected that the minimum photon energy {\displaystyle \hbar \omega } \hbar \omega  required to liberate an electron (and generate a current) is

{\displaystyle \hbar \omega =W_{\rm {e}}}\hbar \omega = W_{\rm e}
where We is the work function of the emitter.

Photoelectric measurements require a great deal of care, as an incorrectly designed experimental geometry can result in an erroneous measurement of work function.[8] This may be responsible for the large variation in work function values in scientific literature. Moreover, the minimum energy can be misleading in materials where there are no actual electron states at the Fermi level that are available for excitation. For example, in a semiconductor the minimum photon energy would actually correspond to the valence band edge rather than work function.[11]

Of course, the photoelectric effect may be used in the retarding mode, as with the thermionic apparatus described above. In the retarding case, the dark collector's work function is measured instead.

Kelvin probe method
See also: Volta potential, Kelvin probe force microscope, and Scanning Kelvin probe

Kelvin probe energy diagram at flat vacuum configuration, used for measuring work function difference between sample and probe.
The Kelvin probe technique relies on the detection of an electric field (gradient in Ï) between a sample material and probe material. The electric field can be varied by the voltage ÎVsp that is applied to the probe relative to the sample. If the voltage is chosen such that the electric field is eliminated (the flat vacuum condition), then

{\displaystyle e\Delta V_{\rm {sp}}=W_{\rm {s}}-W_{\rm {p}},\quad {\text{when}}~\phi ~{\text{is flat}}.}e\Delta V_{\rm sp} = W_{\rm s} - W_{\rm p}, \quad \text{when}~\phi~\text{is flat}.
Since the experimenter controls and knows ÎVsp, then finding the flat vacuum condition gives directly the work function difference between the two materials. The only question is, how to detect the flat vacuum condition? Typically, the electric field is detected by varying the distance between the sample and probe. When the distance is changed but ÎVsp is held constant, a current will flow due to the change in capacitance. This current is proportional to the vacuum electric field, and so when the electric field is neutralized no current will flow.

Although the Kelvin probe technique only measures a work function difference, it is possible to obtain an absolute work function by first calibrating the probe against a reference material (with known work function) and then using the same probe to measure a desired sample.[9] The Kelvin probe technique can be used to obtain work function maps of a surface with extremely high spatial resolution, by using a sharp tip for the probe (see Kelvin probe force microscope).

Work functions of elements
The work function depends on the configurations of atoms at the surface of the material. For example, on polycrystalline silver the work function is 4.26 eV, but on silver crystals it varies for different crystal faces as (100) face: 4.64 eV, (110) face: 4.52 eV, (111) face: 4.74 eV.[12] Ranges for typical surfaces are shown in the table below.[13]

Work function of elements (eV)
Ag	4.26 â 4.74	Al	4.06 â 4.26	As	3.75
Au	5.10 â 5.47	B	~4.45	Ba	2.52 â 2.70
Be	4.98	Bi	4.31	C	~5
Ca	2.87	Cd	4.08	Ce	2.9
Co	5	Cr	4.5	Cs	1.95
Cu	4.53 â 5.10	Eu	2.5	Fe:	4.67 â 4.81
Ga	4.32	Gd	2.90	Hf	3.90
Hg	4.475	In	4.09	Ir	5.00 â 5.67
K	2.29	La	3.5	Li	2.9
Lu	~3.3	Mg	3.66	Mn	4.1
Mo	4.36 â 4.95	Na	2.36	Nb	3.95 â 4.87
Nd	3.2	Ni	5.04 â 5.35	Os	5.93
Pb	4.25	Pd	5.22 â 5.60	Pt	5.12 â 5.93
Rb	2.261	Re	4.72	Rh	4.98
Ru	4.71	Sb	4.55 â 4.70	Sc	3.5
Se	5.9	Si	4.60 â 4.85	Sm	2.7
Sn	4.42	Sr	~2.59	Ta	4.00 â 4.80
Tb	3.00	Te	4.95	Th	3.4
Ti	4.33	Tl	~3.84	U	3.63 â 3.90
V	4.3	W	4.32 â 5.22	Y	3.1
Yb	2.60[14]	Zn	3.63 â 4.9	Zr	4.05
Physical factors that determine the work function
Due to the complications described in the modelling section below, it is difficult to theoretically predict the work function with accuracy. Various trends have, however, been identified. The work function tends to be smaller for metals with an open lattice,[clarification needed] and larger for metals in which the atoms are closely packed. It is somewhat higher on dense crystal faces than open crystal faces, also depending on surface reconstructions for the given crystal face.

Surface dipole
The work function is not simply dependent on the "internal vacuum level" inside the material (i.e., its average electrostatic potential), because of the formation of an atomic-scale electric double layer at the surface.[6] This surface electric dipole gives a jump in the electrostatic potential between the material and the vacuum.

A variety of factors are responsible for the surface electric dipole. Even with a completely clean surface, the electrons can spread slightly into the vacuum, leaving behind a slightly positively charged layer of material. This primarily occurs in metals, where the bound electrons do not encounter a hard wall potential at the surface but rather a gradual ramping potential due to image charge attraction. The amount of surface dipole depends on the detailed layout of the atoms at the surface of the material, leading to the variation in work function for different crystal faces.

Doping and electric field effect (semiconductors)

Band diagram of semiconductor-vacuum interface showing electron affinity EEA, defined as the difference between near-surface vacuum energy Evac, and near-surface conduction band edge EC. Also shown: Fermi level EF, valence band edge EV, work function W.
In a semiconductor, the work function is sensitive to the doping level at the surface of the semiconductor. Since the doping near the surface can also be controlled by electric fields, the work function of a semiconductor is also sensitive to the electric field in the vacuum.

The reason for the dependence is that, typically, the vacuum level and the conduction band edge retain a fixed spacing independent of doping. This spacing is called the electron affinity (note that this has a different meaning than the electron affinity of chemistry); in silicon for example the electron affinity is 4.05 eV.[15] If the electron affinity EEA and the surface's band-referenced Fermi level EF-EC are known, then the work function is given by

{\displaystyle W=E_{\rm {EA}}+E_{\rm {C}}-E_{\rm {F}}} W = E_{\rm EA} + E_{\rm C} - E_{\rm F}
where EC is taken at the surface.

From this one might expect that by doping the bulk of the semiconductor, the work function can be tuned. In reality, however, the energies of the bands near the surface are often pinned to the Fermi level, due to the influence of surface states.[16] If there is a large density of surface states, then the work function of the semiconductor will show a very weak dependence on doping or electric field.[17]

Theoretical models of metal work functions
Theoretical modeling of the work function is difficult, as an accurate model requires a careful treatment of both electronic many body effects and surface chemistry; both of these topics are already complex in their own right.

One of the earliest successful models for metal work function trends was the jellium model,[18] which allowed for oscillations in electronic density nearby the abrupt surface (these are similar to Friedel oscillations) as well as the tail of electron density extending outside the surface. This model showed why the density of conduction electrons (as represented by the WignerâSeitz radius rs) is an important parameter in determining work function.

The jellium model is only a partial explanation, as its predictions still show significant deviation from real work functions. More recent models have focused on including more accurate forms of electron exchange and correlation effects, as well as including the crystal face dependence (this requires the inclusion of the actual atomic lattice, something that is neglected in the jellium model).[6][19]

References
 Kittel, Charles. Introduction to Solid State Physics (7th ed.). Wiley.
 Herbert Kroemer, "Quasi-Electric Fields and Band Offsets: Teaching Electrons New Tricks" Nobel lecture
 "Barrier Height Correlations and Systematics". academic.brooklyn.cuny.edu. Retrieved 11 April 2018.
 Behunin, R. O.; Intravaia, F.; Dalvit, D. A. R.; Neto, P. A. M.; Reynaud, S. (2012). "Modeling electrostatic patch effects in Casimir force measurements". Physical Review A. 85 (1): 012504. arXiv:1108.1761. Bibcode:2012PhRvA..85a2504B. doi:10.1103/PhysRevA.85.012504.
 Will, C. M. (2011). "Finally, results from Gravity Probe B". Physics. 4 (43): 43. arXiv:1106.1198. Bibcode:2011PhyOJ...4...43W. doi:10.1103/Physics.4.43.
 "Metal surfaces 1a". venables.asu.edu. Retrieved 11 April 2018.
 Thomas Iii, S. W.; Vella, S. J.; Dickey, M. D.; Kaufman, G. K.; Whitesides, G. M. (2009). "Controlling the Kinetics of Contact Electrification with Patterned Surfaces". Journal of the American Chemical Society. 131 (25): 8746â8747. CiteSeerX 10.1.1.670.4392. doi:10.1021/ja902862b. PMID 19499916.
 Helander, M. G.; Greiner, M. T.; Wang, Z. B.; Lu, Z. H. (2010). "Pitfalls in measuring work function using photoelectron spectroscopy". Applied Surface Science. 256 (8): 2602. Bibcode:2010ApSS..256.2602H. doi:10.1016/j.apsusc.2009.11.002.
 FernÃ¡ndez Garrillo, P. A.; GrÃ©vin, B.; Chevalier, N.; Borowik, Å. (2018). "Calibrated work function mapping by Kelvin probe force microscopy". Review of Scientific Instruments. 89 (4): 043702. doi:10.1063/1.5007619. PMID 29716375.
 G.L. Kulcinski, "Thermionic Energy Conversion" [1]
 "Photoelectron Emission". www.virginia.edu. Retrieved 11 April 2018.
 Dweydari, A. W.; Mee, C. H. B. (1975). "Work function measurements on (100) and (110) surfaces of silver". Physica Status Solidi A. 27 (1): 223. Bibcode:1975PSSAR..27..223D. doi:10.1002/pssa.2210270126.
 CRC Handbook of Chemistry and Physics version 2008, p. 12â114.
 Nikolic, M. V.; Radic, S. M.; Minic, V.; Ristic, M. M. (February 1996). "The dependence of the work function of rare earth metals on their electron structure". Microelectronics Journal. 27 (1): 93â96. doi:10.1016/0026-2692(95)00097-6. ISSN 0026-2692.
 Virginia Semiconductor (June 2002). "The General Properties of Si, Ge, SiGe, SiO2 and Si3N4" (PDF). Retrieved 6 Jan 2019.
 "Semiconductor Free Surfaces". academic.brooklyn.cuny.edu. Retrieved 11 April 2018.
 Bardeen, J. (1947). "Surface States and Rectification at a Metal Semi-Conductor Contact". Physical Review. 71 (10): 717â727. Bibcode:1947PhRv...71..717B. doi:10.1103/PhysRev.71.717.
 Lang, N.; Kohn, W. (1971). "Theory of Metal Surfaces: Work Function". Physical Review B. 3 (4): 1215. Bibcode:1971PhRvB...3.1215L. doi:10.1103/PhysRevB.3.1215.
 Kiejna, A.; Wojciechowski, K.F. (1996). Metal Surface Electron Physics. Elsevier. ISBN 9780080536347.
Further reading
Ashcroft; Mermin (1976). Solid State Physics. Thomson Learning, Inc.
Goldstein, Newbury; et al. (2003). Scanning Electron Microscopy and X-Ray Microanalysis. New York: Springer.
For a quick reference to values of work function of the elements:

Michaelson, Herbert B. (1977). "The work function of the elements and its periodicity". J. Appl. Phys. 48 (11): 4729. Bibcode:1977JAP....48.4729M. doi:10.1063/1.323539.
External links
Work function of polymeric insulators (Table 2.1)
Work function of diamond and doped carbon
Work functions of common metals
Work functions of various metals for the photoelectric effect
Physics of free surfaces of semiconductors
*Some of the work functions listed on these sites do not agree!*

vte
Thermionic valves
Theoretical principles
Thermionic emissionWork functionHot cathodeSpace chargeControl gridSuppressor gridAnodeGlowing anodeGetter
Types
DiodeAudionTriodeAcorn tubeNuvistorTetrodeBeam tetrodePentodePentagrid (Hexode, Heptode, Octode)NonodeCathode ray tubeAdditronBackward-wave oscillatorBeam deflection tubeCharactronCompactronEidophorIconoscopeInductive output tubeKinescopeKlystronMagic eyeMagnetronMonoscopePhototubePhotomultiplierSelectron tubeStorage tubeSutton tubeTalaria projectorTraveling-wave tubeTrochotronVideo camera tubeWilliams tubeFleming valve
Numbering systems
RMARETMAMarconi-OsramMullardâPhilipsJISRussian
Examples
List of vacuum tubesList of tube sockets
Authority control Edit this at Wikidata
NDL: 01212250
Categories: Condensed matter physicsConcepts in physicsVacuumVacuum tubes
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Magyar
Ð ÑÑÑÐºÐ¸Ð¹
ä¸­æ
22 more
Edit links
This page was last edited on 25 October 2020, at 07:02 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Fermi level
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Not to be confused with Fermi energy.
The Fermi level of a solid-state body is the thermodynamic work required to add one electron to the body. It is a thermodynamic quantity usually denoted by Âµ or EF[1] for brevity. The Fermi level does not include the work required to remove the electron from wherever it came from. A precise understanding of the Fermi levelâhow it relates to electronic band structure in determining electronic properties, how it relates to the voltage and flow of charge in an electronic circuitâis essential to an understanding of solid-state physics.

In band structure theory, used in solid state physics to analyze the energy levels in a solid, the Fermi level can be considered to be a hypothetical energy level of an electron, such that at thermodynamic equilibrium this energy level would have a 50% probability of being occupied at any given time. The position of the Fermi level in relation to the band energy levels is a crucial factor in determining electrical properties. The Fermi level does not necessarily correspond to an actual energy level (in an insulator the Fermi level lies in the band gap), nor does it require the existence of a band structure. Nonetheless, the Fermi level is a precisely defined thermodynamic quantity, and differences in Fermi level can be measured simply with a voltmeter.


Contents
1	Voltage measurement
2	Band structure of solids
2.1	Local conduction band referencing, internal chemical potential and the parameter Î¶
3	Temperature out of equilibrium
4	Technicalities
4.1	Terminology problems
4.2	Fermi level referencing and the location of zero Fermi level
4.2.1	Why it is not advisable to use "the energy in vacuum" as a reference zero
4.3	Discrete charging effects in small systems
5	Footnotes and references
Voltage measurement

A voltmeter measures differences in Fermi level divided by electron charge.
Sometimes it is said that electric currents are driven by differences in electrostatic potential (Galvani potential), but this is not exactly true.[2] As a counterexample, multi-material devices such as pân junctions contain internal electrostatic potential differences at equilibrium, yet without any accompanying net current; if a voltmeter is attached to the junction, one simply measures zero volts.[3] Clearly, the electrostatic potential is not the only factor influencing the flow of charge in a materialâPauli repulsion, carrier concentration gradients, electromagnetic induction, and thermal effects also play an important role.

In fact, the quantity called voltage as measured in an electronic circuit has a simple relationship to the chemical potential for electrons (Fermi level). When the leads of a voltmeter are attached to two points in a circuit, the displayed voltage is a measure of the total work transferred when a unit charge is allowed to move from one point to the other. If a simple wire is connected between two points of differing voltage (forming a short circuit), current will flow from positive to negative voltage, converting the available work into heat.

The Fermi level of a body expresses the work required to add an electron to it, or equally the work obtained by removing an electron. Therefore, VA â VB, the observed difference in voltage between two points, A and B, in an electronic circuit is exactly related to the corresponding chemical potential difference, ÂµA â ÂµB, in Fermi level by the formula[4]

{\displaystyle V_{\mathrm {A} }-V_{\mathrm {B} }={\frac {\mu _{\mathrm {A} }-\mu _{\mathrm {B} }}{-e}}}{\displaystyle V_{\mathrm {A} }-V_{\mathrm {B} }={\frac {\mu _{\mathrm {A} }-\mu _{\mathrm {B} }}{-e}}}
where âe is the electron charge.

From the above discussion it can be seen that electrons will move from a body of high Âµ (low voltage) to low Âµ (high voltage) if a simple path is provided. This flow of electrons will cause the lower Âµ to increase (due to charging or other repulsion effects) and likewise cause the higher Âµ to decrease. Eventually, Âµ will settle down to the same value in both bodies. This leads to an important fact regarding the equilibrium (off) state of an electronic circuit:

An electronic circuit in thermodynamic equilibrium will have a constant Fermi level throughout its connected parts.[according to whom?]
This also means that the voltage (measured with a voltmeter) between any two points will be zero, at equilibrium. Note that thermodynamic equilibrium here requires that the circuit be internally connected and not contain any batteries or other power sources, nor any variations in temperature.

Band structure of solids

Filling of the electronic states in various types of materials at equilibrium. Here, height is energy while width is the density of available states for a certain energy in the material listed. The shade follows the FermiâDirac distribution (black = all states filled, white = no state filled). In metals and semimetals the Fermi level EF lies inside at least one band. In insulators and semiconductors the Fermi level is inside a band gap; however, in semiconductors the bands are near enough to the Fermi level to be thermally populated with electrons or holes.edit

Fermi-Dirac distribution {\displaystyle f(\epsilon )\ }{\displaystyle f(\epsilon )\ } vs. energy {\displaystyle \epsilon \ }\epsilon \ , with Î¼ = 0.55 eV and for various temperatures in the range 50K â¤ T â¤ 375K.
In the band theory of solids, electrons are considered to occupy a series of bands composed of single-particle energy eigenstates each labelled by Ïµ. Although this single particle picture is an approximation, it greatly simplifies the understanding of electronic behaviour and it generally provides correct results when applied correctly.

The FermiâDirac distribution, {\displaystyle f(\epsilon )}f(\epsilon ), gives the probability that (at thermodynamic equilibrium) a state having energy Ïµ is occupied by an electron:[5]

{\displaystyle f(\epsilon )={\frac {1}{e^{(\epsilon -\mu )/kT}+1}}}{\displaystyle f(\epsilon )={\frac {1}{e^{(\epsilon -\mu )/kT}+1}}}
Here, T is the absolute temperature and k is Boltzmann's constant. If there is a state at the Fermi level (Ïµ = Âµ), then this state will have a 50% chance of being occupied. The distribution is plotted in the left figure. The closer f is to 1, the higher chance this state is occupied. The closer f is to 0, the higher chance this state is empty.

The location of Âµ within a material's band structure is important in determining the electrical behaviour of the material.

In an insulator, Âµ lies within a large band gap, far away from any states that are able to carry current.
In a metal, semimetal or degenerate semiconductor, Âµ lies within a delocalized band. A large number of states nearby Âµ are thermally active and readily carry current.
In an intrinsic or lightly doped semiconductor, Âµ is close enough to a band edge that there are a dilute number of thermally excited carriers residing near that band edge.
In semiconductors and semimetals the position of Âµ relative to the band structure can usually be controlled to a significant degree by doping or gating. These controls do not change Âµ which is fixed by the electrodes, but rather they cause the entire band structure to shift up and down (sometimes also changing the band structure's shape). For further information about the Fermi levels of semiconductors, see (for example) Sze.[6]

Local conduction band referencing, internal chemical potential and the parameter Î¶
If the symbol â° is used to denote an electron energy level measured relative to the energy of the edge of its enclosing band, ÏµC, then in general we have â° = Ïµ â ÏµC. We can define a parameter Î¶[7] that references the Fermi level with respect to the band edge:

{\displaystyle \zeta =\mu -\epsilon _{\rm {C}}.}\zeta =\mu -\epsilon _{\rm {C}}.
It follows that the FermiâDirac distribution function can be written as

{\displaystyle f({\mathcal {E}})={\frac {1}{e^{({\mathcal {E}}-\zeta )/kT}+1}}.}{\displaystyle f({\mathcal {E}})={\frac {1}{e^{({\mathcal {E}}-\zeta )/kT}+1}}.}
The band theory of metals was initially developed by Sommerfeld, from 1927 onwards, who paid great attention to the underlying thermodynamics and statistical mechanics. Confusingly, in some contexts the band-referenced quantity Î¶ may be called the Fermi level, chemical potential, or electrochemical potential, leading to ambiguity with the globally-referenced Fermi level. In this article, the terms conduction-band referenced Fermi level or internal chemical potential are used to refer to Î¶.


Example of variations in conduction band edge EC in a band diagram of GaAs/AlGaAs heterojunction-based high-electron-mobility transistor.
Î¶ is directly related to the number of active charge carriers as well as their typical kinetic energy, and hence it is directly involved in determining the local properties of the material (such as electrical conductivity). For this reason it is common to focus on the value of Î¶ when concentrating on the properties of electrons in a single, homogeneous conductive material. By analogy to the energy states of a free electron, the â° of a state is the kinetic energy of that state and ÏµC is its potential energy. With this in mind, the parameter, Î¶, could also be labelled the Fermi kinetic energy.

Unlike Âµ, the parameter, Î¶, is not a constant at equilibrium, but rather varies from location to location in a material due to variations in ÏµC, which is determined by factors such as material quality and impurities/dopants. Near the surface of a semiconductor or semimetal, Î¶ can be strongly controlled by externally applied electric fields, as is done in a field effect transistor. In a multi-band material, Î¶ may even take on multiple values in a single location. For example, in a piece of aluminum metal there are two conduction bands crossing the Fermi level (even more bands in other materials);[8] each band has a different edge energy, ÏµC, and a different Î¶.

The value of Î¶ at zero temperature is widely known as the Fermi energy, sometimes written Î¶0. Confusingly (again), the name Fermi energy sometimes is used to refer to Î¶ at non-zero temperature.

Temperature out of equilibrium
See also: Quasi-Fermi level
The Fermi level, Î¼, and temperature, T, are well defined constants for a solid-state device in thermodynamic equilibrium situation, such as when it is sitting on the shelf doing nothing. When the device is brought out of equilibrium and put into use, then strictly speaking the Fermi level and temperature are no longer well defined. Fortunately, it is often possible to define a quasi-Fermi level and quasi-temperature for a given location, that accurately describe the occupation of states in terms of a thermal distribution. The device is said to be in quasi-equilibrium when and where such a description is possible.

The quasi-equilibrium approach allows one to build a simple picture of some non-equilibrium effects as the electrical conductivity of a piece of metal (as resulting from a gradient of Î¼) or its thermal conductivity (as resulting from a gradient in T). The quasi-Î¼ and quasi-T can vary (or not exist at all) in any non-equilibrium situation, such as:

If the system contains a chemical imbalance (as in a battery).
If the system is exposed to changing electromagnetic fields (as in capacitors, inductors, and transformers).
Under illumination from a light-source with a different temperature, such as the sun (as in solar cells),
When the temperature is not constant within the device (as in thermocouples),
When the device has been altered, but has not had enough time to re-equilibrate (as in piezoelectric or pyroelectric substances).
In some situations, such as immediately after a material experiences a high-energy laser pulse, the electron distribution cannot be described by any thermal distribution. One cannot define the quasi-Fermi level or quasi-temperature in this case; the electrons are simply said to be non-thermalized. In less dramatic situations, such as in a solar cell under constant illumination, a quasi-equilibrium description may be possible but requiring the assignment of distinct values of Î¼ and T to different bands (conduction band vs. valence band). Even then, the values of Î¼ and T may jump discontinuously across a material interface (e.g., pân junction) when a current is being driven, and be ill-defined at the interface itself.

Technicalities
Terminology problems
The term Fermi level is mainly used in discussing the solid state physics of electrons in semiconductors, and a precise usage of this term is necessary to describe band diagrams in devices comprising different materials with different levels of doping. In these contexts, however, one may also see Fermi level used imprecisely to refer to the band-referenced Fermi level, Âµ â ÏµC, called Î¶ above. It is common to see scientists and engineers refer to "controlling", "pinning", or "tuning" the Fermi level inside a conductor, when they are in fact describing changes in ÏµC due to doping or the field effect. In fact, thermodynamic equilibrium guarantees that the Fermi level in a conductor is always fixed to be exactly equal to the Fermi level of the electrodes; only the band structure (not the Fermi level) can be changed by doping or the field effect (see also band diagram). A similar ambiguity exists between the terms, chemical potential and electrochemical potential.

It is also important to note that Fermi level is not necessarily the same thing as Fermi energy. In the wider context of quantum mechanics, the term Fermi energy usually refers to the maximum kinetic energy of a fermion in an idealized non-interacting, disorder free, zero temperature Fermi gas. This concept is very theoretical (there is no such thing as a non-interacting Fermi gas, and zero temperature is impossible to achieve). However, it finds some use in approximately describing white dwarfs, neutron stars, atomic nuclei, and electrons in a metal. On the other hand, in the fields of semiconductor physics and engineering, Fermi energy often is used to refer to the Fermi level described in this article.[9]

Fermi level referencing and the location of zero Fermi level
Much like the choice of origin in a coordinate system, the zero point of energy can be defined arbitrarily. Observable phenomena only depend on energy differences. When comparing distinct bodies, however, it is important that they all be consistent in their choice of the location of zero energy, or else nonsensical results will be obtained. It can therefore be helpful to explicitly name a common point to ensure that different components are in agreement. On the other hand, if a reference point is inherently ambiguous (such as "the vacuum", see below) it will instead cause more problems.

A practical and well-justified choice of common point is a bulky, physical conductor, such as the electrical ground or earth. Such a conductor can be considered to be in a good thermodynamic equilibrium and so its Âµ is well defined. It provides a reservoir of charge, so that large numbers of electrons may be added or removed without incurring charging effects. It also has the advantage of being accessible, so that the Fermi level of any other object can be measured simply with a voltmeter.

Why it is not advisable to use "the energy in vacuum" as a reference zero

When the two metals depicted here are in thermodynamic equilibrium as shown (equal Fermi levels EF), the vacuum electrostatic potential Ï is not flat due to a difference in work function.
In principle, one might consider using the state of a stationary electron in the vacuum as a reference point for energies. This approach is not advisable unless one is careful to define exactly where the vacuum is.[10] The problem is that not all points in the vacuum are equivalent.

At thermodynamic equilibrium, it is typical for electrical potential differences of order 1 V to exist in the vacuum (Volta potentials). The source of this vacuum potential variation is the variation in work function between the different conducting materials exposed to vacuum. Just outside a conductor, the electrostatic potential depends sensitively on the material, as well as which surface is selected (its crystal orientation, contamination, and other details).

The parameter that gives the best approximation to universality is the Earth-referenced Fermi level suggested above. This also has the advantage that it can be measured with a voltmeter.

Discrete charging effects in small systems
In cases where the "charging effects" due to a single electron are non-negligible, the above definitions should be clarified. For example, consider a capacitor made of two identical parallel-plates. If the capacitor is uncharged, the Fermi level is the same on both sides, so one might think that it should take no energy to move an electron from one plate to the other. But when the electron has been moved, the capacitor has become (slightly) charged, so this does take a slight amount of energy. In a normal capacitor, this is negligible, but in a nano-scale capacitor it can be more important.

In this case one must be precise about the thermodynamic definition of the chemical potential as well as the state of the device: is it electrically isolated, or is it connected to an electrode?

When the body is able to exchange electrons and energy with an electrode (reservoir), it is described by the grand canonical ensemble. The value of chemical potential Âµ can be said to be fixed by the electrode, and the number of electrons N on the body may fluctuate. In this case, the chemical potential of a body is the infinitesimal amount of work needed to increase the average number of electrons by an infinitesimal amount (even though the number of electrons at any time is an integer, the average number varies continuously.):
{\displaystyle \mu (\left\langle N\right\rangle ,T)=\left({\frac {\partial F}{\partial \left\langle N\right\rangle }}\right)_{T},}{\displaystyle \mu (\left\langle N\right\rangle ,T)=\left({\frac {\partial F}{\partial \left\langle N\right\rangle }}\right)_{T},}
where F(N, T) is the free energy function of the grand canonical ensemble.
If the number of electrons in the body is fixed (but the body is still thermally connected to a heat bath), then it is in the canonical ensemble. We can define a "chemical potential" in this case literally as the work required to add one electron to a body that already has exactly N electrons,[11]
{\displaystyle \mu '(N,T)=F(N+1,T)-F(N,T),}{\displaystyle \mu '(N,T)=F(N+1,T)-F(N,T),}
where F(N, T) is the free energy function of the canonical ensemble, alternatively,
{\displaystyle \mu ''(N,T)=F(N,T)-F(N-1,T)=\mu '(N-1,T).}{\displaystyle \mu ''(N,T)=F(N,T)-F(N-1,T)=\mu '(N-1,T).}
These chemical potentials are not equivalent, Âµ â  Âµ' â  Âµ'', except in the thermodynamic limit. The distinction is important in small systems such as those showing Coulomb blockade.[12] The parameter, Âµ, (i.e., in the case where the number of electrons is allowed to fluctuate) remains exactly related to the voltmeter voltage, even in small systems. To be precise, then, the Fermi level is defined not by a deterministic charging event by one electron charge, but rather a statistical charging event by an infinitesimal fraction of an electron.

Footnotes and references
 Kittel, Charles. Introduction to Solid State Physics, 7th Edition. Wiley.
 Riess, I (1997). "What does a voltmeter measure?". Solid State Ionics. 95 (3â4): 327â328. doi:10.1016/S0167-2738(96)00542-5.
 Sah, Chih-Tang (1991). Fundamentals of Solid-State Electronics. World Scientific. p. 404. ISBN 978-9810206376.
 Datta, Supriyo (2005). Quantum Transport: Atom to Transistor. Cambridge University Press. p. 7. ISBN 9780521631457.
 Kittel, Charles; Herbert Kroemer (1980-01-15). Thermal Physics (2nd Edition). W. H. Freeman. p. 357. ISBN 978-0-7167-1088-2.
 Sze, S. M. (1964). Physics of Semiconductor Devices. Wiley. ISBN 978-0-471-05661-4.
 Sommerfeld, Arnold (1964). Thermodynamics and Statistical Mechanics. Academic Press.
 "3D Fermi Surface Site". Phys.ufl.edu. 1998-05-27. Retrieved 2013-04-22.
 For example: D. Chattopadhyay (2006). Electronics (fundamentals And Applications). ISBN 978-81-224-1780-7. and Balkanski and Wallis (2000-09-01). Semiconductor Physics and Applications. ISBN 978-0-19-851740-5.
 Technically, it is possible to consider the vacuum to be an insulator and in fact its Fermi level is defined if its surroundings are in equilibrium. Typically however the Fermi level is two to five electron volts below the vacuum electrostatic potential energy, depending on the work function of the nearby vacuum wall material. Only at high temperatures will the equilibrium vacuum be populated with a significant number of electrons (this is the basis of thermionic emission).
 Shegelski, Mark R. A. (May 2004). "The chemical potential of an ideal intrinsic semiconductor". American Journal of Physics. 72 (5): 676â678. Bibcode:2004AmJPh..72..676S. doi:10.1119/1.1629090. Archived from the original on 2013-07-03.
 Beenakker, C. W. J. (1991). "Theory of Coulomb-blockade oscillations in the conductance of a quantum dot" (PDF). Physical Review B. 44 (4): 1646â1656. Bibcode:1991PhRvB..44.1646B. doi:10.1103/PhysRevB.44.1646. hdl:1887/3358. PMID 9999698.
Categories: Electronic band structuresFermiâDirac statistics
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
FranÃ§ais
íêµ­ì´
Bahasa Indonesia
æ¥æ¬èª
PortuguÃªs
Tiáº¿ng Viá»t
ä¸­æ
7 more
Edit links
This page was last edited on 3 July 2020, at 15:17 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Page semi-protected
Electric battery
From Wikipedia, the free encyclopedia
  (Redirected from Battery (electricity))
Jump to navigationJump to search
For other uses, see Battery (disambiguation).
Battery
Batteries.jpg
Various cells and batteries (top left to bottom right): two AA, one D, one handheld ham radio battery, two 9-volt (PP3), two AAA, one C, one camcorder battery, one cordless phone battery
Type	Power source
Working principle	Electrochemical reactions, Electromotive force
First production	1800s
Electronic symbol
Battery symbol2.svg
The symbol for a battery in a circuit diagram. It originated as a schematic drawing of the earliest type of battery, a voltaic pile.
A battery is a device consisting of one or more electrochemical cells with external connections[1] for powering electrical devices such as flashlights, mobile phones, and electric cars. When a battery is supplying electric power, its positive terminal is the cathode and its negative terminal is the anode.[2] The terminal marked negative is the source of electrons that will flow through an external electric circuit to the positive terminal. When a battery is connected to an external electric load, a redox reaction converts high-energy reactants to lower-energy products, and the free-energy difference is delivered to the external circuit as electrical energy.[3] Historically the term "battery" specifically referred to a device composed of multiple cells, however the usage has evolved to include devices composed of a single cell.[4]

Primary (single-use or "disposable") batteries are used once and discarded, as the electrode materials are irreversibly changed during discharge; a common example is the alkaline battery used for flashlights and a multitude of portable electronic devices. Secondary (rechargeable) batteries can be discharged and recharged multiple times using an applied electric current; the original composition of the electrodes can be restored by reverse current. Examples include the lead-acid batteries used in vehicles and lithium-ion batteries used for portable electronics such as laptops and mobile phones.

Batteries come in many shapes and sizes, from miniature cells used to power hearing aids and wristwatches to small, thin cells used in smartphones, to large lead acid batteries or lithium-ion batteries in vehicles, and at the largest extreme, huge battery banks the size of rooms that provide standby or emergency power for telephone exchanges and computer data centers.

Batteries have much lower specific energy (energy per unit mass) than common fuels such as gasoline. In automobiles, this is somewhat offset by the higher efficiency of electric motors in converting chemical energy to mechanical work, compared to combustion engines.


Contents
1	History
2	Principle of operation
3	Categories and types of batteries
3.1	Primary
3.2	Secondary
3.3	Cell types
3.4	Cell performance
4	Capacity and discharge
4.1	C rate
4.2	Fast-charging, large and light batteries
5	Lifetime
5.1	Self-discharge
5.2	Corrosion
5.3	Physical component changes
5.4	Charge/discharge speed
5.5	Overcharging
5.6	Memory effect
5.7	Environmental conditions
5.8	Storage
6	Battery sizes
7	Hazards
7.1	Explosion
7.2	Leakage
7.3	Toxic materials
7.4	Ingestion
8	Chemistry
8.1	Primary batteries and their characteristics
8.2	Secondary (rechargeable) batteries and their characteristics
9	Solid-state batteries
10	Homemade cells
11	See also
12	References
13	Further reading
14	External links
History
Main article: History of the battery

A voltaic pile, the first battery

Italian physicist Alessandro Volta demonstrating his pile to French emperor Napoleon Bonaparte
The usage of "battery" to describe a group of electrical devices dates to Benjamin Franklin, who in 1748 described multiple Leyden jars by analogy to a battery of cannon[5] (Benjamin Franklin borrowed the term "battery" from the military, which refers to weapons functioning together[6]).

Italian physicist Alessandro Volta built and described the first electrochemical battery, the voltaic pile, in 1800.[7] This was a stack of copper and zinc plates, separated by brine-soaked paper disks, that could produce a steady current for a considerable length of time. Volta did not understand that the voltage was due to chemical reactions. He thought that his cells were an inexhaustible source of energy,[8] and that the associated corrosion effects at the electrodes were a mere nuisance, rather than an unavoidable consequence of their operation, as Michael Faraday showed in 1834.[9]

Although early batteries were of great value for experimental purposes, in practice their voltages fluctuated and they could not provide a large current for a sustained period. The Daniell cell, invented in 1836 by British chemist John Frederic Daniell, was the first practical source of electricity, becoming an industry standard and seeing widespread adoption as a power source for electrical telegraph networks.[10] It consisted of a copper pot filled with a copper sulfate solution, in which was immersed an unglazed earthenware container filled with sulfuric acid and a zinc electrode.[11]

These wet cells used liquid electrolytes, which were prone to leakage and spillage if not handled correctly. Many used glass jars to hold their components, which made them fragile and potentially dangerous. These characteristics made wet cells unsuitable for portable appliances. Near the end of the nineteenth century, the invention of dry cell batteries, which replaced the liquid electrolyte with a paste, made portable electrical devices practical.[12]

Principle of operation
Main article: Electrochemical cell

A voltaic cell for demonstration purposes. In this example the two half-cells are linked by a salt bridge that permits the transfer of ions.
Batteries convert chemical energy directly to electrical energy. In many cases, the electrical energy released is the difference in the cohesive[13] or bond energies of the metals, oxides, or molecules undergoing the electrochemical reaction.[3] For instance, energy can be stored in Zn or Li, which are high-energy metals because they are not stabilized by d-electron bonding, unlike transition metals. Batteries are designed such that the energetically favorable redox reaction can occur only if electrons move through the external part of the circuit.

A battery consists of some number of voltaic cells. Each cell consists of two half-cells connected in series by a conductive electrolyte containing metal cations. One half-cell includes electrolyte and the negative electrode, the electrode to which anions (negatively charged ions) migrate; the other half-cell includes electrolyte and the positive electrode, to which cations (positively charged ions) migrate. Cations are reduced (electrons are added) at the cathode, while metal atoms are oxidized (electrons are removed) at the anode.[14] Some cells use different electrolytes for each half-cell; then a separator is used to prevent mixing of the electrolytes while allowing ions to flow between half-cells to complete the electrical circuit.

Each half-cell has an electromotive force (emf, measured in volts) relative to a standard. The net emf of the cell is the difference between the emfs of its half-cells.[15] Thus, if the electrodes have emfs {\displaystyle {\mathcal {E}}_{1}}\mathcal{E}_1 and {\displaystyle {\mathcal {E}}_{2}}\mathcal{E}_2, then the net emf is {\displaystyle {\mathcal {E}}_{2}-{\mathcal {E}}_{1}}\mathcal{E}_{2}-\mathcal{E}_{1}; in other words, the net emf is the difference between the reduction potentials of the half-reactions.[16]

The electrical driving force or {\displaystyle \displaystyle {\Delta V_{bat}}}\displaystyle{\Delta V_{bat}} across the terminals of a cell is known as the terminal voltage (difference) and is measured in volts.[17] The terminal voltage of a cell that is neither charging nor discharging is called the open-circuit voltage and equals the emf of the cell. Because of internal resistance,[18] the terminal voltage of a cell that is discharging is smaller in magnitude than the open-circuit voltage and the terminal voltage of a cell that is charging exceeds the open-circuit voltage.[19] An ideal cell has negligible internal resistance, so it would maintain a constant terminal voltage of {\displaystyle {\mathcal {E}}}{\mathcal {E}} until exhausted, then dropping to zero. If such a cell maintained 1.5 volts and produce a charge of one coulomb then on complete discharge it would have performed 1.5 joules of work.[17] In actual cells, the internal resistance increases under discharge[18] and the open-circuit voltage also decreases under discharge. If the voltage and resistance are plotted against time, the resulting graphs typically are a curve; the shape of the curve varies according to the chemistry and internal arrangement employed.

The voltage developed across a cell's terminals depends on the energy release of the chemical reactions of its electrodes and electrolyte. Alkaline and zincâcarbon cells have different chemistries, but approximately the same emf of 1.5 volts; likewise NiCd and NiMH cells have different chemistries, but approximately the same emf of 1.2 volts.[20] The high electrochemical potential changes in the reactions of lithium compounds give lithium cells emfs of 3 volts or more.[21]

Categories and types of batteries
Main article: List of battery types

From top to bottom: a large 4.5-volt 3R12 battery, a D Cell, a C cell, an AA cell, an AAA cell, an AAAA cell, an A23 battery, a 9-volt PP3 battery, and a pair of button cells (CR2032 and LR44)
Batteries are classified into primary and secondary forms:

Primary batteries are designed to be used until exhausted of energy then discarded. Their chemical reactions are generally not reversible, so they cannot be recharged. When the supply of reactants in the battery is exhausted, the battery stops producing current and is useless.[22]
Secondary batteries can be recharged; that is, they can have their chemical reactions reversed by applying electric current to the cell. This regenerates the original chemical reactants, so they can be used, recharged, and used again multiple times.[23]
Some types of primary batteries used, for example, for telegraph circuits, were restored to operation by replacing the electrodes.[24] Secondary batteries are not indefinitely rechargeable due to dissipation of the active materials, loss of electrolyte and internal corrosion.

Primary
Main article: Primary cell
Primary batteries, or primary cells, can produce current immediately on assembly. These are most commonly used in portable devices that have low current drain, are used only intermittently, or are used well away from an alternative power source, such as in alarm and communication circuits where other electric power is only intermittently available. Disposable primary cells cannot be reliably recharged, since the chemical reactions are not easily reversible and active materials may not return to their original forms. Battery manufacturers recommend against attempting to recharge primary cells.[25] In general, these have higher energy densities than rechargeable batteries,[26] but disposable batteries do not fare well under high-drain applications with loads under 75 ohms (75 Î©). Common types of disposable batteries include zincâcarbon batteries and alkaline batteries.

Secondary
Main article: Rechargeable battery
Secondary batteries, also known as secondary cells, or rechargeable batteries, must be charged before first use; they are usually assembled with active materials in the discharged state. Rechargeable batteries are (re)charged by applying electric current, which reverses the chemical reactions that occur during discharge/use. Devices to supply the appropriate current are called chargers.

The oldest form of rechargeable battery is the leadâacid battery, which are widely used in automotive and boating applications. This technology contains liquid electrolyte in an unsealed container, requiring that the battery be kept upright and the area be well ventilated to ensure safe dispersal of the hydrogen gas it produces during overcharging. The leadâacid battery is relatively heavy for the amount of electrical energy it can supply. Its low manufacturing cost and its high surge current levels make it common where its capacity (over approximately 10 Ah) is more important than weight and handling issues. A common application is the modern car battery, which can, in general, deliver a peak current of 450 amperes.

The sealed valve regulated leadâacid battery (VRLA battery) is popular in the automotive industry as a replacement for the leadâacid wet cell. The VRLA battery uses an immobilized sulfuric acid electrolyte, reducing the chance of leakage and extending shelf life.[27] VRLA batteries immobilize the electrolyte. The two types are:

Gel batteries (or "gel cell") use a semi-solid electrolyte.
Absorbed Glass Mat (AGM) batteries absorb the electrolyte in a special fiberglass matting.
Other portable rechargeable batteries include several sealed "dry cell" types, that are useful in applications such as mobile phones and laptop computers. Cells of this type (in order of increasing power density and cost) include nickelâcadmium (NiCd), nickelâzinc (NiZn), nickel metal hydride (NiMH), and lithium-ion (Li-ion) cells. Li-ion has by far the highest share of the dry cell rechargeable market. NiMH has replaced NiCd in most applications due to its higher capacity, but NiCd remains in use in power tools, two-way radios, and medical equipment.

In the 2000s, developments include batteries with embedded electronics such as USBCELL, which allows charging an AA battery through a USB connector,[28] nanoball batteries that allow for a discharge rate about 100x greater than current batteries, and smart battery packs with state-of-charge monitors and battery protection circuits that prevent damage on over-discharge. Low self-discharge (LSD) allows secondary cells to be charged prior to shipping.

Cell types
Many types of electrochemical cells have been produced, with varying chemical processes and designs, including galvanic cells, electrolytic cells, fuel cells, flow cells and voltaic piles.[29]

Wet cell
A wet cell battery has a liquid electrolyte. Other names are flooded cell, since the liquid covers all internal parts or vented cell, since gases produced during operation can escape to the air. Wet cells were a precursor to dry cells and are commonly used as a learning tool for electrochemistry. They can be built with common laboratory supplies, such as beakers, for demonstrations of how electrochemical cells work. A particular type of wet cell known as a concentration cell is important in understanding corrosion. Wet cells may be primary cells (non-rechargeable) or secondary cells (rechargeable). Originally, all practical primary batteries such as the Daniell cell were built as open-top glass jar wet cells. Other primary wet cells are the Leclanche cell, Grove cell, Bunsen cell, Chromic acid cell, Clark cell, and Weston cell. The Leclanche cell chemistry was adapted to the first dry cells. Wet cells are still used in automobile batteries and in industry for standby power for switchgear, telecommunication or large uninterruptible power supplies, but in many places batteries with gel cells have been used instead. These applications commonly use leadâacid or nickelâcadmium cells.

Dry cell
Further information: Dry cell

Line art drawing of a dry cell:
1. brass cap, 2. plastic seal, 3. expansion space, 4. porous cardboard, 5. zinc can, 6. carbon rod, 7. chemical mixture
A dry cell uses a paste electrolyte, with only enough moisture to allow current to flow. Unlike a wet cell, a dry cell can operate in any orientation without spilling, as it contains no free liquid, making it suitable for portable equipment. By comparison, the first wet cells were typically fragile glass containers with lead rods hanging from the open top and needed careful handling to avoid spillage. Leadâacid batteries did not achieve the safety and portability of the dry cell until the development of the gel battery.

A common dry cell is the zincâcarbon battery, sometimes called the dry LeclanchÃ© cell, with a nominal voltage of 1.5 volts, the same as the alkaline battery (since both use the same zincâmanganese dioxide combination). A standard dry cell comprises a zinc anode, usually in the form of a cylindrical pot, with a carbon cathode in the form of a central rod. The electrolyte is ammonium chloride in the form of a paste next to the zinc anode. The remaining space between the electrolyte and carbon cathode is taken up by a second paste consisting of ammonium chloride and manganese dioxide, the latter acting as a depolariser. In some designs, the ammonium chloride is replaced by zinc chloride.

Molten salt
Molten salt batteries are primary or secondary batteries that use a molten salt as electrolyte. They operate at high temperatures and must be well insulated to retain heat.

Reserve
A reserve battery can be stored unassembled (unactivated and supplying no power) for a long period (perhaps years). When the battery is needed, then it is assembled (e.g., by adding electrolyte); once assembled, the battery is charged and ready to work. For example, a battery for an electronic artillery fuze might be activated by the impact of firing a gun. The acceleration breaks a capsule of electrolyte that activates the battery and powers the fuze's circuits. Reserve batteries are usually designed for a short service life (seconds or minutes) after long storage (years). A water-activated battery for oceanographic instruments or military applications becomes activated on immersion in water.

Cell performance
A battery's characteristics may vary over load cycle, over charge cycle, and over lifetime due to many factors including internal chemistry, current drain, and temperature. At low temperatures, a battery cannot deliver as much power. As such, in cold climates, some car owners install battery warmers, which are small electric heating pads that keep the car battery warm.

Capacity and discharge

A device to check battery voltage
A battery's capacity is the amount of electric charge it can deliver at the rated voltage. The more electrode material contained in the cell the greater its capacity. A small cell has less capacity than a larger cell with the same chemistry, although they develop the same open-circuit voltage.[30] Capacity is measured in units such as amp-hour (AÂ·h). The rated capacity of a battery is usually expressed as the product of 20 hours multiplied by the current that a new battery can consistently supply for 20 hours at 68 Â°F (20 Â°C), while remaining above a specified terminal voltage per cell. For example, a battery rated at 100 AÂ·h can deliver 5 A over a 20-hour period at room temperature. The fraction of the stored charge that a battery can deliver depends on multiple factors, including battery chemistry, the rate at which the charge is delivered (current), the required terminal voltage, the storage period, ambient temperature and other factors.[30]

The higher the discharge rate, the lower the capacity.[31] The relationship between current, discharge time and capacity for a lead acid battery is approximated (over a typical range of current values) by Peukert's law:

{\displaystyle t={\frac {Q_{P}}{I^{k}}}}t = \frac {Q_P} {I^k}
where

{\displaystyle Q_{P}}Q_P is the capacity when discharged at a rate of 1 amp.
{\displaystyle I}I is the current drawn from battery (A).
{\displaystyle t}t is the amount of time (in hours) that a battery can sustain.
{\displaystyle k}k is a constant around 1.3.
Batteries that are stored for a long period or that are discharged at a small fraction of the capacity lose capacity due to the presence of generally irreversible side reactions that consume charge carriers without producing current. This phenomenon is known as internal self-discharge. Further, when batteries are recharged, additional side reactions can occur, reducing capacity for subsequent discharges. After enough recharges, in essence all capacity is lost and the battery stops producing power.

Internal energy losses and limitations on the rate that ions pass through the electrolyte cause battery efficiency to vary. Above a minimum threshold, discharging at a low rate delivers more of the battery's capacity than at a higher rate. Installing batteries with varying AÂ·h ratings does not affect device operation (although it may affect the operation interval) rated for a specific voltage unless load limits are exceeded. High-drain loads such as digital cameras can reduce total capacity, as happens with alkaline batteries. For example, a battery rated at 2 AÂ·h for a 10- or 20-hour discharge would not sustain a current of 1 A for a full two hours as its stated capacity implies.

C rate
See also: Battery charger Â§ C-rate
The C-rate is a measure of the rate at which a battery is being charged or discharged. It is defined as the current through the battery divided by the theoretical current draw under which the battery would deliver its nominal rated capacity in one hour.[32] It has the units hâ1.

C-rate is used as a rating on batteries to indicate the maximum current that a battery can safely deliver on a circuit. Standards for rechargeable batteries generally rate the capacity over a 4-hour, 8 hour or longer discharge time. Types intended for special purposes, such as in a computer uninterruptible power supply, may be rated by manufacturers for discharge periods much less than one hour. Because of internal resistance loss and the chemical processes inside the cells, a battery rarely delivers nameplate rated capacity in only one hour.

Fast-charging, large and light batteries
As of 2012, lithium iron phosphate (LiFePO
4) battery technology was the fastest-charging/discharging, fully discharging in 10â20 seconds.[33]

As of 2017, the world's largest battery was built in South Australia by Tesla. It can store 129 MWh.[34] A battery in Hebei Province, China which can store 36 MWh of electricity was built in 2013 at a cost of $500 million.[35] Another large battery, composed of NiâCd cells, was in Fairbanks, Alaska. It covered 2,000 square metres (22,000 sq ft)âbigger than a football pitchâand weighed 1,300 tonnes. It was manufactured by ABB to provide backup power in the event of a blackout. The battery can provide 40 MW of power for up to seven minutes.[36] Sodiumâsulfur batteries have been used to store wind power.[37] A 4.4 MWh battery system that can deliver 11 MW for 25 minutes stabilizes the output of the Auwahi wind farm in Hawaii.[38]

Lithiumâsulfur batteries were used on the longest and highest solar-powered flight.[39]

Lifetime
Battery life (and its synonym battery lifetime) has two meanings for rechargeable batteries but only one for non-chargeables. For rechargeables, it can mean either the length of time a device can run on a fully charged battery or the number of charge/discharge cycles possible before the cells fail to operate satisfactorily. For a non-rechargeable these two lives are equal since the cells last for only one cycle by definition. (The term shelf life is used to describe how long a battery will retain its performance between manufacture and use.) Available capacity of all batteries drops with decreasing temperature. In contrast to most of today's batteries, the Zamboni pile, invented in 1812, offers a very long service life without refurbishment or recharge, although it supplies current only in the nanoamp range. The Oxford Electric Bell has been ringing almost continuously since 1840 on its original pair of batteries, thought to be Zamboni piles.[citation needed]

Self-discharge
Disposable batteries typically lose 8 to 20 percent of their original charge per year when stored at room temperature (20â30 Â°C).[40] This is known as the "self-discharge" rate, and is due to non-current-producing "side" chemical reactions that occur within the cell even when no load is applied. The rate of side reactions is reduced for batteries stored at lower temperatures, although some can be damaged by freezing.

Old rechargeable batteries self-discharge more rapidly than disposable alkaline batteries, especially nickel-based batteries; a freshly charged nickel cadmium (NiCd) battery loses 10% of its charge in the first 24 hours, and thereafter discharges at a rate of about 10% a month. However, newer low self-discharge nickel metal hydride (NiMH) batteries and modern lithium designs display a lower self-discharge rate (but still higher than for primary batteries).

Corrosion
Internal parts may corrode and fail, or the active materials may be slowly converted to inactive forms.

Physical component changes
The active material on the battery plates changes chemical composition on each charge and discharge cycle; active material may be lost due to physical changes of volume, further limiting the number of times the battery can be recharged. Most nickel-based batteries are partially discharged when purchased, and must be charged before first use.[41] Newer NiMH batteries are ready to be used when purchased, and have only 15% discharge in a year.[42]

Some deterioration occurs on each chargeâdischarge cycle. Degradation usually occurs because electrolyte migrates away from the electrodes or because active material detaches from the electrodes. Low-capacity NiMH batteries (1,700â2,000 mAÂ·h) can be charged some 1,000 times, whereas high-capacity NiMH batteries (above 2,500 mAÂ·h) last about 500 cycles.[43] NiCd batteries tend to be rated for 1,000 cycles before their internal resistance permanently increases beyond usable values.

Charge/discharge speed
Fast charging increases component changes, shortening battery lifespan.[43]

Overcharging
If a charger cannot detect when the battery is fully charged then overcharging is likely, damaging it.[44]

Memory effect
See also: Nickelâcadmium battery Â§ Memory effect
NiCd cells, if used in a particular repetitive manner, may show a decrease in capacity called "memory effect".[45] The effect can be avoided with simple practices. NiMH cells, although similar in chemistry, suffer less from memory effect.[46]


An analog camcorder [lithium ion] battery
Environmental conditions
Automotive leadâacid rechargeable batteries must endure stress due to vibration, shock, and temperature range. Because of these stresses and sulfation of their lead plates, few automotive batteries last beyond six years of regular use.[47] Automotive starting (SLI: Starting, Lighting, Ignition) batteries have many thin plates to maximize current. In general, the thicker the plates the longer the life. They are typically discharged only slightly before recharge.

"Deep-cycle" leadâacid batteries such as those used in electric golf carts have much thicker plates to extend longevity.[48] The main benefit of the leadâacid battery is its low cost; its main drawbacks are large size and weight for a given capacity and voltage. Leadâacid batteries should never be discharged to below 20% of their capacity,[49] because internal resistance will cause heat and damage when they are recharged. Deep-cycle leadâacid systems often use a low-charge warning light or a low-charge power cut-off switch to prevent the type of damage that will shorten the battery's life.[50]

Storage
Battery life can be extended by storing the batteries at a low temperature, as in a refrigerator or freezer, which slows the side reactions. Such storage can extend the life of alkaline batteries by about 5%; rechargeable batteries can hold their charge much longer, depending upon type.[51] To reach their maximum voltage, batteries must be returned to room temperature; discharging an alkaline battery at 250 mA at 0 Â°C is only half as efficient as at 20 Â°C.[26] Alkaline battery manufacturers such as Duracell do not recommend refrigerating batteries.[25]

Battery sizes
Main article: List of battery sizes
Primary batteries readily available to consumers range from tiny button cells used for electric watches, to the No. 6 cell used for signal circuits or other long duration applications. Secondary cells are made in very large sizes; very large batteries can power a submarine or stabilize an electrical grid and help level out peak loads.

Hazards
Explosion

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (April 2017) (Learn how and when to remove this template message)
See also: Safety of electronic cigarettes Â§ Fires, explosions, and other battery-related malfunctions

Battery after explosion
A battery explosion is generally caused by misuse or malfunction, such as attempting to recharge a primary (non-rechargeable) battery, or a short circuit.

When a battery is recharged at an excessive rate, an explosive gas mixture of hydrogen and oxygen may be produced faster than it can escape from within the battery (e.g. through a built-in vent), leading to pressure build-up and eventual bursting of the battery case. In extreme cases, battery chemicals may spray violently from the casing and cause injury. Overcharging â that is, attempting to charge a battery beyond its electrical capacity â can also lead to a battery explosion, in addition to leakage or irreversible damage. It may also cause damage to the charger or device in which the overcharged battery is later used.

Car batteries are most likely to explode when a short circuit generates very large currents. Such batteries produce hydrogen, which is very explosive, when they are overcharged (because of electrolysis of the water in the electrolyte). During normal use, the amount of overcharging is usually very small and generates little hydrogen, which dissipates quickly. However, when "jump starting" a car, the high current can cause the rapid release of large volumes of hydrogen, which can be ignited explosively by a nearby spark, e.g. when disconnecting a jumper cable.

Disposing of a battery via incineration may cause an explosion as steam builds up within the sealed case.

Recalls of devices using lithium-ion batteries have become more common in recent years. This is in response to reported accidents and failures, occasionally ignition or explosion.[52][53] An expert summary of the problem indicates that this type uses "liquid electrolytes to transport lithium ions between the anode and the cathode. If a battery cell is charged too quickly, it can cause a short circuit, leading to explosions and fires".[54][55]

Leakage

Leak-damaged alkaline battery
Many battery chemicals are corrosive, poisonous or both. If leakage occurs, either spontaneously or through accident, the chemicals released may be dangerous. For example, disposable batteries often use a zinc "can" both as a reactant and as the container to hold the other reagents. If this kind of battery is over-discharged, the reagents can emerge through the cardboard and plastic that form the remainder of the container. The active chemical leakage can then damage or disable the equipment that the batteries power. For this reason, many electronic device manufacturers recommend removing the batteries from devices that will not be used for extended periods of time.

Toxic materials
Many types of batteries employ toxic materials such as lead, mercury, and cadmium as an electrode or electrolyte. When each battery reaches end of life it must be disposed of to prevent environmental damage.[56] Batteries are one form of electronic waste (e-waste). E-waste recycling services recover toxic substances, which can then be used for new batteries.[57] Of the nearly three billion batteries purchased annually in the United States, about 179,000 tons end up in landfills across the country.[58] In the United States, the Mercury-Containing and Rechargeable Battery Management Act of 1996 banned the sale of mercury-containing batteries, enacted uniform labeling requirements for rechargeable batteries and required that rechargeable batteries be easily removable.[59] California and New York City prohibit the disposal of rechargeable batteries in solid waste, and along with Maine require recycling of cell phones.[60] The rechargeable battery industry operates nationwide recycling programs in the United States and Canada, with dropoff points at local retailers.[60]

The Battery Directive of the European Union has similar requirements, in addition to requiring increased recycling of batteries and promoting research on improved battery recycling methods.[61] In accordance with this directive all batteries to be sold within the EU must be marked with the "collection symbol" (a crossed-out wheeled bin). This must cover at least 3% of the surface of prismatic batteries and 1.5% of the surface of cylindrical batteries. All packaging must be marked likewise.[62]

Ingestion
Batteries may be harmful or fatal if swallowed.[63] Small button cells can be swallowed, in particular by young children. While in the digestive tract, the battery's electrical discharge may lead to tissue damage;[64] such damage is occasionally serious and can lead to death. Ingested disk batteries do not usually cause problems unless they become lodged in the gastrointestinal tract. The most common place for disk batteries to become lodged is the esophagus, resulting in clinical sequelae. Batteries that successfully traverse the esophagus are unlikely to lodge elsewhere. The likelihood that a disk battery will lodge in the esophagus is a function of the patient's age and battery size. Disk batteries of 16 mm have become lodged in the esophagi of 2 children younger than 1 year.[citation needed] Older children do not have problems with batteries smaller than 21â23 mm. Liquefaction necrosis may occur because sodium hydroxide is generated by the current produced by the battery (usually at the anode). Perforation has occurred as rapidly as 6 hours after ingestion.[65]

Chemistry
Many important cell properties, such as voltage, energy density, flammability, available cell constructions, operating temperature range and shelf life, are dictated by battery chemistry.

Primary batteries and their characteristics
Chemistry	Anode (â)	Cathode (+)	Max. voltage, theoretical (V)	Nominal voltage, practical (V)	Specific energy (kJ/kg)	Elaboration	Shelf life at 25 Â°C, 80% capacity (months)
Zincâcarbon	Zn	MnO2	1.6	1.2	130	Inexpensive.	18
Zincâchloride			1.5			Also known as "heavy-duty", inexpensive.
Alkaline
(zincâmanganese dioxide)	Zn	MnO2	1.5	1.15	400-590	Moderate energy density.
Good for high- and low-drain uses.	30
Nickel oxyhydroxide
(zincâmanganese dioxide/nickel oxyhydroxide)			1.7			Moderate energy density.
Good for high drain uses.
Lithium
(lithiumâcopper oxide)
LiâCuO	Li	CuO	1.7			No longer manufactured.
Replaced by silver oxide (IEC-type "SR") batteries.
Lithium
(lithiumâiron disulfide)
LiFeS2	Li	FeS2	1.8	1.5	1070	Expensive.
Used in 'plus' or 'extra' batteries.	337[66]
Lithium
(lithiumâmanganese dioxide)
LiMnO2	Li	MnO2	3.0		830â1010	Expensive.
Used only in high-drain devices or for long shelf-life due to very low rate of self-discharge.
'Lithium' alone usually refers to this type of chemistry.
Lithium
(lithiumâcarbon fluoride)
Liâ(CF)n	Li	(CF)n	3.6	3.0			120
Lithium
(lithiumâchromium oxide)
LiâCrO2	Li	CrO2	3.8	3.0			108
Lithium
(lithium-silicon)

Li22Si5
Mercury oxide	Zn	HgO	1.34	1.2		High-drain and constant voltage.
Banned in most countries because of health concerns.	36
Zincâair	Zn	O2	1.6	1.1	1590[67]	Used mostly in hearing aids.
Zamboni pile	Zn	Ag or Au		0.8		Very long life
Very low (nanoamp, nA) current	>2,000
Silver-oxide (silverâzinc)	Zn	Ag2O	1.85	1.5	470	Very expensive.
Used only commercially in 'button' cells.	30
Magnesium	Mg	MnO2	2.0	1.5			40

Secondary (rechargeable) batteries and their characteristics
Chemistry	Cell
voltage	Specific
energy
(kJ/kg)	Energy
density
(kJ/liter)	Comments
NiCd	1.2	140		Nickelâcadmium chemistry.
Inexpensive.
High-/low-drain, moderate energy density.
Can withstand very high discharge rates with virtually no loss of capacity.
Moderate rate of self-discharge.
Environmental hazard due to Cadmium â use now virtually prohibited in Europe.
Leadâacid	2.1	140		Moderately expensive.
Moderate energy density.
Moderate rate of self-discharge.
Higher discharge rates result in considerable loss of capacity.
Environmental hazard due to Lead.
Common use â Automobile batteries
NiMH	1.2	360		Nickelâmetal hydride chemistry.
Inexpensive.
Performs better than alkaline batteries in higher drain devices.
Traditional chemistry has high energy density, but also a high rate of self-discharge.
Newer chemistry has low self-discharge rate, but also a ~25% lower energy density.
Used in some cars.
NiZn	1.6	360		Nickel-zinc chemistry.
Moderately inexpensive.
High drain device suitable.
Low self-discharge rate.
Voltage closer to alkaline primary cells than other secondary cells.
No toxic components.
Newly introduced to the market (2009). Has not yet established a track record.
Limited size availability.
AgZn	1.86
1.5	460		Silver-zinc chemistry.
Smaller volume than equivalent Li-ion.
Extremely expensive due to silver.
Very high energy density.
Very high drain capable.
For many years considered obsolete due to high silver prices.
Cell suffers from oxidation if unused.
Reactions are not fully understood.
Terminal voltage very stable but suddenly drops to 1.5 volts at 70â80% charge (believed to be
due to presence of both argentous and argentic oxide in positive plate â one is consumed first).
Has been used in lieu of primary battery (moon buggy).
Is being developed once again as a replacement for Li-ion.
LiFePO4	3.3
3.0	360	790	Lithium-Iron-Phosphate chemistry.
Lithium ion	3.6	460		Various lithium chemistries.
Very expensive.
Very high energy density.
Not usually available in "common" battery sizes.
Lithium polymer battery is common in laptop computers, digital cameras, camcorders, and cellphones.
Very low rate of self-discharge.

Terminal voltage varies from 4.2 to 3.0 volts during discharge.
Volatile: Chance of explosion if short-circuited, allowed to overheat, or not manufactured with rigorous quality standards.
Solid-state batteries
On 28 February 2017, the University of Texas at Austin issued a press release about a new type of solid-state battery, developed by a team led by lithium-ion battery inventor John Goodenough, "that could lead to safer, faster-charging, longer-lasting rechargeable batteries for handheld mobile devices, electric cars and stationary energy storage".[68] More specifics about the new technology were published in the peer-reviewed scientific journal Energy & Environmental Science.

Independent reviews of the technology discuss the risk of fire and explosion from lithium-ion batteries under certain conditions because they use liquid electrolytes. The newly developed battery should be safer since it uses glass electrolytes that should eliminate short circuits. The solid-state battery is also said to have "three times the energy density", increasing its useful life in electric vehicles, for example. It should also be more ecologically sound since the technology uses less expensive, earth-friendly materials such as sodium extracted from seawater. They also have much longer life; "the cells have demonstrated more than 1,200 cycles with low cell resistance". The research and prototypes are not expected to lead to a commercially viable product in the near future, if ever, according to Chris Robinson of LUX Research. "This will have no tangible effect on electric vehicle adoption in the next 15 years, if it does at all. A key hurdle that many solid-state electrolytes face is lack of a scalable and cost-effective manufacturing process," he told The American Energy News in an e-mail.[69]

Homemade cells
Almost any liquid or moist object that has enough ions to be electrically conductive can serve as the electrolyte for a cell. As a novelty or science demonstration, it is possible to insert two electrodes made of different metals into a lemon,[70] potato,[71] etc. and generate small amounts of electricity. "Two-potato clocks" are also widely available in hobby and toy stores; they consist of a pair of cells, each consisting of a potato (lemon, et cetera) with two electrodes inserted into it, wired in series to form a battery with enough voltage to power a digital clock.[72] Homemade cells of this kind are of no practical use.

A voltaic pile can be made from two coins (such as a nickel and a penny) and a piece of paper towel dipped in salt water. Such a pile generates a very low voltage but, when many are stacked in series, they can replace normal batteries for a short time.[73]

Sony has developed a biological battery that generates electricity from sugar in a way that is similar to the processes observed in living organisms. The battery generates electricity through the use of enzymes that break down carbohydrates.[74]

Lead acid cells can easily be manufactured at home, but a tedious charge/discharge cycle is needed to 'form' the plates. This is a process in which lead sulfate forms on the plates and, during charge, is converted to lead dioxide (positive plate) and pure lead (negative plate). Repeating this process results in a microscopically rough surface, increasing the surface area, increasing the current the cell can deliver.[75]

Daniell cells are easy to make at home. Aluminiumâair batteries can be produced with high-purity aluminium. Aluminium foil batteries will produce some electricity, but are not efficient, in part because a significant amount of (combustible) hydrogen gas is produced.

See also
icon	Energy portal
icon	Renewable energy portal
icon	Electronics portal
Baghdad Battery
Battery electric vehicle
Battery holder
Battery isolator
Battery management system
Battery nomenclature
Battery pack
Battery regulations in the United Kingdom
Battery simulator
Battery (vacuum tube)
Comparison of battery types
Depth of discharge
Electric-vehicle battery
Grid energy storage
Nanowire battery
Search for the Super Battery (2017 PBS film)
State of charge
State of health
Trickle charging
References
 Crompton, T. R. (20 March 2000). Battery Reference Book (third ed.). Newnes. p. Glossary 3. ISBN 978-0-08-049995-6. Retrieved 18 March 2016.
 Pauling, Linus (1988). "15: Oxidation-Reduction Reactions; Electrolysis". General Chemistry. New York: Dover Publications, Inc. p. 539. ISBN 978-0-486-65622-9.
 Schmidt-Rohr, Klaus (2018). "How Batteries Store and Release Energy: Explaining Basic Electrochemistry". Journal of Chemical Education. 95 (10): 1801â1810. Bibcode:2018JChEd..95.1801S. doi:10.1021/acs.jchemed.8b00479.
 Pistoia, Gianfranco (25 January 2005). Batteries for Portable Devices. Elsevier. p. 1. ISBN 978-0-08-045556-3. Retrieved 18 March 2016.
 Bellis, Mary. History of the Electric Battery. About.com. Retrieved 11 August 2008.
 National Geographic Society. "Quiz: What You Don't Know About Batteries". National Geographic.
 Bellis, Mary. Biography of Alessandro Volta â Stored Electricity and the First Battery. About.com. Retrieved 7 August 2008.
 Stinner, Arthur. Alessandro Volta and Luigi Galvani Archived 10 September 2008 at the Wayback Machine (PDF). Retrieved 11 August 2008.
 Electric Battery History â Invention of the Electric Battery Archived 22 February 2019 at the Wayback Machine. The Great Idea Finder. Retrieved 11 August 2008.
 Battery History, Technology, Applications and Development. MPower Solutions Ltd. Retrieved 19 March 2007.
 Borvon, GÃ©rard (10 September 2012). "History of the electrical units". Association S-EAU-S.
 "Columbia Dry Cell Battery". National Historic Chemical Landmarks. American Chemical Society. Archived from the original on 23 February 2013. Retrieved 25 March 2013.
 Ashcroft, N.W.; Mermin (1976). Solid State Physics. N.D. Belmont, CA: Brooks/Cole.
 Dingrando 665.
 Saslow 338.
 Dingrando 666.
 Knight 943.
 Knight 976.
 Terminal Voltage â Tiscali Reference Archived 11 April 2008 at the Wayback Machine. Originally from Hutchinson Encyclopaedia. Retrieved 7 April 2007.
 Dingrando 674.
 Dingrando 677.
 Dingrando 675.
 Fink, Ch. 11, Sec. "Batteries and Fuel Cells."
 Franklin Leonard Pope, Modern Practice of the Electric Telegraph 15th Edition, D. Van Nostrand Company, New York, 1899, pp. 7â11. Available on the Internet Archive
 Duracell: Battery Care. Retrieved 10 August 2008.
 Alkaline Manganese Dioxide Handbook and Application Manual (PDF). Energizer. Retrieved 25 August 2008.
 Dynasty VRLA Batteries and Their Application Archived 6 February 2009 at the Wayback Machine. C&D Technologies, Inc. Retrieved 26 August 2008.
 USBCELL â Revolutionary rechargeable USB battery that can charge from any USB port. Retrieved 6 November 2007.
 "Spotlight on Photovoltaics & Fuel Cells: A Web-based Study & Comparison" (PDF). pp. 1â2. Retrieved 14 March 2007.
 Battery Knowledge â AA Portable Power Corp. Retrieved 16 April 2007. Archived 23 May 2007 at the Wayback Machine
 "Battery Capacity". techlib.com.
 A Guide to Understanding Battery Specifications, MIT Electric Vehicle Team, December 2008
 Kang, B.; Ceder, G. (2009). "Battery materials for ultrafast charging and discharging". Nature. 458 (7235): 190â193. Bibcode:2009Natur.458..190K. doi:10.1038/nature07853. PMID 19279634. S2CID 20592628. 1:00â6:50 (audio)
 "Elon Musk wins $50m bet with giant battery for South Australia". Sky News. 24 November 2017. Retrieved 20 September 2018.
 Dillow, Clay (21 December 2012). "China Builds the World's Largest Battery, a Building-Sized, 36-Megawatt-Hour Behemoth | Popular Science". Popsci.com. Retrieved 31 July 2013.
 Conway, E. (2 September 2008) "World's biggest battery switched on in Alaska" Telegraph.co.uk
 Biello, D. (22 December 2008) "Storing the Breeze: New Battery Might Make Wind Power More Reliable" Scientific American
 "Auwahi Wind | Energy Solutions | Sempra U.S. Gas & Power, LLC". Semprausgp.com. Archived from the original on 2 May 2014. Retrieved 31 July 2013.
 Amos, J. (24 August 2008) "Solar plane makes record flight" BBC News
 Self discharge of batteries â Corrosion Doctors. Retrieved 9 September 2007.
 Energizer Rechargeable Batteries and Chargers: Frequently Asked Questions Archived 9 February 2009 at the Wayback Machine. Energizer. Retrieved 3 February 2009.
 [1] Archived 2 February 2010 at the Wayback Machine
 Rechargeable battery Tips â NIMH Technology Information. Retrieved 10 August 2007. Archived 8 August 2007 at the Wayback Machine
 battery myths vs battery facts â free information to help you learn the difference. Retrieved 10 August 2007.
 Filip M. Gieszczykiewicz. "Sci.Electronics FAQ: More Battery Info". repairfaq.org.
 RechargheableBatteryInfo.com, ed. (28 October 2005), What does 'memory effect' mean?, archived from the original on 15 July 2007, retrieved 10 August 2007
 Rich, Vincent (1994). The International Lead Trade. Cambridge: Woodhead. 129.
 Deep Cycle Battery FAQ. Northern Arizona Wind & Sun. Retrieved 3 February 2009.
 Car and Deep Cycle Battery FAQ. Rainbow Power Company. Retrieved 3 February 2009.
 Deep cycle battery guide Archived 17 February 2009 at the Wayback Machine. Energy Matters. Retrieved 3 February 2009.
 Ask Yahoo: Does putting batteries in the freezer make them last longer? Archived 27 April 2006 at the Wayback Machine. Retrieved 7 March 2007.
 Schweber, Bill (4 August 2015). "Lithium Batteries: The Pros and Cons". GlobalSpec. GlobalSpec. Retrieved 15 March 2017.
 Fowler, Suzanne (21 September 2016). "Samsung's Recall â The Problem with Lithium Ion Batteries". The New York Times. New York. Retrieved 15 March 2016.
 Hislop, Martin (1 March 2017). "Solid-state EV battery breakthrough from Li-ion battery inventor John Goodenough". North American Energy News. The American Energy News. Retrieved 15 March 2017.
 "battery hazards". YouTube. Retrieved 20 September 2018.
 Batteries â Product Stewardship Archived 29 September 2006 at the Wayback Machine. EPA. Retrieved 11 September 2007.
 Battery Recycling Â» Earth 911. Retrieved 9 September 2007.
 "San Francisco Supervisor Takes Aim at Toxic Battery Waste". Environmental News Network (11 July 2001).
 Mercury-Containing and Rechargeable Battery Management Act Archived 6 February 2009 at the Wayback Machine
 [2][permanent dead link]
 Disposal of spent batteries and accumulators. European Union. Retrieved 27 July 2009.
 Guidelines on Portable Batteries Marking Requirements in the European Union 2008 â EPBA-EU Archived 7 October 2011 at the Wayback Machine
 Product Safety DataSheet â Energizer (p. 2). Retrieved 9 September 2007.
 "Swallowed a Button Battery? | Battery in the Nose or Ear?". Poison.org. 3 March 2010. Archived from the original on 16 August 2013. Retrieved 26 July 2013.
 "Disk Battery Ingestion: Background, Pathophysiology, Epidemiology". 9 June 2016 â via eMedicine.
 "Lithium Iron Disulfide Handbook and Application Manual" (PDF). energizer.com. Retrieved 20 September 2018.
 Excludes the mass of the air oxidizer.
 "Lithium-Ion Battery Inventor Introduces New Technology for Fast-Charging, Noncombustible Batteries". University of Texas at Austin. University of Texas. 28 February 2017. Retrieved 15 March 2017. ...first all-solid-state battery cells that could lead to safer, faster-charging, longer-lasting rechargeable batteries for handheld mobile devices, electric cars and stationary energy storage.
 Hislop, Martin (1 March 2017). "Solid-state EV battery breakthrough from Li-ion battery inventor John Goodenough". North American Energy News. The American Energy News. Retrieved 15 March 2017. But even John Goodenoughâs work doesnât change my forecast that EVs will take at least 50 years to reach 70 to 80 percent of the global vehicle market.
 ushistory.org: The Lemon Battery. Accessed 10 April 2007.
 ZOOM activities: phenom Potato Battery. Accessed 10 April 2007.
 Two-Potato Clock â Science Kit and Boreal Laboratories[permanent dead link]. Accessed 10 April 2007.
 Howstuffworks "Battery Experiments: Voltaic Pile". Accessed 10 April 2007.
 Sony Develops A Bio Battery Powered By Sugar. Accessed 24 August 2007.
 "Home made lead acid batteries". Windpower.org.za. 16 September 2007. Archived from the original on 31 July 2013. Retrieved 26 July 2013.
Further reading
Dingrando, Laurel; et al. (2007). Chemistry: Matter and Change. New York: Glencoe/McGraw-Hill. ISBN 978-0-07-877237-5. Ch. 21 (pp. 662â695) is on electrochemistry.
Fink, Donald G.; H. Wayne Beaty (1978). Standard Handbook for Electrical Engineers, Eleventh Edition. New York: McGraw-Hill. ISBN 978-0-07-020974-9.
Knight, Randall D. (2004). Physics for Scientists and Engineers: A Strategic Approach. San Francisco: Pearson Education. ISBN 978-0-8053-8960-9. Chs. 28â31 (pp. 879â995) contain information on electric potential.
Linden, David; Thomas B. Reddy (2001). Handbook of Batteries. New York: McGraw-Hill. ISBN 978-0-07-135978-8.
Saslow, Wayne M. (2002). Electricity, Magnetism, and Light. Toronto: Thomson Learning. ISBN 978-0-12-619455-5. Chs. 8â9 (pp. 336â418) have more information on batteries.
External links
	Wikimedia Commons has media related to Battery.
Batteries at Curlie
Non-rechargeable batteries
HowStuffWorks: How batteries work
Other Battery Cell Types
DoITPoMS Teaching and Learning Package- "Batteries"
The Physics arXiv Blog (17 August 2013). "First Atomic Level Simulation of a Whole Battery | MIT Technology Review". Technologyreview.com. Retrieved 21 August 2013.
vte
Battery sizes
vte
Galvanic cells
vte
Energy
Authority control Edit this at Wikidata
BNE: XX526307BNF: cb11933136g (data)GND: 4004687-4LCCN: sh85041588NARA: 10638068NDL: 00561468
Categories: Battery (electricity)Italian inventionsElectric powerConsumer electronics18th-century inventions
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadView sourceView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tagalog
Tiáº¿ng Viá»t
ä¸­æ
85 more
Edit links
This page was last edited on 29 October 2020, at 18:27 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Gastrointestinal tract
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"Enteric" redirects here. For other uses, see Enteric (disambiguation).
"Guts" redirects here. For other uses, see Guts (disambiguation).
Gastrointestinal tract
Stomach colon rectum diagram-en.svg
Diagram of stomach, intestines and rectum in the average human
Details
System	Digestive system
Identifiers
Latin	Tractus digestorius (mouth to anus),
canalis alimentarius (esophagus to large intestine),
canalis gastrointestinales (stomach to large intestine)
MeSH	D041981
Anatomical terminology
[edit on Wikidata]
Stomach colon rectum diagram-en.svg
Major parts of the
Gastrointestinal tract
Upper gastrointestinal tract[show]
Lower gastrointestinal tract[show]
See also[show]
vte
The gastrointestinal tract, (GI tract, GIT, digestive tract, digestion tract, alimentary canal) is the tract from the mouth to the anus which includes all the organs of the digestive system in humans and other animals. Food taken in through the mouth is digested to extract nutrients and absorb energy, and the waste expelled as feces. The mouth, esophagus, stomach and intestines are all part of the gastrointestinal tract. Gastrointestinal is an adjective meaning of or pertaining to the stomach and intestines. A tract is a collection of related anatomic structures or a series of connected body organs.

All vertebrates and most invertebrates have a digestive tract. The sponges, cnidarians, and ctenophores are the early invertebrates with an incomplete digestive tract having just one opening instead of two, where food is taken in and waste expelled.[1][2]

The human gastrointestinal tract consists of the esophagus, stomach, and intestines, and is divided into the upper and lower gastrointestinal tracts.[3] The GI tract includes all structures between the mouth and the anus,[4] forming a continuous passageway that includes the main organs of digestion, namely, the stomach, small intestine, and large intestine. However, the complete human digestive system is made up of the gastrointestinal tract plus the accessory organs of digestion (the tongue, salivary glands, pancreas, liver and gallbladder).[5] The tract may also be divided into foregut, midgut, and hindgut, reflecting the embryological origin of each segment. The whole human GI tract is about nine metres (30 feet) long at autopsy. It is considerably shorter in the living body because the intestines, which are tubes of smooth muscle tissue, maintain constant muscle tone in a halfway-tense state but can relax in spots to allow for local distention and peristalsis.[6][7]

The gastrointestinal tract contains trillions of microbes, with some 4,000 different strains of bacteria having diverse roles in maintenance of immune health and metabolism.[8][9][10] Cells of the GI tract release hormones to help regulate the digestive process. These digestive hormones, including gastrin, secretin, cholecystokinin, and ghrelin, are mediated through either intracrine or autocrine mechanisms, indicating that the cells releasing these hormones are conserved structures throughout evolution.[11]


Contents
1	Human gastrointestinal tract
1.1	Structure
1.1.1	Upper gastrointestinal tract
1.1.2	Lower gastrointestinal tract
1.1.2.1	Small intestine
1.1.2.2	Large intestine
1.1.3	Development
1.1.4	Histology
1.1.4.1	Mucosa
1.1.4.2	Submucosa
1.1.4.3	Muscular layer
1.1.4.4	Adventitia and serosa
1.1.5	Gene and protein expression
1.1.6	Time taken
1.1.7	Immune function
1.1.7.1	Immune barrier
1.1.7.2	Immune system homeostasis
1.1.8	Intestinal microbiota
1.1.9	Detoxification and drug metabolism
2	Clinical significance
2.1	Diseases
2.2	Symptoms
2.3	Treatment
2.4	Imaging
2.5	Other related diseases
3	Uses of animal guts
4	Other animals
5	See also
6	References
7	External links
Human gastrointestinal tract
Structure

Upper and lower human gastrointestinal tract

Illustration of human gastrointestinal tract
The structure and function can be described both as gross anatomy and as microscopic anatomy or histology. The tract itself is divided into upper and lower tracts, and the intestines small and large parts.[12]

Upper gastrointestinal tract
Main articles: Esophagus, Stomach, and duodenum
The upper gastrointestinal tract consists of the mouth, pharynx, esophagus, stomach, and duodenum.[13] The exact demarcation between the upper and lower tracts is the suspensory muscle of the duodenum. This differentiates the embryonic borders between the foregut and midgut, and is also the division commonly used by clinicians to describe gastrointestinal bleeding as being of either "upper" or "lower" origin. Upon dissection, the duodenum may appear to be a unified organ, but it is divided into four segments based upon function, location, and internal anatomy. The four segments of the duodenum are as follows (starting at the stomach, and moving toward the jejunum): bulb, descending, horizontal, and ascending. The suspensory muscle attaches the superior border of the ascending duodenum to the diaphragm.

The suspensory muscle is an important anatomical landmark which shows the formal division between the duodenum and the jejunum, the first and second parts of the small intestine, respectively.[14] This is a thin muscle which is derived from the embryonic mesoderm.

Lower gastrointestinal tract
The lower gastrointestinal tract includes most of the small intestine and all of the large intestine.[15] In human anatomy, the intestine (bowel, or gut. Greek: Ã©ntera) is the segment of the gastrointestinal tract extending from the pyloric sphincter of the stomach to the anus and, as in other mammals, consists of two segments, the small intestine and the large intestine. In humans, the small intestine is further subdivided into the duodenum, jejunum and ileum while the large intestine is subdivided into the, cecum, ascending, transverse, descending and sigmoid colon, rectum, and anal canal.[16][17]

Small intestine
Main article: Small intestine
The small intestine begins at the duodenum and is a tubular structure, usually between 6 and 7 m long.[18] Its mucosal area in an adult human is about 30 m2.[19] The combination of the circular folds, the villi, and the microvilli increases the absorptive area of the mucosa about 600-fold, making a total area of about 250 square meters for the entire small intestine.[20] Its main function is to absorb the products of digestion (including carbohydrates, proteins, lipids, and vitamins) into the bloodstream. There are three major divisions:

Duodenum: A short structure (about 20â25 cm long[18]) which receives chyme from the stomach, together with pancreatic juice containing digestive enzymes and bile from the gall bladder. The digestive enzymes break down proteins, and bile emulsifies fats into micelles. The duodenum contains Brunner's glands which produce a mucus-rich alkaline secretion containing bicarbonate. These secretions, in combination with bicarbonate from the pancreas, neutralize the stomach acids contained in the chyme.
Jejunum: This is the midsection of the small intestine, connecting the duodenum to the ileum. It is about 2.5 m long, and contains the circular folds also known as plicae circulares, and villi that increase its surface area. Products of digestion (sugars, amino acids, and fatty acids) are absorbed into the bloodstream here.
Ileum: The final section of the small intestine. It is about 3 m long, and contains villi similar to the jejunum. It absorbs mainly vitamin B12 and bile acids, as well as any other remaining nutrients.
Large intestine
Main article: Large intestine
The large intestine also called the colon, consists of the cecum, rectum, and anal canal. It also includes the appendix, which is attached to the cecum. The colon is further divided into:

Cecum (first portion of the colon) and appendix
Ascending colon (ascending in the back wall of the abdomen)
Right colic flexure (flexed portion of the ascending and transverse colon apparent to the liver)
Transverse colon (passing below the diaphragm)
Left colic flexure (flexed portion of the transverse and descending colon apparent to the spleen)
Descending colon (descending down the left side of the abdomen)
Sigmoid colon (a loop of the colon closest to the rectum)
Rectum
Anus
The main function of the large intestine is to absorb water. The area of the large intestinal mucosa of an adult human is about 2 m2.[19]

Development
Main article: Development of the digestive system
The gut is an endoderm-derived structure. At approximately the sixteenth day of human development, the embryo begins to fold ventrally (with the embryo's ventral surface becoming concave) in two directions: the sides of the embryo fold in on each other and the head and tail fold toward one another. The result is that a piece of the yolk sac, an endoderm-lined structure in contact with the ventral aspect of the embryo, begins to be pinched off to become the primitive gut. The yolk sac remains connected to the gut tube via the vitelline duct. Usually, this structure regresses during development; in cases where it does not, it is known as Meckel's diverticulum.

During fetal life, the primitive gut is gradually patterned into three segments: foregut, midgut, and hindgut. Although these terms are often used in reference to segments of the primitive gut, they are also used regularly to describe regions of the definitive gut as well.

Each segment of the gut is further specified and gives rise to specific gut and gut-related structures in later development. Components derived from the gut proper, including the stomach and colon, develop as swellings or dilatations in the cells of the primitive gut. In contrast, gut-related derivatives â that is, those structures that derive from the primitive gut but are not part of the gut proper, in general, develop as out-pouchings of the primitive gut. The blood vessels supplying these structures remain constant throughout development.[21]

Part	Part in adult	Gives rise to	Arterial supply
Foregut	esophagus to first 2 sections of the duodenum	Esophagus, stomach, duodenum (1st and 2nd parts), liver, gallbladder, pancreas, superior portion of pancreas
(Note that though the spleen is supplied by the celiac trunk, it is derived from dorsal mesentery and therefore not a foregut derivative)	celiac trunk
Midgut	lower duodenum, to the first two-thirds of the transverse colon	lower duodenum, jejunum, ileum, cecum, appendix, ascending colon, and first two-thirds of the transverse colon	branches of the superior mesenteric artery
Hindgut	last third of the transverse colon, to the upper part of the anal canal	last third of the transverse colon, descending colon, rectum, and upper part of the anal canal	branches of the inferior mesenteric artery
Histology
Main article: Gastrointestinal wall

General structure of the gut wall
1: Mucosa: Epithelium
2: Mucosa: Lamina propria
3: Mucosa: Muscularis mucosae
4: Lumen
5: Lymphatic tissue
6: Duct of gland outside tract
7: Gland in mucosa
8: Submucosa
9: Glands in submucosa
10: Meissner's submucosal plexus
11: Vein
12: Muscularis: Circular muscle
13: Muscularis: Longitudinal muscle
14: Serosa: Areolar connective tissue
15: Serosa: Epithelium
16: Auerbach's myenteric plexus
17: Nerve
18: Artery
19: Mesentery
The gastrointestinal tract has a form of general histology with some differences that reflect the specialization in functional anatomy.[22] The GI tract can be divided into four concentric layers in the following order:

Mucosa
Submucosa
Muscular layer
Adventitia or serosa
Mucosa
See also: Oral mucosa and Gastric mucosa
The mucosa is the innermost layer of the gastrointestinal tract. The mucosa surrounds the lumen, or open space within the tube. This layer comes in direct contact with digested food (chyme). The mucosa is made up of:

Epithelium â innermost layer. Responsible for most digestive, absorptive and secretory processes.
Lamina propria â a layer of connective tissue. Unusually cellular compared to most connective tissue
Muscularis mucosae â a thin layer of smooth muscle that aids the passing of material and enhances the interaction between the epithelial layer and the contents of the lumen by agitation and peristalsis.
The mucosae are highly specialized in each organ of the gastrointestinal tract to deal with the different conditions. The most variation is seen in the epithelium.

Submucosa
Main article: Submucosa
The submucosa consists of a dense irregular layer of connective tissue with large blood vessels, lymphatics, and nerves branching into the mucosa and muscularis externa. It contains the submucosal plexus, an enteric nervous plexus, situated on the inner surface of the muscularis externa.

Muscular layer
The muscular layer consists of an inner circular layer and a longitudinal outer layer. The circular layer prevents food from traveling backward and the longitudinal layer shortens the tract. The layers are not truly longitudinal or circular, rather the layers of muscle are helical with different pitches. The inner circular is helical with a steep pitch and the outer longitudinal is helical with a much shallower pitch.[23] Whilst the muscularis externa is similar throughout the entire gastrointestinal tract, an exception is the stomach which has an additional inner oblique muscular layer to aid with grinding and mixing of food. The muscularis externa of the stomach is composed of the inner oblique layer, middle circular layer, and outer longitudinal layer.

Between the circular and longitudinal muscle layers is the myenteric plexus. This controls peristalsis. Activity is initiated by the pacemaker cells, (myenteric interstitial cells of Cajal). The gut has intrinsic peristaltic activity (basal electrical rhythm) due to its self-contained enteric nervous system. The rate can be modulated by the rest of the autonomic nervous system.[23]

The coordinated contractions of these layers is called peristalsis and propels the food through the tract. Food in the GI tract is called a bolus (ball of food) from the mouth down to the stomach. After the stomach, the food is partially digested and semi-liquid, and is referred to as chyme. In the large intestine the remaining semi-solid substance is referred to as faeces.[23]

Adventitia and serosa
Main articles: Serous membrane and Adventitia
The outermost layer of the gastrointestinal tract consists of several layers of connective tissue.

Intraperitoneal parts of the GI tract are covered with serosa. These include most of the stomach, first part of the duodenum, all of the small intestine, caecum and appendix, transverse colon, sigmoid colon and rectum. In these sections of the gut there is clear boundary between the gut and the surrounding tissue. These parts of the tract have a mesentery.

Retroperitoneal parts are covered with adventitia. They blend into the surrounding tissue and are fixed in position. For example, the retroperitoneal section of the duodenum usually passes through the transpyloric plane. These include the esophagus, pylorus of the stomach, distal duodenum, ascending colon, descending colon and anal canal. In addition, the oral cavity has adventitia.

Gene and protein expression
Approximately 20,000 protein coding genes are expressed in human cells and 75% of these genes are expressed in at least one of the different parts of the digestive organ system.[24][25] Over 600 of these genes are more specifically expressed in one or more parts of the GI tract and the corresponding proteins have functions related to digestion of food and uptake of nutrients. Examples of specific proteins with such functions are pepsinogen PGC and the lipase LIPF, expressed in chief cells, and gastric ATPase ATP4A and gastric intrinsic factor GIF, expressed in parietal cells of the stomach mucosa. Specific proteins expressed in the stomach and duodenum involved in defence include mucin proteins, such as mucin 6 and intelectin-1.[26]

Time taken
The time taken for food to transit through the gastrointestinal tract varies on multiple factors, including age, ethnicity, and gender.[medical citation needed] Several techniques have been used to measure transit time, including radiography following a barium-labeled meal, breath hydrogen analysis, and scintigraphic analysis following a radiolabeled meal.[medical citation needed] It takes 2.5 to 3 hours for 50% of the contents to leave the stomach.[medical citation needed] The rate of digestion is also dependent of the material being digested, as food composition from the same meal may leave the stomach at different rates.[medical citation needed] Total emptying of the stomach takes around 4â5 hours, and transit through the colon takes 30 to 50 hours.[27][28][29]

Immune function
Immune barrier
The gastrointestinal tract forms an important part of the immune system.[30] The surface area of the digestive tract is estimated to be about 32 square meters, or about half a badminton court.[19] With such a large exposure (more than three times larger than the exposed surface of the skin), these immune components function to prevent pathogens from entering the blood and lymph circulatory systems.[31] Fundamental components of this protection are provided by the intestinal mucosal barrier which is composed of physical, biochemical, and immune elements elaborated by the intestinal mucosa.[32] Microorganisms also are kept at bay by an extensive immune system comprising the gut-associated lymphoid tissue (GALT)

There are additional factors contributing to protection from pathogen invasion. For example, low pH (ranging from 1 to 4) of the stomach is fatal for many microorganisms that enter it.[33] Similarly, mucus (containing IgA antibodies) neutralizes many pathogenic microorganisms.[34] Other factors in the GI tract contribution to immune function include enzymes secreted in the saliva and bile.

Immune system homeostasis
Beneficial bacteria also can contribute to the homeostasis of the gastrointestinal immune system. For example, Clostridia, one of the most predominant bacterial groups in the GI tract, play an important role in influencing the dynamics of the gut's immune system.[35] It has been demonstrated that the intake of a high fiber diet could be the responsible for the induction of T-regulatory cells (Tregs). This is due to the production of short-chain fatty acids during the fermentation of plant-derived nutrients such as butyrate and propionate. Basically, the butyrate induces the differentiation of Treg cells by enhancing histone H3 acetylation in the promoter and conserved non-coding sequence regions of the FOXP3 locus, thus regulating the T cells, resulting in the reduction of the inflammatory response and allergies.

Intestinal microbiota
The large intestine hosts several kinds of bacteria that can deal with molecules that the human body cannot otherwise break down.[36] This is an example of symbiosis. These bacteria also account for the production of gases at host-pathogen interface, inside our intestine(this gas is released as flatulence when eliminated through the anus). However the large intestine is mainly concerned with the absorption of water from digested material (which is regulated by the hypothalamus) and the re absorption of sodium, as well as any nutrients that may have escaped primary digestion in the ileum.[citation needed]

Health-enhancing intestinal bacteria of the gut flora serve to prevent the overgrowth of potentially harmful bacteria in the gut. These two types of bacteria compete for space and "food", as there are limited resources within the intestinal tract. A ratio of 80-85% beneficial to 15â20% potentially harmful bacteria generally is considered normal within the intestines.[citation needed]

Detoxification and drug metabolism
Enzymes such as CYP3A4, along with the antiporter activities, are also instrumental in the intestine's role of drug metabolism in the detoxification of antigens and xenobiotics.[37]

Clinical significance
This section discusses related diseases, medical associations with the gastrointestinal tract, and use in surgery.
Main articles: Gastrointestinal disease and Gastroenterology
Further information: Clinical significance
Diseases
There are many diseases and conditions that can affect the gastrointestinal system, including infections, inflammation and cancer.

Various pathogens, such as bacteria that cause foodborne illnesses, can induce gastroenteritis which results from inflammation of the stomach and small intestine. Antibiotics to treat such bacterial infections can decrease the microbiome diversity of the gastrointestinal tract, and further enable inflammatory mediators.[38] Gastroenteritis is the most common disease of the GI tract.

Gastrointestinal cancer may occur at any point in the gastrointestinal tract, and includes mouth cancer, tongue cancer, oesophageal cancer, stomach cancer, and colorectal cancer.
Inflammatory conditions. Ileitis is an inflammation of the ileum, colitis is an inflammation of the large intestine.
Appendicitis is inflammation of the appendix located at the caecum. This is a potentially fatal condition if left untreated; most cases of appendicitis require surgical intervention.
Diverticular disease is a condition that is very common in older people in industrialized countries. It usually affects the large intestine but has been known to affect the small intestine as well. Diverticulosis occurs when pouches form on the intestinal wall. Once the pouches become inflamed it is known as diverticulitis.

Inflammatory bowel disease is an inflammatory condition affecting the bowel walls, and includes the subtypes Crohn's disease and ulcerative colitis. While Crohn's can affect the entire gastrointestinal tract, ulcerative colitis is limited to the large intestine. Crohn's disease is widely regarded as an autoimmune disease. Although ulcerative colitis is often treated as though it were an autoimmune disease, there is no consensus that it actually is such.

Functional gastrointestinal disorders the most common of which is irritable bowel syndrome. Functional constipation and chronic functional abdominal pain are other functional disorders of the intestine that have physiological causes but do not have identifiable structural, chemical, or infectious pathologies.

Symptoms
Several symptoms are used to indicate problems with the gastrointestinal tract:

Vomiting, which may include regurgitation of food or the vomiting of blood
Diarrhea, or the passage of liquid or more frequent stools
Constipation, which refers to the passage of fewer and hardened stools
Blood in stool, which includes fresh red blood, maroon-coloured blood, and tarry-coloured blood
Treatment
Gastrointestinal surgery can often be performed in the outpatient setting. In the United States in 2012, operations on the digestive system accounted for 3 of the 25 most common ambulatory surgery procedures and constituted 9.1 percent of all outpatient ambulatory surgeries.[39]

Imaging
Various methods of imaging the gastrointestinal tract include the upper and lower gastrointestinal series:

Radioopaque dyes may be swallowed to produce a barium swallow
Parts of the tract may be visualised by camera. This is known as endoscopy if examining the upper gastrointestinal tract, and colonoscopy or sigmoidoscopy if examining the lower gastrointestinal tract. Capsule endoscopy is where a capsule containing a camera is swallowed in order to examine the tract. Biopsies may also be taken when examined.
An abdominal x-ray may be used to examine the lower gastrointestinal tract.
Other related diseases
Cholera
Enteric duplication cyst
Giardiasis
Pancreatitis
Peptic ulcer disease
Yellow fever
Helicobacter pylori is a gram-negative spiral bacterium. Over half the world's population is infected with it, mainly during childhood; it is not certain how the disease is transmitted. It colonizes the gastrointestinal system, predominantly the stomach. The bacterium has specific survival conditions that our gastric microenvironment: it is both capnophilic and microaerophilic. Helicobacter also exhibits a tropism for gastric epithelial lining and the gastric mucosal layer about it. Gastric colonization of this bacterium triggers a robust immune response leading to moderate to severe inflammation, known as gastritis. Signs and symptoms of infection are gastritis, burning abdominal pain, weight loss, loss of appetite, bloating, burping, nausea, bloody vomit, and black tarry stools. Infection can be detected in a number of ways: GI X-rays, endoscopy, blood tests for anti-Helicobacter antibodies, a stool test, and a urease breath test (which is a by-product of the bacteria). If caught soon enough, it can be treated with three doses of different proton pump inhibitors as well as two antibiotics, taking about a week to cure. If not caught soon enough, surgery may be required.[40][41][42][43]
Intestinal pseudo-obstruction is a syndrome caused by a malformation of the digestive system, characterized by a severe impairment in the ability of the intestines to push and assimilate. Symptoms include daily abdominal and stomach pain, nausea, severe distension, vomiting, heartburn, dysphagia, diarrhea, constipation, dehydration and malnutrition. There is no cure for intestinal pseudo-obstruction. Different types of surgery and treatment managing life-threatening complications such as ileus and volvulus, intestinal stasis which lead to bacterial overgrowth, and resection of affected or dead parts of the gut may be needed. Many patients require parenteral nutrition.
Ileus is a blockage of the intestines.
Coeliac disease is a common form of malabsorption, affecting up to 1% of people of northern European descent. An autoimmune response is triggered in intestinal cells by digestion of gluten proteins. Ingestion of proteins found in wheat, barley and rye, causes villous atrophy in the small intestine. Lifelong dietary avoidance of these foodstuffs in a gluten-free diet is the only treatment.
Enteroviruses are named by their transmission-route through the intestine (enteric meaning intestinal), but their symptoms aren't mainly associated with the intestine.
Endometriosis can affect the intestines, with similar symptoms to IBS.
Bowel twist (or similarly, bowel strangulation) is a comparatively rare event (usually developing sometime after major bowel surgery). It is, however, hard to diagnose correctly, and if left uncorrected can lead to bowel infarction and death. (The singer Maurice Gibb is understood to have died from this.)
Angiodysplasia of the colon
Constipation
Diarrhea
Hirschsprung's disease (aganglionosis)
Intussusception
Polyp (medicine) (see also colorectal polyp)
Pseudomembranous colitis
Toxic megacolon usually a complication of ulcerative colitis
Uses of animal guts
Intestines from animals other than humans are used in a number of ways. From each species of livestock that is a source of milk, a corresponding rennet is obtained from the intestines of milk-fed calves. Pig and calf intestines are eaten, and pig intestines are used as sausage casings. Calf intestines supply calf-intestinal alkaline phosphatase (CIP), and are used to make goldbeater's skin. Other uses are:

The use of animal gut strings by musicians can be traced back to the third dynasty of Egypt. In the recent past, strings were made out of lamb gut. With the advent of the modern era, musicians have tended to use strings made of silk, or synthetic materials such as nylon or steel. Some instrumentalists, however, still use gut strings in order to evoke the older tone quality. Although such strings were commonly referred to as "catgut" strings, cats were never used as a source for gut strings.[44]
Sheep gut was the original source for natural gut string used in racquets, such as for tennis. Today, synthetic strings are much more common, but the best gut strings are now made out of cow gut.
Gut cord has also been used to produce strings for the snares that provide a snare drum's characteristic buzzing timbre. While the modern snare drum almost always uses metal wire rather than gut cord, the North African bendir frame drum still uses gut for this purpose.
"Natural" sausage hulls, or casings, are made of animal gut, especially hog, beef, and lamb.
The wrapping of kokoretsi, gardoubakia, and torcinello is made of lamb (or goat) gut.
Haggis is traditionally boiled in, and served in, a sheep stomach.
Chitterlings, a kind of food, consist of thoroughly washed pig's gut.
Animal gut was used to make the cord lines in longcase clocks and for fusee movements in bracket clocks, but may be replaced by metal wire.
The oldest known condoms, from 1640 AD, were made from animal intestine.[45]
Other animals
Further information: Digestive system of ruminants
Many birds and other animals have a specialised stomach in the digestive tract called a gizzard used for grinding up food.

Another feature not found in the human but found in a range of other animals is the crop. In birds this is found as a pouch alongside the esophagus.

Other animals including amphibians, birds, reptiles, and egg-laying mammals have a major difference in their GI tract in that it ends in a cloaca and not an anus.

In 2020, the oldest known fossil digestive tract, of an extinct wormlike organism in the Cloudinidae was discovered; it lived during the late Ediacaran period about 550 million years ago.[46][47]

See also
This article uses anatomical terminology.
Gastrointestinal physiology
All pages with titles beginning with Gastrointestinal
All pages with titles containing Gastrointestinal
References
 "Welcome to CK-12 Foundation". www.ck12.org. Retrieved 2 July 2020.
 Ruppert EE, Fox RS, Barnes RD (2004). "Introduction to Bilateria". Invertebrate Zoology (7 ed.). Brooks / Cole. p. 197 [1]. ISBN 978-0-03-025982-1.
 "gastrointestinal tract" at Dorland's Medical Dictionary
 Gastrointestinal+tract at the US National Library of Medicine Medical Subject Headings (MeSH)
 "digestive system" at Dorland's Medical Dictionary
 G., Hounnou; C., Destrieux; J., DesmÃ©; P., Bertrand; S., Velut (2002-12-01). "Anatomical study of the length of the human intestine". Surgical and Radiologic Anatomy. 24 (5): 290â294. doi:10.1007/s00276-002-0057-y. ISSN 0930-1038. PMID 12497219. S2CID 33366428.
 Raines, Daniel; Arbour, Adrienne; Thompson, Hilary W.; Figueroa-Bodine, Jazmin; Joseph, Saju (2014-05-26). "Variation in small bowel length: Factor in achieving total enteroscopy?". Digestive Endoscopy. 27 (1): 67â72. doi:10.1111/den.12309. ISSN 0915-5635. PMID 24861190. S2CID 19069407.
 Lin, L; Zhang, J (2017). "Role of intestinal microbiota and metabolites on gut homeostasis and human diseases". BMC Immunology. 18 (1): 2. doi:10.1186/s12865-016-0187-3. PMC 5219689. PMID 28061847.
 Marchesi, J. R; Adams, D. H; Fava, F; Hermes, G. D; Hirschfield, G. M; Hold, G; Quraishi, M. N; Kinross, J; Smidt, H; Tuohy, K. M; Thomas, L. V; Zoetendal, E. G; Hart, A (2015). "The gut microbiota and host health: A new clinical frontier". Gut. 65 (2): 330â339. doi:10.1136/gutjnl-2015-309990. PMC 4752653. PMID 26338727.
 Clarke, Gerard; Stilling, Roman M; Kennedy, Paul J; Stanton, Catherine; Cryan, John F; Dinan, Timothy G (2014). "Minireview: Gut Microbiota: The Neglected Endocrine Organ". Molecular Endocrinology. 28 (8): 1221â38. doi:10.1210/me.2014-1108. PMC 5414803. PMID 24892638.
 Nelson RJ. 2005. Introduction to Behavioral Endocrinology. Sinauer Associates: Massachusetts. p 57.
 Thomasino, Anne Marie (2001). "Length of a Human Intestine". The Physics Factbook.
 Upper+Gastrointestinal+Tract at the US National Library of Medicine Medical Subject Headings (MeSH)
 David A. Warrell (2005). Oxford textbook of medicine: Sections 18-33. Oxford University Press. pp. 511â. ISBN 978-0-19-856978-7. Retrieved 1 July 2010.
 Lower+Gastrointestinal+Tract at the US National Library of Medicine Medical Subject Headings (MeSH)
 Kapoor, Vinay Kumar (13 Jul 2011). Gest, Thomas R. (ed.). "Large Intestine Anatomy". Medscape. WebMD LLC. Retrieved 2013-08-20.
 Gray, Henry (1918). Gray's Anatomy. Philadelphia: Lea & Febiger.
 Drake, Richard L.; Vogl, Wayne; Tibbitts, Adam W.M. Mitchell; illustrations by Richard; Richardson, Paul (2015). Gray's anatomy for students (3rd ed.). Philadelphia: Elsevier/Churchill Livingstone. p. 312. ISBN 978-0-8089-2306-0.
 Helander, Herbert F.; FÃ¤ndriks, Lars (2014-06-01). "Surface area of the digestive tract - revisited". Scandinavian Journal of Gastroenterology. 49 (6): 681â689. doi:10.3109/00365521.2014.898326. ISSN 1502-7708. PMID 24694282. S2CID 11094705.
 Hall, John (2011). Guyton and Hall textbook of medical physiology (Twelfth ed.). p. 794. ISBN 9781416045748.
 Bruce M. Carlson (2004). Human Embryology and Developmental Biology (3rd ed.). Saint Louis: Mosby. ISBN 978-0-323-03649-8.
 Abraham L. Kierszenbaum (2002). Histology and cell biology: an introduction to pathology. St. Louis: Mosby. ISBN 978-0-323-01639-1.
 Sarna, S.K. (2010). "Introduction". Colonic Motility: From Bench Side to Bedside. San Rafael, California: Morgan & Claypool Life Sciences. ISBN 9781615041503.
 "The human proteome in gastrointestinal tract - The Human Protein Atlas". www.proteinatlas.org. Retrieved 2017-09-21.
 UhlÃ©n, Mathias; Fagerberg, Linn; HallstrÃ¶m, BjÃ¶rn M.; Lindskog, Cecilia; Oksvold, Per; Mardinoglu, Adil; Sivertsson, Ãsa; Kampf, Caroline; SjÃ¶stedt, Evelina (2015-01-23). "Tissue-based map of the human proteome". Science. 347 (6220): 1260419. doi:10.1126/science.1260419. ISSN 0036-8075. PMID 25613900. S2CID 802377.
 Gremel, Gabriela; Wanders, Alkwin; Cedernaes, Jonathan; Fagerberg, Linn; HallstrÃ¶m, BjÃ¶rn; Edlund, Karolina; SjÃ¶stedt, Evelina; UhlÃ©n, Mathias; PontÃ©n, Fredrik (2015-01-01). "The human gastrointestinal tract-specific transcriptome and proteome as defined by RNA sequencing and antibody-based profiling". Journal of Gastroenterology. 50 (1): 46â57. doi:10.1007/s00535-014-0958-7. ISSN 0944-1174. PMID 24789573. S2CID 21302849.
 Kim, SK (1968). "Small intestine transit time in the normal small bowel study". American Journal of Roentgenology. 104 (3): 522â524. doi:10.2214/ajr.104.3.522. PMID 5687899.
 Ghoshal, U. C.; Sengar, V.; Srivastava, D. (2012). "Colonic Transit Study Technique and Interpretation: Can These be Uniform Globally in Different Populations with Non-uniform Colon Transit Time?". Journal of Neurogastroenterology and Motility. 18 (2): 227â228. doi:10.5056/jnm.2012.18.2.227. PMC 3325313. PMID 22523737.
 "Gastrointestinal Transit: How Long Does It Take?".
 Mowat, Allan M.; Agace, William W. (2014-10-01). "Regional specialization within the intestinal immune system". Nature Reviews. Immunology. 14 (10): 667â685. doi:10.1038/nri3738. ISSN 1474-1741. PMID 25234148. S2CID 31460146.
 Flannigan, Kyle L.; Geem, Duke; Harusato, Akihito; Denning, Timothy L. (2015-07-01). "Intestinal Antigen-Presenting Cells: Key Regulators of Immune Homeostasis and Inflammation". The American Journal of Pathology. 185 (7): 1809â1819. doi:10.1016/j.ajpath.2015.02.024. ISSN 1525-2191. PMC 4483458. PMID 25976247.
 SÃ¡nchez de Medina, FermÃ­n; Romero-Calvo, Isabel; Mascaraque, Cristina; MartÃ­nez-Augustin, Olga (2014-12-01). "Intestinal inflammation and mucosal barrier function". Inflammatory Bowel Diseases. 20 (12): 2394â2404. doi:10.1097/MIB.0000000000000204. ISSN 1536-4844. PMID 25222662. S2CID 11434730.
 Schubert, Mitchell L. (2014-11-01). "Gastric secretion". Current Opinion in Gastroenterology. 30 (6): 578â582. doi:10.1097/MOG.0000000000000125. ISSN 1531-7056. PMID 25211241. S2CID 8267813.
 MÃ¡rquez, Mercedes; FernÃ¡ndez GutiÃ©rrez Del Ãlamo, Clotilde; GirÃ³n-GonzÃ¡lez, JosÃ© Antonio (2016-01-28). "Gut epithelial barrier dysfunction in human immunodeficiency virus-hepatitis C virus coinfected patients: Influence on innate and acquired immunity". World Journal of Gastroenterology. 22 (4): 1433â1448. doi:10.3748/wjg.v22.i4.1433. ISSN 2219-2840. PMC 4721978. PMID 26819512.
 Furusawa, Yukihiro; Obata, Yuuki; Fukuda, Shinji; Endo, Takaho A.; Nakato, Gaku; Takahashi, Daisuke; Nakanishi, Yumiko; Uetake, Chikako; Kato, Keiko; Kato, Tamotsu; Takahashi, Masumi; Fukuda, Noriko N.; Murakami, Shinnosuke; Miyauchi, Eiji; Hino, Shingo; Atarashi, Koji; Onawa, Satoshi; Fujimura, Yumiko; Lockett, Trevor; Clarke, Julie M.; Topping, David L.; Tomita, Masaru; Hori, Shohei; Ohara, Osamu; Morita, Tatsuya; Koseki, Haruhiko; Kikuchi, Jun; Honda, Kenya; Hase, Koji; Ohno, Hiroshi (2013). "Commensal microbe-derived butyrate induces the differentiation of colonic regulatory T cells". Nature. 504 (7480): 446â450. Bibcode:2013Natur.504..446F. doi:10.1038/nature12721. PMID 24226770. S2CID 4408815.
 Knight, Judson (2002). Science of Everyday Things: Real-life biology. 4. ISBN 9780787656348.
 Jakoby, WB; Ziegler, DM (5 December 1990). "The enzymes of detoxication". The Journal of Biological Chemistry. 265 (34): 20715â8. PMID 2249981.
 Nitzan, Orna; Elias, Mazen; Peretz, Avi; Saliba, Walid (2016-01-21). "Role of antibiotics for treatment of inflammatory bowel disease". World Journal of Gastroenterology. 22 (3): 1078â1087. doi:10.3748/wjg.v22.i3.1078. ISSN 1007-9327. PMC 4716021. PMID 26811648.
 Wier LM, Steiner CA, Owens PL (February 2015). "Surgeries in Hospital-Owned Outpatient Facilities, 2012". HCUP Statistical Brief #188. Rockville, MD: Agency for Healthcare Research and Quality.
 Fox, James; Timothy Wang (January 2007). "Inflammation, Atrophy, and Gastric Cancer". Journal of Clinical Investigation. review. 117 (1): 60â69. doi:10.1172/JCI30111. PMC 1716216. PMID 17200707. Retrieved 19 May 2014.
 Murphy, Kenneth (20 May 2014). Janeway's Immunobiology. New York: Garland Science, Taylor and Francis Group, LLC. pp. 389â398. ISBN 978-0-8153-4243-4.
 Parham, Peter (20 May 2014). The Immune System. New York: Garland Science Taylor and Francis Group LLC. p. 494. ISBN 978-0-8153-4146-8.
 Goering, Richard (20 May 2014). MIMS Medical Microbiology. Philadelphia: Elsevier. pp. 32, 64, 294, 133â4, 208, 303â4, 502. ISBN 978-0-3230-4475-2.
 Hiskey, Daven. "Violin strings were never made out of actual cat guts". TodayIFoundOut.com. Retrieved 15 December 2015.
 "World's oldest condom". Ananova. 2008. Retrieved 2008-04-11.
 Joel, Lucas (10 January 2020). "Fossil Reveals Earth's Oldest Known Animal Guts - The find in a Nevada desert revealed an intestine inside a creature that looks like a worm made of a stack of ice cream cones". The New York Times. Retrieved 10 January 2020.
 Svhiffbauer, James D.; et al. (10 January 2020). "Discovery of bilaterian-type through-guts in cloudinomorphs from the terminal Ediacaran Period". Nature Communications. 11 (205): 205. Bibcode:2020NatCo..11..205S. doi:10.1038/s41467-019-13882-z. PMC 6954273. PMID 31924764.
External links
	Look up gastrointestinal tract, gastrointestinal, or tract in Wiktionary, the free dictionary.
	Wikimedia Commons has media related to Gastrointestinal tract and Digestive system.
	The Wikibook Human Physiology has a page on the topic of: The gastrointestinal system
The gastro intestinal tract in the Human Protein Atlas
Your Digestive System and How It Works at National Institutes of Health
vte
Human systems and organs
vte
Anatomy of the mouth
vte
Anatomy of the gastrointestinal tract, excluding the mouth
Upper
Pharynx
MusclesSpaces peripharyngeal retropharyngeal parapharyngeal retrovisceral dangerprevertebralPterygomandibular raphePharyngeal rapheBuccopharyngeal fasciaPharyngobasilar fasciaPyriform sinus
Esophagus
Sphincters upperlowerglands
Stomach
Curvatures greaterlesserAngular incisureCardiaBodyFundusPylorus antrumcanalsphincterGastric mucosaGastric foldsMicroanatomy Gastric pitsGastric glandsCardiac glandsFundic glandsPyloric glandsFoveolar cellParietal cellGastric chief cellEnterochromaffin-like cell
Lower
Small intestine
Microanatomy
Intestinal villusIntestinal glandEnterocyteEnteroendocrine cellGoblet cellPaneth cell
Duodenum
Suspensory muscleMajor duodenal papillaMinor duodenal papillaDuodenojejunal flexureBrunner's glands
Jejunum
No substructures
Ileum
Ileocecal valvePeyer's patchesMicrofold cell
Large intestine
Cecum
Appendix
Colon
Ascending colonHepatic flexureTransverse colonSplenic flexureDescending colonSigmoid colonContinuous taenia colihaustraepiploic appendix
Rectum
Transverse foldsAmpulla
Anal canal
AnusAnal columnsAnal valvesAnal sinusesPectinate lineInternal anal sphincterIntersphincteric grooveExternal anal sphincter
Wall
Serosa / AdventitiaSubserosaMuscular layerSubmucosaCircular foldsMucosaMuscularis mucosa
Authority control Edit this at Wikidata
LCCN: sh85053487
Categories: AbdomenDigestive systemEndocrine systemRoutes of administration
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
EspaÃ±ol
FranÃ§ais
à¤¹à¤¿à¤¨à¥à¤¦à¥
Bahasa Indonesia
Bahasa Melayu
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
23 more
Edit links
This page was last edited on 19 October 2020, at 11:06 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Bile acid
From Wikipedia, the free encyclopedia
  (Redirected from Bile acids)
Jump to navigationJump to search
Bile acids are steroid acids found predominantly in the bile of mammals and other vertebrates. Diverse bile acids are synthesized in the liver.[1] Bile acids are conjugated with taurine or glycine residues to give anions called bile salts.[2][3][4]

Primary bile acids are those synthesized by the liver. Secondary bile acids result from bacterial actions in the colon. In humans, taurocholic acid and glycocholic acid (derivatives of cholic acid) and taurochenodeoxycholic acid and glycochenodeoxycholic acid (derivatives of chenodeoxycholic acid) are the major bile salts. They are roughly equal in concentration.[5] The salts of their 7-alpha-dehydroxylated derivatives, deoxycholic acid and lithocholic acid, are also found, with derivatives of cholic, chenodeoxycholic and deoxycholic acids accounting for over 90% of human biliary bile acids.[5]

Bile acids comprise about 80% of the organic compounds in bile (others are phospholipids and cholesterol).[5] An increased secretion of bile acids produces an increase in bile flow. Bile acids facilitate digestion of dietary fats and oils. They serve as micelle-forming surfactants, which encapsulate nutrients, facilitating their absorption.[6] These micelles are suspended in the chyme before further processing. Bile acids also have hormonal actions throughout the body, particularly through the farnesoid X receptor and GPBAR1 (also known as TGR5).[7]


Structure of cholic acid showing relationship to other bile acids

Contents
1	Production
2	Functions
3	Structure and synthesis
4	Hormonal actions
4.1	Regulation of synthesis
4.2	Metabolic functions
4.3	Other interactions
5	Clinical significance
5.1	Hyperlipidemia
5.2	Cholestasis
5.3	Gallstones
5.4	Bile acid diarrhea
5.5	Bile acids and colon cancer
5.6	Dermatology
6	References
7	External links
Production
Bile acid synthesis occurs in liver cells, which synthesize primary bile acids (cholic acid and chenodeoxycholic acid in humans) via cytochrome P450-mediated oxidation of cholesterol in a multi-step process. Approximately 600 mg of bile salts are synthesized daily to replace bile acids lost in the feces, although, as described below, much larger amounts are secreted, reabsorbed in the gut and recycled. The rate-limiting step in synthesis is the addition of a hydroxyl group of the 7th position of the steroid nucleus by the enzyme cholesterol 7 alpha-hydroxylase. This enzyme is down-regulated by cholic acid, up-regulated by cholesterol and is inhibited by the actions of the ileal hormone FGF15/19.[2][3]

Prior to secreting any of the bile acids (primary or secondary, see below), liver cells conjugate them with either glycine or taurine, to form a total of 8 possible conjugated bile acids. These conjugated bile acids are often referred to as bile salts. The pKa of the unconjugated bile acids are between 5 and 6.5,[4] and the pH of the duodenum ranges between 3 and 5, so when unconjugated bile acids are in the duodenum, they are almost always protonated (HA form), which makes them relatively insoluble in water. Conjugating bile acids with amino acids lowers the pKa of the bile-acid/amino-acid conjugate to between 1 and 4. Thus conjugated bile acids are almost always in their deprotonated (A-) form in the duodenum, which makes them much more water-soluble and much more able to fulfil their physiologic function of emulsifying fats.[8][9]

Once secreted into the lumen of the intestine, bile salts are modified by gut bacteria. They are partial dehydroxylated. Their glycine and taurine groups are removed to give the secondary bile acids, deoxycholic acid and lithocholic acid. Cholic acid is converted into deoxycholic acid and chenodeoxycholic acid into lithocholic acid. All four of these bile acids recycled, in a process known as enterohepatic circulation.[2][3]

Functions
As amphipathic molecules with hydrophobic and hydrophilic regions, conjugated bile salts sit at the lipid/water interface and, at the right concentration, form micelles.[9] The added solubility of conjugated bile salts aids in their function by preventing passive re-absorption in the small intestine. As a result, the concentration of bile acids/salts in the small intestine is high enough to form micelles and solubilize lipids. "Critical micellar concentration" refers to both an intrinsic property of the bile acid itself and amount of bile acid necessary to function in the spontaneous and dynamic formation of micelles.[9] Bile acid-containing micelles aid lipases to digest lipids and bring them near the intestinal brush border membrane, which results in fat absorption.[6]

Synthesis of bile acids is a major route of cholesterol metabolism in most species other than humans. The body produces about 800 mg of cholesterol per day and about half of that is used for bile acid synthesis producing 400â600 mg daily. Human adults secrete between 12-18 g of bile acids into the intestine each day, mostly after meals. The bile acid pool size is between 4â6 g, which means that bile acids are recycled several times each day. About 95% of bile acids are reabsorbed by active transport in the ileum and recycled back to the liver for further secretion into the biliary system and gallbladder. This enterohepatic circulation of bile acids allows a low rate of synthesis, only about 0.3g/day, but with large amounts being secreted into the intestine.[5]

Bile acids have other functions, including eliminating cholesterol from the body, driving the flow of bile to eliminate certain catabolites (including bilirubin), emulsifying fat-soluble vitamins to enable their absorption, and aiding in motility and the reduction of the bacteria flora found in the small intestine and biliary tract.[5]

Bile acids have metabolic actions in the body resembling those of hormones, acting through two specific receptors, the farnesoid X receptor and G protein-coupled bile acid receptor/TGR5.[7][10] They bind less specifically to some other receptors and have been reported to regulate the activity of certain enzymes [11] and ion channels [12] and the synthesis of diverse substances including endogenous fatty acid ethanolamides.[13] [14]

Structure and synthesis
The structures of the principal human bile acids

Cholic acid



Glycocholic acid



Taurocholic acid



Deoxycholic acid



Chenodeoxycholic acid



Glycochenodeoxycholic acid



Taurochenodeoxycholic acid



Lithocholic acid

Bile salts constitute a large family of molecules, composed of a steroid structure with four rings, a five- or eight-carbon side-chain terminating in a carboxylic acid, and several hydroxyl groups, the number and orientation of which is different among the specific bile salts.[1] The four rings are labeled A, B, C, and D, from the farthest to the closest to the side chain with the carboxyl group. The D-ring is smaller by one carbon than the other three. The structure is commonly drawn with A at the left and D at the right. The hydroxyl groups can be in either of two configurations: either up (or out), termed beta (Î²; often drawn by convention as a solid line), or down, termed alpha (Î±; displayed as a dashed line). All bile acids have a 3-hydroxyl group, derived from the parent molecule, cholesterol, in which the 3-hydroxyl is beta.[1]


IUPAC recommended ring lettering (left) and atom numbering (right) of the steroid skeleton. The four rings A-D form a sterane core.
The initial step in the classical pathway of hepatic synthesis of bile acids is the enzymatic addition of a 7Î± hydroxyl group by cholesterol 7Î±-hydroxylase (CYP7A1) forming 7Î±-hydroxycholesterol. This is then metabolised to 7Î±-hydroxy-4-cholesten-3-one. There are multiple steps in bile acid synthesis requiring 14 enzymes in all.[3] These result in the junction between the first two steroid rings (A and B) being altered, making the molecule bent; in this process, the 3-hydroxyl is converted to the Î± orientation. The simplest 24-carbon bile acid has two hydroxyl groups at positions 3Î± and 7Î±. This is 3Î±,7Î±-dihydroxy-5Î²-cholan-24-oic acid, or, as more usually known, chenodeoxycholic acid. This bile acid was first isolated from the domestic goose, from which the "cheno" portion of the name was derived. The 5Î² in the name denotes the orientation of the junction between rings A and B of the steroid nucleus (in this case, they are bent). The term "cholan" denotes a particular steroid structure of 24 carbons, and the "24-oic acid" indicates that the carboxylic acid is found at position 24, at the end of the side-chain. Chenodeoxycholic acid is made by many species, and is the prototypic functional bile acid.[2][3]

An alternative (acidic) pathway of bile acid synthesis is initiated by mitochondrial sterol 27-hydroxylase (CYP27A1), expressed in liver, and also in macrophages and other tissues. CYP27A1 contributes significantly to total bile acid synthesis by catalyzing sterol side chain oxidation, after which cleavage of a three-carbon unit in the peroxisomes leads to formation of a C24 bile acid. Minor pathways initiated by 25-hydroxylase in the liver and 24-hydroxylase in the brain also may contribute to bile acid synthesis. 7Î±-hydroxylase (CYP7B1) generates oxysterols, which may be further converted in the liver to CDCA.[2][3]

Cholic acid, 3Î±,7Î±,12Î±-trihydroxy-5Î²-cholan-24-oic acid, the most abundant bile acid in humans and many other species, was discovered before chenodeoxycholic acid. It is a tri-hydroxy-bile acid with 3 hydroxyl groups (3Î±, 7Î± and 12Î±). In its synthesis in the liver, 12Î± hydroxylation is performed by the additional action of CYP8B1. As this had already been described, the discovery of chenodeoxcholic acid (with 2 hydroxyl groups) made this new bile acid a "deoxycholic acid" in that it had one fewer hydroxyl group than cholic acid.[2][3]

Deoxycholic acid is formed from cholic acid by 7-dehydroxylation, resulting in 2 hydroxyl groups (3Î± and 12Î±). This process with chenodeoxycholic acid results in a bile acid with only a 3Î± hydroxyl group, termed lithocholic acid (litho = stone) having been identified first in a gallstone from a calf. It is poorly water-soluble and rather toxic to cells.[2][3]

Different vertebrate families have evolved to use modifications of most positions on the steroid nucleus and side-chain of the bile acid structure. To avoid the problems associated with the production of lithocholic acid, most species add a third hydroxyl group to chenodeoxycholic acid. The subsequent removal of the 7Î± hydroxyl group by intestinal bacteria will then result in a less toxic but still-functional dihydroxy bile acid. Over the course of vertebrate evolution, a number of positions have been chosen for placement of the third hydroxyl group. Initially, the 16Î± position was favored, in particular in birds. Later, this position was superseded in a large number of species selecting the 12Î± position. Primates (including humans) utilize 12Î± for their third hydroxyl group position, producing cholic acid. In mice and other rodents, 6Î² hydroxylation forms muricholic acids (Î± or Î² depending on the 7 hydroxyl position). Pigs have 6Î± hydroxylation in hyocholic acid (3Î±,6Î±,7Î±-trihydroxy-5Î²-cholanoic acid), and other species have a hydroxyl group on position 23 of the side-chain.

Ursodeoxycholic acid was first isolated from bear bile, which has been used medicinally for centuries. Its structure resembles chenodeoxycholic acid but with the 7-hydroxyl group in the Î² position.[1]

Obeticholic acid, 6Î±-ethyl-chenodeoxycholic acid, is a semi-synthetic bile acid with greater activity as FXR agonist which is undergoing investigation as a pharmaceutical agent.

Hormonal actions
Bile acids also act as steroid hormones, secreted from the liver, absorbed from the intestine and having various direct metabolic actions in the body through the nuclear receptor Farnesoid X receptor (FXR), also known by its gene name NR1H4.[15][16][17] Another bile acid receptor is the cell membrane receptor known as G protein-coupled bile acid receptor 1 or TGR5. Many of their functions as signaling molecules in the liver and the intestines are by activating FXR, whereas TGR5 may be involved in metabolic, endocrine and neurological functions.[7]

Regulation of synthesis
As surfactants or detergents, bile acids are potentially toxic to cells, and so their concentrations are tightly regulated. Activation of FXR in the liver inhibits synthesis of bile acids, and is one mechanism of feedback control when bile acid levels are too high. Secondly, FXR activation by bile acids during absorption in the intestine increases transcription and synthesis of FGF19, which then inhibits bile acid synthesis in the liver.[18]

Metabolic functions
Emerging evidence associates FXR activation with alterations in triglyceride metabolism, glucose metabolism, and liver growth.[7][19]

Other interactions
Bile acids bind to some other proteins in addition to their hormone receptors (FXR and TGR5) and their transporters. Among these protein targets, the enzyme N-acyl phosphatidylethanolamine-specific phospholipase D (NAPE-PLD) generates bioactive lipid amides (e.g. the endogenous cannabinoid anandamide) that play important roles in several physiological pathways including stress and pain responses, appetite, and lifespan. NAPE-PLD orchestrates a direct cross-talk between lipid amide signals and bile acid physiology.[13]

Clinical significance
Hyperlipidemia
As bile acids are made from endogenous cholesterol, disruption of the enterohepatic circulation of bile acids will lower cholesterol. Bile acid sequestrants bind bile acids in the gut, preventing reabsorption. In so doing, more endogenous cholesterol is shunted into the production of bile acids, thereby lowering cholesterol levels. The sequestered bile acids are then excreted in the feces.[20]

Cholestasis
Tests for bile acids are useful in both human and veterinary medicine, as they aid in the diagnosis of a number of conditions, including types of cholestasis such as intrahepatic cholestasis of pregnancy, portosystemic shunt, and hepatic microvascular dysplasia in dogs.[21] Structural or functional abnormalities of the biliary system result in an increase in bilirubin (jaundice) and in bile acids in the blood. Bile acids are related to the itching (pruritus) which is common in cholestatic conditions such as primary biliary cirrhosis (PBC), primary sclerosing cholangitis or intrahepatic cholestasis of pregnancy.[22] Treatment with ursodeoxycholic acid has been used for many years in these cholestatic disorders.[23][24]

Gallstones
Main article: Gallstones
The relationship of bile acids to cholesterol saturation in bile and cholesterol precipitation to produce gallstones has been studied extensively. Gallstones may result from increased saturation of cholesterol or bilirubin, or from bile stasis. Lower concentrations of bile acids or phospholipids in bile reduce cholesterol solubility and lead to microcrystal formation. Oral therapy with chenodeoxycholic acid and/or ursodeoxycholic acid has been used to dissolve cholesterol gallstones.[25][26][27] Stones may recur when treatment is stopped. Bile acid therapy may be of value to prevent stones in certain circumstances such as following bariatric surgery.[28]

Bile acid diarrhea
Excess concentrations of bile acids in the colon are a cause of chronic diarrhea. It is commonly found when the ileum is abnormal or has been surgically removed, as in Crohn's disease, or cause a condition that resembles diarrhea-predominant irritable bowel syndrome (IBS-D). This condition of bile acid diarrhea/bile acid malabsorption can be diagnosed by the SeHCAT test and treated with bile acid sequestrants.[29]

Bile acids and colon cancer
Bile acids may have some importance in the development of colorectal cancer.[30] Deoxycholic acid (DCA) is increased in the colonic contents of humans in response to a high fat diet.[31] In populations with a high incidence of colorectal cancer, fecal concentrations of bile acids are higher,[32][33] and this association suggests that increased colonic exposure to bile acids could play a role in the development of cancer. In one particular comparison, the fecal DCA concentrations in Native Africans in South Africa (who eat a low fat diet) compared to African Americans (who eat a higher fat diet) was 7.30 vs. 37.51 nmol/g wet weight stool.[34] Native Africans in South Africa have a low incidence rate of colon cancer of less than 1:100,000,[35] compared to the high incidence rate for male African Americans of 72:100,000.[36]

Experimental studies also suggest mechanisms for bile acids in colon cancer. Exposure of colonic cells to high DCA concentrations increase formation of reactive oxygen species, causing oxidative stress, and also increase DNA damage.[37] Mice fed a diet with added DCA mimicking colonic DCA levels in humans on a high fat diet developed colonic neoplasia, including adenomas and adenocarcinomas (cancers), unlike mice fed a control diet producing one-tenth the level of colonic DCA who had no colonic neoplasia.[38][39]

The effects of ursodeoxycholic acid (UDCA) in modifying the risk of colorectal cancer has been looked at in several studies, particularly in primary sclerosing cholangitis and inflammatory bowel disease, with varying results partly related to dosage.[40][41] Genetic variation in the key bile acid synthesis enzyme, CYP7A1, influenced the effectiveness of UDCA in colorectal adenoma prevention in a large trial.[42]

Dermatology
Bile acids may be used in subcutaneous injections to remove unwanted fat (see Mesotherapy). Deoxycholic acid as an injectable has received FDA approval to dissolve submental fat.[43] Phase III trials showed significant responses although many subjects had mild adverse reactions of bruising, swelling, pain, numbness, erythema, and firmness around the treated area.[44][45]

References
 Hofmann AF, Hagey LR, Krasowski MD (February 2010). "Bile salts of vertebrates: structural variation and possible evolutionary significance". J. Lipid Res. 51 (2): 226â46. doi:10.1194/jlr.R000042. PMC 2803226. PMID 19638645.
 Russell DW (2003). "The enzymes, regulation, and genetics of bile acid synthesis". Annu. Rev. Biochem. 72: 137â74. doi:10.1146/annurev.biochem.72.121801.161712. PMID 12543708.
 Chiang JY (October 2009). "Bile acids: regulation of synthesis". J. Lipid Res. 50 (10): 1955â66. doi:10.1194/jlr.R900010-JLR200. PMC 2739756. PMID 19346330.
 Carey, MC.; Small, DM. (Oct 1972). "Micelle formation by bile salts. Physical-chemical and thermodynamic considerations". Arch Intern Med. 130 (4): 506â27. doi:10.1001/archinte.1972.03650040040005. PMID 4562149.
 Hofmann AF (1999). "The continuing importance of bile acids in liver and intestinal disease". Arch. Intern. Med. 159 (22): 2647â58. doi:10.1001/archinte.159.22.2647. PMID 10597755.
 Hofmann AF, BorgstrÃ¶m B (February 1964). "The intraluminal phase of fat digestion in man: the lipid content of the micellar and oil phases of intestinal content obtained during fat digestion and absorption". J. Clin. Invest. 43 (2): 247â57. doi:10.1172/JCI104909. PMC 289518. PMID 14162533.
 Fiorucci S, Mencarelli A, Palladino G, Cipriani S (November 2009). "Bile-acid-activated receptors: targeting TGR5 and farnesoid-X-receptor in lipid and glucose disorders". Trends Pharmacol. Sci. 30 (11): 570â80. doi:10.1016/j.tips.2009.08.001. PMID 19758712.
 'Essentials of Medical Biochemistry, Lieberman, Marks and Smith, eds, p432, 2007'
 Hofmann AF (October 1963). "The function of bile salts in fat absorption. The solvent properties of dilute micellar solutions of conjugated bile salts". Biochem. J. 89: 57â68. doi:10.1042/bj0890057. PMC 1202272. PMID 14097367.
 Li T, Chiang JY (2014). "Bile acid signaling in metabolic disease and drug therapy". Pharmacol. Rev. 66 (4): 948â83. doi:10.1124/pr.113.008201. PMC 4180336. PMID 25073467.
 Nagahashi M, Takabe K, Liu R, Peng K, Wang X, Wang Y, Hait NC, Wang X, Allegood JC, Yamada A, Aoyagi T, Liang J, Pandak WM, Spiegel S, Hylemon PB, Zhou H (2015). "Conjugated bile acid-activated S1P receptor 2 is a key regulator of sphingosine kinase 2 and hepatic gene expression". Hepatology. 61 (4): 1216â26. doi:10.1002/hep.27592. PMC 4376566. PMID 25363242.
 Wiemuth D, Sahin H, Falkenburger BH, Lefevre CM, Wasmuth HE, Grunder S (2012). "BASIC--a bile acid-sensitive ion channel highly expressed in bile ducts". FASEB J. 26 (10): 4122â30. doi:10.1096/fj.12-207043. PMID 22735174.
 Magotti P, Bauer I, Igarashi M, Babagoli M, Marotta R, Piomelli D, Garau G (2015). "Structure of Human N-Acylphosphatidylethanolamine-Hydrolyzing Phospholipase D: Regulation of Fatty Acid Ethanolamide Biosynthesis by Bile Acids". Structure. 23 (3): 598â604. doi:10.1016/j.str.2014.12.018. PMC 4351732. PMID 25684574.
 Margheritis, E, Castellani B, Magotti P, Peruzzi S, Romeo E, Natali F, Mostarda S, Gioiello A, Piomelli D, Garau G (2016). "Bile Acid Recognition by NAPE-PLD". ACS Chemical Biology. 11 (10): 2908â2914. doi:10.1021/acschembio.6b00624. PMC 5074845. PMID 27571266.
 Makishima M, Okamoto AY, Repa JJ, et al. (May 1999). "Identification of a nuclear receptor for bile acids". Science. 284 (5418): 1362â5. Bibcode:1999Sci...284.1362M. doi:10.1126/science.284.5418.1362. PMID 10334992.
 Parks DJ, Blanchard SG, Bledsoe RK, et al. (May 1999). "Bile acids: natural ligands for an orphan nuclear receptor". Science. 284 (5418): 1365â8. Bibcode:1999Sci...284.1365P. doi:10.1126/science.284.5418.1365. PMID 10334993.
 Wang H, Chen J, Hollister K, Sowers LC, Forman BM (May 1999). "Endogenous bile acids are ligands for the nuclear receptor FXR/BAR". Mol. Cell. 3 (5): 543â53. doi:10.1016/s1097-2765(00)80348-2. PMID 10360171.
 Kim, I; Ahn, SH; Inagaki, T; Choi, M; Ito, S; Guo, GL; Kliewer, SA; Gonzalez, FJ (2007). "Differential regulation of bile acid homeostasis by the farnesoid X receptor in liver and intestine". Journal of Lipid Research. 48 (12): 2664â72. doi:10.1194/jlr.M700330-JLR200. PMID 17720959.
 Shapiro, Hagit; Kolodziejczyk, Aleksandra A.; Halstuch, Daniel; Elinav, Eran (2018-01-16). "Bile acids in glucose metabolism in health and disease". Journal of Experimental Medicine. 215 (2): 383â396. doi:10.1084/jem.20171965. ISSN 0022-1007. PMC 5789421. PMID 29339445.
 Davidson MH (2011). "A systematic review of bile acid sequestrant therapy in children with familial hypercholesterolemia". J Clin Lipidol. 5 (2): 76â81. doi:10.1016/j.jacl.2011.01.005. PMID 21392720.
 Allen L, Stobie D, Mauldin GN, Baer KE (January 1999). "Clinicopathologic features of dogs with hepatic microvascular dysplasia with and without portosystemic shunts: 42 cases (1991-1996)". J. Am. Vet. Med. Assoc. 214 (2): 218â20. PMID 9926012.
 Pusl T, Beuers U (2007). "Intrahepatic cholestasis of pregnancy". Orphanet J Rare Dis. 2: 26. doi:10.1186/1750-1172-2-26. PMC 1891276. PMID 17535422.
 Poupon RE, Balkau B, EschwÃ¨ge E, Poupon R (May 1991). "A multicenter, controlled trial of ursodiol for the treatment of primary biliary cirrhosis. UDCA-PBC Study Group". N. Engl. J. Med. 324 (22): 1548â54. doi:10.1056/NEJM199105303242204. PMID 1674105.
 Glantz A, Marschall HU, Lammert F, Mattsson LA (December 2005). "Intrahepatic cholestasis of pregnancy: a randomized controlled trial comparing dexamethasone and ursodeoxycholic acid". Hepatology. 42 (6): 1399â405. doi:10.1002/hep.20952. PMID 16317669.
 Danzinger RG, Hofmann AF, Schoenfield LJ, Thistle JL (January 1972). "Dissolution of cholesterol gallstones by chenodeoxycholic acid". N. Engl. J. Med. 286 (1): 1â8. doi:10.1056/NEJM197201062860101. PMID 5006919.
 Thistle JL, Hofmann AF (September 1973). "Efficacy and specificity of chenodeoxycholic acid therapy for dissolving gallstones". N. Engl. J. Med. 289 (13): 655â9. doi:10.1056/NEJM197309272891303. PMID 4580472.
 Petroni ML, Jazrawi RP, Pazzi P, et al. (January 2001). "Ursodeoxycholic acid alone or with chenodeoxycholic acid for dissolution of cholesterol gallstones: a randomized multicentre trial. The British-Italian Gallstone Study group". Aliment. Pharmacol. Ther. 15 (1): 123â8. doi:10.1046/j.1365-2036.2001.00853.x. PMID 11136285.
 Uy MC, Talingdan-Te MC, Espinosa WZ, Daez ML, Ong JP (December 2008). "Ursodeoxycholic acid in the prevention of gallstone formation after bariatric surgery: a meta-analysis". Obes Surg. 18 (12): 1532â8. doi:10.1007/s11695-008-9587-7. PMID 18574646. S2CID 207302960.
 Pattni, S; Walters, JR (2009). "Recent advances in the understanding of bile acid malabsorption". British Medical Bulletin. 92: 79â93. doi:10.1093/bmb/ldp032. PMID 19900947.
 Degirolamo C, Modica S, Palasciano G, Moschetta A (2011). "Bile acids and colon cancer: Solving the puzzle with nuclear receptors". Trends Mol Med. 17 (10): 564â72. doi:10.1016/j.molmed.2011.05.010. PMID 21724466.
 Reddy BS, Hanson D, Mangat S, et al. (September 1980). "Effect of high-fat, high-beef diet and of mode of cooking of beef in the diet on fecal bacterial enzymes and fecal bile acids and neutral sterols". J. Nutr. 110 (9): 1880â7. doi:10.1093/jn/110.9.1880. PMID 7411244.
 Hill MJ (May 1990). "Bile flow and colon cancer". Mutat. Res. 238 (3): 313â20. doi:10.1016/0165-1110(90)90023-5. PMID 2188127.
 Cheah PY (1990). "Hypotheses for the etiology of colorectal cancer--an overview". Nutr Cancer. 14 (1): 5â13. doi:10.1080/01635589009514073. PMID 2195469.
 Ou J, DeLany JP, Zhang M, Sharma S, O'Keefe SJ (2012). "Association between low colonic short-chain fatty acids and high bile acids in high colon cancer risk populations". Nutr Cancer. 64 (1): 34â40. doi:10.1080/01635581.2012.630164. PMC 6844083. PMID 22136517.
 O'Keefe SJ, Kidd M, Espitalier-Noel G, Owira P (May 1999). "Rarity of colon cancer in Africans is associated with low animal product consumption, not fiber". Am. J. Gastroenterol. 94 (5): 1373â80. PMID 10235221.
 American Cancer Society. Cancer Facts and Figures 2009. http://www.cancer.org/Research/CancerFactsFigures/cancer-facts-figures-2009
 Bernstein H, Bernstein C, Payne CM, Dvorak K (July 2009). "Bile acids as endogenous etiologic agents in gastrointestinal cancer". World J. Gastroenterol. 15 (27): 3329â40. doi:10.3748/wjg.15.3329. PMC 2712893. PMID 19610133.
 Bernstein C, Holubec H, Bhattacharyya AK, et al. (August 2011). "Carcinogenicity of deoxycholate, a secondary bile acid". Arch. Toxicol. 85 (8): 863â71. doi:10.1007/s00204-011-0648-7. PMC 3149672. PMID 21267546.
 Prasad AR, Prasad S, Nguyen H, Facista A, Lewis C, Zaitlin B, Bernstein H, Bernstein C (Jul 2014). "Novel diet-related mouse model of colon cancer parallels human colon cancer". World J Gastrointest Oncol. 6 (7): 225â43. doi:10.4251/wjgo.v6.i7.225. PMC 4092339. PMID 25024814.
 Singh S, Khanna S, Pardi DS, Loftus EV, Talwalkar JA (2013). "Effect of ursodeoxycholic acid use on the risk of colorectal neoplasia in patients with primary sclerosing cholangitis and inflammatory bowel disease: a systematic review and meta-analysis". Inflamm. Bowel Dis. 19 (8): 1631â8. doi:10.1097/MIB.0b013e318286fa61. PMID 23665966. S2CID 39918727.
 Eaton JE, Silveira MG, Pardi DS, Sinakos E, Kowdley KV, Luketic VA, Harrison ME, McCashland T, Befeler AS, Harnois D, Jorgensen R, Petz J, Lindor KD (2011). "High-dose ursodeoxycholic acid is associated with the development of colorectal neoplasia in patients with ulcerative colitis and primary sclerosing cholangitis". Am. J. Gastroenterol. 106 (9): 1638â45. doi:10.1038/ajg.2011.156. PMC 3168684. PMID 21556038.
 Wertheim BC, Smith JW, Fang C, Alberts DS, Lance P, Thompson PA (2012). "Risk modification of colorectal adenoma by CYP7A1 polymorphisms and the role of bile acid metabolism in carcinogenesis". Cancer Prev Res (Phila). 5 (2): 197â204. doi:10.1158/1940-6207.CAPR-11-0320. PMC 3400261. PMID 22058145.
 "Deoxycholic acid injection". Medline plus. Retrieved 26 August 2015.
 Ascher B, Hoffmann K, Walker P, Lippert S, Wollina U, Havlickova B (2014). "Efficacy, patient-reported outcomes and safety profile of ATX-101 (deoxycholic acid), an injectable drug for the reduction of unwanted submental fat: results from a phase III, randomized, placebo-controlled study". J Eur Acad Dermatol Venereol. 28 (12): 1707â15. doi:10.1111/jdv.12377. PMC 4263247. PMID 24605812.
 Wollina U, Goldman A (2015). "ATX-101 for reduction of submental fat". Expert Opin Pharmacother. 16 (5): 755â62. doi:10.1517/14656566.2015.1019465. PMID 25724831. S2CID 23094631.
External links
Bile+Acids+and+Salts at the US National Library of Medicine Medical Subject Headings (MeSH)
Special Issue on "Bile Acids"
vte
Steroid classification
vte
Bile and liver therapy (A05)
vte
FXR and LXR modulators
vte
Xenobiotic-sensing receptor modulators
Authority control Edit this at Wikidata
GND: 4155851-0LCCN: sh85014008LNB: 000294980
Issoria lathonia.jpgBiology portalWHO Rod.svgMedicine portal
Categories: Bile acidsHepatology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
21 more
Edit links
This page was last edited on 31 August 2020, at 09:45 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Surfactant
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
See also: Pulmonary surfactant
	Look up surfactant in Wiktionary, the free dictionary.

Schematic diagram of a micelle of oil in aqueous suspension, such as might occur in an emulsion of oil in water. In this example, the surfactant molecules' oil-soluble tails project into the oil (blue), while the water-soluble ends remain in contact with the water phase (red).
Surfactants are molecules that spontaneously bond with each other to form sealed bubbles.[1] Surfactants are compounds that lower the surface tension (or interfacial tension) between two liquids, between a gas and a liquid, or between a liquid and a solid. Surfactants may act as detergents, wetting agents, emulsifiers, foaming agents, or dispersants.

The word "surfactant" is a blend of surface-active agent,[2] coined c.â 1950.[3]

Agents that increase surface tension are "surface active" in the literal sense but are not called surfactants since their effect is opposite to the common meaning. A common example[of what?] is salting out: by adding an inorganic salt to an aqueous solution of a weakly polar substance, the substance will precipitate. The substance may itself be a surfactant - this is one of the reasons why many surfactants are ineffective in sea water.


Contents
1	Composition and structure
1.1	Structure of surfactant phases in water
1.2	Dynamics of surfactants at interfaces
1.3	Characterization of interfaces and surfactant layers
2	Surfactants in biology
3	Safety and environmental risks
3.1	Biodegradation
4	Applications
4.1	Detergents in biochemistry and biotechnology
4.2	Quantum dot preparation
4.3	Surfactants in droplet-based microfluidics
5	Classification
5.1	Anionic: sulfate, sulfonate, and phosphate, carboxylate derivatives
5.2	Cationic head groups
5.3	Zwitterionic surfactants
5.4	Non-ionic
5.4.1	Ethoxylates
5.4.2	Fatty acid esters of polyhydroxy compounds
6	See also
7	References
8	External links

Schematic diagram of a micelle â the lipophilic tails of the surfactant ions remain inside the oil because they interact more strongly with oil than with water. The polar "heads" of the surfactant molecules coating the micelle interact more strongly with water, so they form a hydrophilic outer layer that forms a barrier between micelles. This inhibits the oil droplets, the hydrophobic cores of micelles, from merging into fewer, larger droplets ("emulsion breaking") of the micelle. The compounds that coat a micelle are typically amphiphilic in nature, meaning that micelles may be stable either as droplets of aprotic solvents such as oil in water, or as protic solvents such as water in oil. When the droplet is aprotic it is sometimes[when?] known as a reverse micelle.
Composition and structure
Surfactants are usually organic compounds that are amphiphilic, meaning they contain both hydrophobic groups (their tails) and hydrophilic groups (their heads).[4] Therefore, a surfactant contains both a water-insoluble (or oil-soluble) component and a water-soluble component. Surfactants will diffuse in water and adsorb at interfaces between air and water or at the interface between oil and water, in the case where water is mixed with oil. The water-insoluble hydrophobic group may extend out of the bulk water phase, into the air or into the oil phase, while the water-soluble head group remains in the water phase.

World production of surfactants is estimated at 15 million tons per year, of which about half are soaps. Other surfactants produced on a particularly large scale are linear alkylbenzene sulfonates (1.7 million tons/y), lignin sulfonates (600,000 tons/y), fatty alcohol ethoxylates (700,000 tons/y), and alkylphenol ethoxylates (500,000 tons/y).[5]


Sodium stearate, the most common component of most soap, which comprises about 50% of commercial surfactants

4-(5-Dodecyl) benzenesulfonate, a linear dodecylbenzenesulfonate, one of the most common surfactants
Structure of surfactant phases in water
In the bulk aqueous phase, surfactants form aggregates, such as micelles, where the hydrophobic tails form the core of the aggregate and the hydrophilic heads are in contact with the surrounding liquid. Other types of aggregates can also be formed, such as spherical or cylindrical micelles or lipid bilayers. The shape of the aggregates depends on the chemical structure of the surfactants, namely the balance in size between the hydrophilic head and hydrophobic tail. A measure of this is the hydrophilic-lipophilic balance (HLB). Surfactants reduce the surface tension of water by adsorbing at the liquid-air interface. The relation that links the surface tension and the surface excess is known as the Gibbs isotherm.

Dynamics of surfactants at interfaces
The dynamics of surfactant adsorption is of great importance for practical applications such as in foaming, emulsifying or coating processes, where bubbles or drops are rapidly generated and need to be stabilized. The dynamics of absorption depend on the diffusion coefficient of the surfactant. As the interface is created, the adsorption is limited by the diffusion of the surfactant to the interface. In some cases, there can exist an energetic barrier to adsorption or desorption of the surfactant. If such a barrier limits the adsorption rate, the dynamics are said to be âkinetically limited'. Such energy barriers can be due to steric or electrostatic repulsions. The surface rheology of surfactant layers, including the elasticity and viscosity of the layer, play an important role in the stability of foams and emulsions.

Characterization of interfaces and surfactant layers
Interfacial and surface tension can be characterized by classical methods such as the -pendant or spinning drop method. Dynamic surface tensions, i.e. surface tension as a function of time, can be obtained by the maximum bubble pressure apparatus

The structure of surfactant layers can be studied by ellipsometry or X-ray reflectivity.

Surface rheology can be characterized by the oscillating drop method or shear surface rheometers such as double-cone, double-ring or magnetic rod shear surface rheometer.

Surfactants in biology

Phosphatidylcholine, found in lecithin, is a pervasive biological surfactant. Shown in red â choline and phosphate group; black â glycerol; green â monounsaturated fatty acid; blue â saturated fatty acid.
The human body produces diverse surfactants. Pulmonary surfactant is produced in the lungs in order to facilitate breathing by increasing total lung capacity, and lung compliance. In respiratory distress syndrome or RDS, surfactant replacement therapy helps patients have normal respiration by using pharmaceutical forms of the surfactants. One example of pharmaceutical pulmonary surfactants is Survanta (beractant) or its generic form Beraksurf produced by Abbvie and Tekzima respectively. Bile salts, a surfactant produced in the liver, play an important role in digestion.[6]

Safety and environmental risks
Most anionic and non-ionic surfactants are non-toxic, having LD50 comparable to sodium chloride. The toxicity of quaternary ammonium compounds, which are antibacterial and antifungal, varies. Dialkyldimethylammonium chlorides (DDAC, DSDMAC) used as fabric softeners have low LD50 (5 g/kg) and are essentially non-toxic, while the disinfectant alkylbenzyldimethylammonium chloride has an LD50 of 0.35 g/kg. Prolonged exposure to surfactants can irritate and damage the skin because surfactants disrupt the lipid membrane that protects skin and other cells. Skin irritancy generally increases in the series non-ionic, amphoteric, anionic, cationic surfactants.[5]

Surfactants are routinely deposited in numerous ways on land and into water systems, whether as part of an intended process or as industrial and household waste.[7][8][9]

Anionic surfactants can be found in soils as the result of sewage sludge application, wastewater irrigation, and remediation processes. Relatively high concentrations of surfactants together with multimetals can represent an environmental risk. At low concentrations, surfactant application is unlikely to have a significant effect on trace metal mobility.[10][11]

In the case of the Deepwater Horizon oil spill, unprecedented amounts of Corexit were sprayed directly into the ocean at the leak and on the sea-water's surface. The apparent theory being that the surfactants isolate droplets of oil, making it easier for petroleum-consuming microbes to digest the oil. The active ingredient in Corexit is dioctyl sodium sulfosuccinate (DOSS), sorbitan monooleate (Span 80), and polyoxyethylenated sorbitan monooleate (Tween-80).[12][13]

Biodegradation
Because of the volume of surfactants released into the environment, their biodegradation is of great interest. Strategies to enhance degradation include ozone treatment and biodegradation.[14][15] Two major surfactants, linear alkylbenzene sulfonates (LAS) and the alkyl phenol ethoxylates (APE) break down under aerobic conditions found in sewage treatment plants and in soil to nonylphenol, which is thought to be an endocrine disruptor.[16][17] Interest in biodegradable surfactants has led to much interest in "biosurfactants" such as those derived from amino acids.[18]

Attracting much attention is the non-biodegradability of fluorosurfactant, e.g. perfluorooctanoic acid (PFOA).[19]

Applications
The annual global production of surfactants was 13 million tonnes in 2008.[20] In 2014, the world market for surfactants reached a volume of more than US$33 billion. Market researchers expect annual revenues to increase by 2.5% per year to around $40.4 billion until 2022. The commercially most significant type of surfactants is currently the anionic surfactant LAS, which is widely used in cleaners and detergents.[21]

Surfactants play an important role as cleaning, wetting, dispersing, emulsifying, foaming and anti-foaming agents in many practical applications and products, including detergents, fabric softeners, motor oils, emulsions, soaps, paints, adhesives, inks, anti-fogs, ski waxes, snowboard wax, deinking of recycled papers, in flotation, washing and enzymatic processes, and laxatives. Also agrochemical formulations such as herbicides (some), insecticides, biocides (sanitizers), and spermicides (nonoxynol-9).[22] Personal care products such as cosmetics, shampoos, shower gel, hair conditioners, and toothpastes. Surfactants are used in firefighting and pipelines (liquid drag reducing agents). Alkali surfactant polymers are used to mobilize oil in oil wells.

The displacement of air from the matrix of cotton pads and bandages so that medicinal solutions can be absorbed for application to various body areas; the displacement of dirt and debris by the use of detergents in the washing of wounds;[23] and the application of medicinal lotions and sprays to surface of skin and mucous membranes.[24]

Detergents in biochemistry and biotechnology
In solution, detergents help solubilize a variety of chemical species by dissociating aggregates and unfolding proteins. Popular surfactants in the biochemistry laboratory are sodium lauryl sulfate (SDS) and cetyl trimethylammonium bromide (CTAB). Detergents are key reagents to extract protein by lysis of the cells and tissues: They disorganize the membrane's lipid bilayer (SDS, Triton X-100, X-114, CHAPS, DOC, and NP-40), and solubilize proteins. Milder detergents such as octyl thioglucoside, octyl glucoside or dodecyl maltoside are used to solubilize membrane proteins such as enzymes and receptors without denaturing them. Non-solubilized material is harvested by centrifugation or other means. For electrophoresis, for example, proteins are classically treated with SDS to denature the native tertiary and quaternary structures, allowing the separation of proteins according to their molecular weight.

Detergents have also been used to decellularise organs. This process maintains a matrix of proteins that preserves the structure of the organ and often the microvascular network. The process has been successfully used to prepare organs such as the liver and heart for transplant in rats.[25] Pulmonary surfactants are also naturally secreted by type II cells of the lung alveoli in mammals.

Quantum dot preparation
Surfactants are used with quantum dots in order to manipulate the growth,[26] assembly, and electrical properties of quantum dots, in addition to mediating reactions on their surfaces. Research is ongoing in how surfactants arrange on the surface of the quantum dots.[27]

Surfactants in droplet-based microfluidics
Surfactants play an important role in droplet-based microfluidics in the stabilization of the droplets, and the prevention of the fusion of droplets during incubation.[28]

Classification
The "tails" of most surfactants are fairly similar, consisting of a hydrocarbon chain, which can be branched, linear, or aromatic. Fluorosurfactants have fluorocarbon chains. Siloxane surfactants have siloxane chains.

Many important surfactants include a polyether chain terminating in a highly polar anionic group. The polyether groups often comprise ethoxylated (polyethylene oxide-like) sequences inserted to increase the hydrophilic character of a surfactant. Polypropylene oxides conversely, may be inserted to increase the lipophilic character of a surfactant.

Surfactant molecules have either one tail or two; those with two tails are said to be double-chained.


Surfactant classification according to the composition of their head: non-ionic, anionic, cationic, amphoteric.
Most commonly, surfactants are classified according to polar head group. A non-ionic surfactant has no charged groups in its head. The head of an ionic surfactant carries a net positive, or negative charge. If the charge is negative, the surfactant is more specifically called anionic; if the charge is positive, it is called cationic. If a surfactant contains a head with two oppositely charged groups, it is termed zwitterionic. Commonly encountered surfactants of each type include:

Anionic: sulfate, sulfonate, and phosphate, carboxylate derivatives
Anionic surfactants contain anionic functional groups at their head, such as sulfate, sulfonate, phosphate, and carboxylates. Prominent alkyl sulfates include ammonium lauryl sulfate, sodium lauryl sulfate (sodium dodecyl sulfate, SLS, or SDS), and the related alkyl-ether sulfates sodium laureth sulfate (sodium lauryl ether sulfate or SLES), and sodium myreth sulfate.

Others include:

Docusate (dioctyl sodium sulfosuccinate)
Perfluorooctanesulfonate (PFOS)
Perfluorobutanesulfonate
Alkyl-aryl ether phosphates
Alkyl ether phosphates
Carboxylates are the most common surfactants and comprise the carboxylate salts (soaps), such as sodium stearate. More specialized species include sodium lauroyl sarcosinate and carboxylate-based fluorosurfactants such as perfluorononanoate, perfluorooctanoate (PFOA or PFO).

Cationic head groups
pH-dependent primary, secondary, or tertiary amines; primary and secondary amines become positively charged at pH < 10:[29] octenidine dihydrochloride.

Permanently charged quaternary ammonium salts: cetrimonium bromide (CTAB), cetylpyridinium chloride (CPC), benzalkonium chloride (BAC), benzethonium chloride (BZT), dimethyldioctadecylammonium chloride, and dioctadecyldimethylammonium bromide (DODAB).

Zwitterionic surfactants
Zwitterionic (amphoteric) surfactants have both cationic and anionic centers attached to the same molecule. The cationic part is based on primary, secondary, or tertiary amines or quaternary ammonium cations. The anionic part can be more variable and include sulfonates, as in the sultaines CHAPS (3-[(3-cholamidopropyl)dimethylammonio]-1-propanesulfonate) and cocamidopropyl hydroxysultaine. Betaines such as cocamidopropyl betaine have a carboxylate with the ammonium. The most common biological zwitterionic surfactants have a phosphate anion with an amine or ammonium, such as the phospholipids phosphatidylserine, phosphatidylethanolamine, phosphatidylcholine, and sphingomyelins.

Lauryldimethylamine oxide and myristamine oxide are two commonly used zwitterionic surfactants of the tertiary amine oxides structural type.

Non-ionic
Non-ionic surfactants have covalently bonded oxygen-containing hydrophilic groups, which are bonded to hydrophobic parent structures. The water-solubility of the oxygen groups is the result of hydrogen bonding. Hydrogen bonding decreases with increasing temperature, and the water solubility of non-ionic surfactants therefore decreases with increasing temperature.

Non-ionic surfactants are less sensitive to water hardness than anionic surfactants, and they foam less strongly. The differences between the individual types of non-ionic surfactants are slight, and the choice is primarily governed having regard to the costs of special properties (e.g., effectiveness and efficiency, toxicity, dermatological compatibility, biodegradability) or permission for use in food.[5]

Ethoxylates
Fatty alcohol ethoxylates
Narrow-range ethoxylate
Octaethylene glycol monododecyl ether
Pentaethylene glycol monododecyl ether
Alkylphenol ethoxylates (APEs or APEOs)
Nonoxynols
Triton X-100
Fatty acid ethoxylates
Fatty acid ethoxylates are a class of very versatile surfactants, which combine in a single molecule the characteristic of a weakly anionic, pH-responsive head group with the presence of stabilizing and temperature responsive ethyleneoxide units.[30]

Special ethoxylated fatty esters and oils
Ethoxylated amines and/or fatty acid amides
Polyethoxylated tallow amine
Cocamide monoethanolamine
Cocamide diethanolamine
Terminally blocked ethoxylates
Poloxamers
Fatty acid esters of polyhydroxy compounds
Fatty acid esters of glycerol
Glycerol monostearate
Glycerol monolaurate
Fatty acid esters of sorbitol
Spans:

Sorbitan monolaurate
Sorbitan monostearate
Sorbitan tristearate
Tweens:

Tween 20
Tween 40
Tween 60
Tween 80
Fatty acid esters of sucrose
Alkyl polyglucosides
Main article: Alkyl polyglycoside
Decyl glucoside
Lauryl glucoside
Octyl glucoside
See also
	Chemistry portal
	Underwater diving portal
Anti-fog â Chemicals that prevent the condensation of water as small droplets on a surface
Cleavable detergent
Emulsion â Mixture of two or more liquids that are generally immiscible
Hydrotrope
MBAS assay, an assay that indicates anionic surfactants in water with a bluing reaction.
Niosome
Oil dispersants
Surfactants in paint
References
 [1] NATURE | A New Chemical 'Tree of The Origins of Life' Reveals Our Possible Molecular Evolution | MICHELLE STARR | 3 OCTOBER 2020
 Rosen MJ, Kunjappu JT (2012). Surfactants and Interfacial Phenomena (4th ed.). Hoboken, New Jersey: John Wiley & Sons. p. 1. ISBN 978-1-118-22902-6. Archived from the original on 8 January 2017. A surfactant (a contraction of surface-active agent) is a substance that, when present at low concentration in a system. has the property of adsorbing onto the surfaces or interfaces of the system and of altering to a marked degree the surface or interfacial free energies of those surfaces (or interfaces).
 "surfactant". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.) - "A new word, Surfactants, has been coined by Antara Products, General Aniline & Film Corporation, and has been presented to the chemical industry to cover all materials that have surface activity, including wetting agents, dispersants, emulsifiers, detergents and foaming agents."
 "Bubbles, Bubbles, Everywhere, But Not a Drop to Drink". The Lipid Chronicles. 11 November 2011. Archived from the original on 26 April 2012. Retrieved 1 August 2012.
 Kurt Kosswig "Surfactants" in Ullmann's Encyclopedia of Industrial Chemistry, Wiley-VCH, 2005, Weinheim. doi:10.1002/14356007.a25_747
 Maldonado-Valderrama, Julia; Wilde, Pete; MacIerzanka, Adam; MacKie, Alan (2011). "The role of bile salts in digestion". Advances in Colloid and Interface Science. 165 (1): 36â46. doi:10.1016/j.cis.2010.12.002. PMID 21236400.
 Metcalfe TL, Dillon PJ, Metcalfe CD (April 2008). "Detecting the transport of toxic pesticides from golf courses into watersheds in the Precambrian Shield region of Ontario, Canada". Environ. Toxicol. Chem. 27 (4): 811â8. doi:10.1897/07-216.1. PMID 18333674.
 Emmanuel E, Hanna K, Bazin C, Keck G, ClÃ©ment B, Perrodin Y (April 2005). "Fate of glutaraldehyde in hospital wastewater and combined effects of glutaraldehyde and surfactants on aquatic organisms". Environ Int. 31 (3): 399â406. doi:10.1016/j.envint.2004.08.011. PMID 15734192.
 Murphy MG, Al-Khalidi M, Crocker JF, Lee SH, O'Regan P, Acott PD (April 2005). "Two formulations of the industrial surfactant, Toximul, differentially reduce mouse weight gain and hepatic glycogen in vivo during early development: effects of exposure to Influenza B Virus". Chemosphere. 59 (2): 235â46. Bibcode:2005Chmsp..59..235M. doi:10.1016/j.chemosphere.2004.11.084. PMID 15722095.
 HernÃ¡ndez-Soriano Mdel C, Degryse F, Smolders E (March 2011). "Mechanisms of enhanced mobilisation of trace metals by anionic surfactants in soil". Environ. Pollut. 159 (3): 809â16. doi:10.1016/j.envpol.2010.11.009. PMID 21163562.
 HernÃ¡ndez-Soriano Mdel C, PeÃ±a A, Dolores Mingorance M (2010). "Release of metals from metal-amended soil treated with a sulfosuccinamate surfactant: effects of surfactant concentration, soil/solution ratio, and pH". J. Environ. Qual. 39 (4): 1298â305. doi:10.2134/jeq2009.0242. PMID 20830918.
 "European Maritime Safety Agency. Manual on the Applicability of Oil Dispersants; Version 2; 2009". Archived from the original on 5 July 2011. Retrieved 19 May 2017.
 Committee on Effectiveness of Oil Spill Dispersants (National Research Council Marine Board) (1989). "Using Oil Spill Dispersants on the Sea". National Academies Press. Retrieved 31 October 2015.
 Rebello, Sharrel; Asok, Aju K.; Mundayoor, Sathish; Jisha, M. S. (2014). "Surfactants: Toxicity, remediation and green surfactants". Environmental Chemistry Letters. 12 (2): 275â287. doi:10.1007/s10311-014-0466-2.
 Ying, Guang-Guo (2006). "Fate, behavior and effects of surfactants and their degradation products in the environment". Environment International. 32 (3): 417â431. doi:10.1016/j.envint.2005.07.004. PMID 16125241.
 Mergel, Maria. "Nonylphenol and Nonylphenol Ethoxylates." Toxipedia.org. N.p., 1 Nov. 2011. Web. 27 Apr. 2014.
 Scott MJ, Jones MN (November 2000). "The biodegradation of surfactants in the environment". Biochim. Biophys. Acta. 1508 (1â2): 235â51. doi:10.1016/S0304-4157(00)00013-7. PMID 11090828.
 Reznik GO, Vishwanath P, Pynn MA, Sitnik JM, Todd JJ, Wu J, et al. (May 2010). "Use of sustainable chemistry to produce an acyl amino acid surfactant". Appl. Microbiol. Biotechnol. 86 (5): 1387â97. doi:10.1007/s00253-009-2431-8. PMID 20094712.
 USEPA: "2010/15 PFOA Stewardship Program" Archived 27 October 2008 at the Wayback Machine Accessed October 26, 2008.
 "Market Report: World Surfactant Market". Acmite Market Intelligence. Archived from the original on 13 September 2010.
 Market Study on Surfactants (2nd edition, April 2015), by Ceresana Research Archived 20 March 2012 at the Wayback Machine
 Paria, Santanu (2008). "Surfactant-enhanced remediation of organic contaminated soil and water". Advances in Colloid and Interface Science. 138 (1): 24â58. doi:10.1016/j.cis.2007.11.001. PMID 18154747.
 Percival, S.l.; Mayer, D.; Malone, M.; Swanson, T; Gibson, D.; Schultz, G. (2 November 2017). "Surfactants and their role in wound cleansing and biofilm management". Journal of Wound Care. 26 (11): 680â690. doi:10.12968/jowc.2017.26.11.680. ISSN 0969-0700. PMID 29131752.
 Mc Callion, O. N. M.; Taylor, K. M. G.; Thomas, M.; Taylor, A. J. (8 March 1996). "The influence of surface tension on aerosols produced by medical nebulisers". International Journal of Pharmaceutics. 129 (1): 123â136. doi:10.1016/0378-5173(95)04279-2. ISSN 0378-5173.
 Wein, Harrison (28 June 2010). "Progress Toward an Artificial Liver Transplant â NIH Research Matters". National Institutes of Health (NIH). Archived from the original on 5 August 2012.
 Murray, C. B.; Kagan, C. R.; Bawendi, M. G. (2000). "Synthesis and Characterization of Monodisperse Nanocrystals and Close-Packed Nanocrystal Assemblies". Annual Review of Materials Research. 30 (1): 545â610. Bibcode:2000AnRMS..30..545M. doi:10.1146/annurev.matsci.30.1.545.
 Zherebetskyy D, Scheele M, Zhang Y, Bronstein N, Thompson C, Britt D, Salmeron M, Alivisatos P, Wang LW (June 2014). "Hydroxylation of the surface of PbS nanocrystals passivated with oleic acid". Science. 344 (6190): 1380â4. Bibcode:2014Sci...344.1380Z. doi:10.1126/science.1252727. PMID 24876347.
 Baret, Jean-Christophe (10 January 2012). "Surfactants in droplet-based microfluidics". Lab on a Chip. 12 (3): 422â433. doi:10.1039/C1LC20582J. ISSN 1473-0189.
 Reich, Hans J. (2012). "Bordwell pKa Table (Acidity in DMSO)". University of Wisconsin.
 Chiappisi, Leonardo (December 2017). "Polyoxyethylene alkyl ether carboxylic acids: An overview of a neglected class of surfactants with multiresponsive properties". Advances in Colloid and Interface Science. 250: 79â94. doi:10.1016/j.cis.2017.10.001. PMID 29056232.
External links
 Media related to Surfactants at Wikimedia Commons
vte
Foam scales and properties
vte
Underwater diving
Authority control Edit this at Wikidata
NDL: 00564605
Categories: SurfactantsBioremediationBiotechnologyCleaning product componentsColloidal chemistryEnvironmental terminologyUnderwater diving physics
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
41 more
Edit links
This page was last edited on 15 October 2020, at 14:03 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

X-ray reflectivity
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
X-ray reflectivity (sometimes known as X-ray specular reflectivity, X-ray reflectometry, or XRR) is a surface-sensitive analytical technique used in chemistry, physics, and materials science to characterize surfaces, thin films and multilayers.[1][2][3][4] It is a form of reflectometry based on the use of X-rays and is related to the techniques of neutron reflectometry and ellipsometry.


Diagram of x-ray specular reflection
The basic principle of X-ray reflectivity is to reflect a beam of X-rays from a flat surface and to then measure the intensity of X-rays reflected in the specular direction (reflected angle equal to incident angle). If the interface is not perfectly sharp and smooth then the reflected intensity will deviate from that predicted by the law of Fresnel reflectivity. The deviations can then be analyzed to obtain the density profile of the interface normal to the surface.


Contents
1	History
2	Approximation
3	Oscillations
4	Curve fitting
4.1	Open source software
5	References
History
The technique appears to have first been applied to X-rays by Lyman G. Parratt in 1954.[5] Parratt's initial work explored the surface of copper-coated glass, but since that time the technique has been extended to a wide range of both solid and liquid interfaces.

Approximation
When an interface is not perfectly sharp, but has an average electron density profile given by {\displaystyle \rho _{e}(z)}\rho _{e}(z), then the X-ray reflectivity can be approximated by:[2]:83

{\displaystyle R(Q)/R_{F}(Q)=\left|{\frac {1}{\rho _{\infty }}}{\int \limits _{-\infty }^{\infty }{e^{iQz}\left({\frac {d\rho _{e}}{dz}}\right)dz}}\right|^{2}}{\displaystyle R(Q)/R_{F}(Q)=\left|{\frac {1}{\rho _{\infty }}}{\int \limits _{-\infty }^{\infty }{e^{iQz}\left({\frac {d\rho _{e}}{dz}}\right)dz}}\right|^{2}}
Here {\displaystyle R(Q)}R(Q) is the reflectivity, {\displaystyle Q=4\pi \sin(\theta )/\lambda }Q=4\pi \sin(\theta )/\lambda , {\displaystyle \lambda }\lambda  is the X-ray wavelength (typically copper's K-alpha peak at 0.154056 nm), {\displaystyle \rho _{\infty }}\rho _{\infty } is the density deep within the material and {\displaystyle \theta }\theta  is the angle of incidence. Below the critical angle {\displaystyle Q<Q_{c}}{\displaystyle Q<Q_{c}} (derived from Snell's law), 100% of incident radiation is reflected, {\displaystyle R=1}{\displaystyle R=1}. For {\displaystyle Q\gg Q_{c}}{\displaystyle Q\gg Q_{c}}, {\displaystyle R\sim Q^{-4}}{\displaystyle R\sim Q^{-4}}. Typically one can then use this formula to compare parameterized models of the average density profile in the z-direction with the measured X-ray reflectivity and then vary the parameters until the theoretical profile matches the measurement.

Oscillations
For films with multiple layers, X-ray reflectivity may show oscillations with Q (angle/wavelength), analogous to the Fabry-PÃ©rot effect, here called Kiessig fringes.[6] The period of these oscillations can be used to infer layer thicknesses, interlayer roughnesses, electron densities and their contrasts, and complex refractive indices (which depend on atomic number and atomic form factor), for example using the Abeles matrix formalism or the recursive Parratt-formalism as follows:

{\displaystyle X_{j}={\frac {R_{j}}{T_{j}}}={\frac {r_{j,j+1}+X_{j+1}e^{2ik_{j+1,z}d_{j}}}{1+r_{j,j+1}X_{j+1}e^{2ik_{j+1,z}d_{j}}}}e^{-2ik_{j,z}d_{j}}}{\displaystyle X_{j}={\frac {R_{j}}{T_{j}}}={\frac {r_{j,j+1}+X_{j+1}e^{2ik_{j+1,z}d_{j}}}{1+r_{j,j+1}X_{j+1}e^{2ik_{j+1,z}d_{j}}}}e^{-2ik_{j,z}d_{j}}}
where Xj is the ratio of reflected and transmitted amplitudes between layers j and j+1, dj is the thickness of layer j, and rj,j+1 is the Fresnel coefficient for layers j and j+1

{\displaystyle r_{j,j+1}={\frac {k_{j,z}-k_{j+1,z}}{k_{j,z}+k_{j+1,z}}}}{\displaystyle r_{j,j+1}={\frac {k_{j,z}-k_{j+1,z}}{k_{j,z}+k_{j+1,z}}}}
where kj,z is the z component of the wavenumber. For specular reflection where the incident and reflected angles are equal, Q used previously is two times kz because {\displaystyle Q=k_{incident}+k_{reflected}}{\displaystyle Q=k_{incident}+k_{reflected}}. With conditions RN+1 = 0 and T1 = 1 for an N-interface system (i.e. nothing coming back from inside the semi-infinite substrate and unit amplitude incident wave), all Xj can be calculated successively. Roughness can also be accounted for by adding the factor

{\displaystyle r_{j,j+1,rough}=r_{j,j+1,ideal}e^{-2k_{j,z}k_{j+1,z}\sigma _{j}^{2}}}{\displaystyle r_{j,j+1,rough}=r_{j,j+1,ideal}e^{-2k_{j,z}k_{j+1,z}\sigma _{j}^{2}}}
where {\displaystyle \sigma }\sigma  is a standard deviation (aka roughness).

Thin film thickness and critical angle can also be approximated with a linear fit of squared incident angle of the peaks {\displaystyle \theta ^{2}}{\displaystyle \theta ^{2}} in rad2 vs unitless squared peak number {\displaystyle N^{2}}N^{2} as follows:

{\displaystyle \theta ^{2}=({\frac {\lambda }{2d}})^{2}N^{2}+\theta _{c}^{2}}{\displaystyle \theta ^{2}=({\frac {\lambda }{2d}})^{2}N^{2}+\theta _{c}^{2}}.
Curve fitting
X-ray reflectivity measurements are analyzed by fitting to the measured data a simulated curve calculated using the recursive Parratt's formalism combined with the rough interface formula. The fitting parameters are typically layer thicknesses, densities (from which the index of refraction {\displaystyle n}n and eventually the wavevector z component {\displaystyle k_{j,z}}{\displaystyle k_{j,z}} is calculated) and interfacial roughnesses. Measurements are typically normalized so that the maximum reflectivity is 1, but normalization factor can be included in fitting, as well. Additional fitting parameters may be background radiation level and limited sample size due to which beam footprint at low angles may exceed the sample size, thus reducing reflectivity.

Several fitting algorithms have been attempted for X-ray reflectivity, some of which find a local optimum instead of the global optimum. The Levenberg-Marquardt method finds a local optimum. Due to the curve having many interference fringes, it finds incorrect layer thicknesses unless the initial guess is extraordinarily good. The derivative-free simplex method also finds a local optimum. In order to find global optimum, global optimization algorithms such as simulated annealing are required. Unfortunately, simulated annealing may be hard to parallelize on modern multicore computers. Given enough time, simulated annealing can be shown to find the global optimum with a probability approaching 1,[7] but such convergence proof does not mean the required time is reasonably low. In 1998,[8] it was found that genetic algorithms are robust and fast fitting methods for X-ray reflectivity. Thus, genetic algorithms have been adopted by the software of practically all X-ray diffractometer manufacturers and also by open source fitting software.

Fitting a curve requires a function usually called fitness function, cost function, fitting error function or figure of merit (FOM). It measures the difference between measured curve and simulated curve, and therefore, lower values are better. When fitting, the measurement and the best simulation are typically represented in logarithmic space.

From mathematical standpoint, the {\displaystyle \chi ^{2}}\chi ^{2} fitting error function takes into account the effects of Poisson-distributed photon counting noise in a mathematically correct way:

{\displaystyle F=\sum _{i}{\frac {(x_{simul,i}-x_{meas,i})^{2}}{x_{meas,i}}}}{\displaystyle F=\sum _{i}{\frac {(x_{simul,i}-x_{meas,i})^{2}}{x_{meas,i}}}}.
However, this {\displaystyle \chi ^{2}}\chi ^{2} function may give too much weight to the high-intensity regions. If high-intensity regions are important (such as when finding mass density from critical angle), this may not be a problem, but the fit may not visually agree with the measurement at low-intensity high-angle ranges.

Another popular fitting error function is the 2-norm in logarithmic space function. It is defined in the following way:

{\displaystyle F={\sqrt {\sum _{i}(\log x_{simul,i}-\log x_{meas,i})^{2}}}}{\displaystyle F={\sqrt {\sum _{i}(\log x_{simul,i}-\log x_{meas,i})^{2}}}}.
Needless to say, in the equation data points with zero measured photon counts need to be removed. This 2-norm in logarithmic space can be generalized to p-norm in logarithmic space. The drawback of this 2-norm in logarithmic space is that it may give too much weight to regions where relative photon counting noise is high.

Open source software
Diffractometer manufacturers typically provide commercial software to be used for X-ray reflectivity measurements. However, several open source software packages are also available: GenX[9] is a commonly used open source X-ray reflectivity curve fitting software. It is implemented in the Python programming language and runs therefore on both Windows and Linux. Motofit[10] runs in the IGOR Pro environment, and thus cannot be used in open-source operating systems such as Linux. Micronova XRR[11] runs under Java and is therefore available on any operating system on which Java is available. Reflex[12] is a standalone software dedicated to the simulation and analysis of X-rays and neutron reflectiity from multilayers. REFLEX is a user-friendly freeware program working under Windows and Linux platforms.

References
 HolÃ½, V.; KubÄna, J.; OhlÃ­dal, I.; Lischka, K.; Plotz, W. (1993-06-15). "X-ray reflection from rough layered systems". Physical Review B. American Physical Society (APS). 47 (23): 15896â15903. doi:10.1103/physrevb.47.15896. ISSN 0163-1829.
 J. Als-Nielsen, D. McMorrow, Elements of Modern X-Ray Physics, Wiley, New York, (2001).
 J. Daillant, A. Gibaud, X-Ray and Neutron Reflectivity: Principles and Applications. Springer, (1999).
 M. Tolan, X-Ray Scattering from Soft-Matter Thin Films, Springer, (1999).
 Parratt, L. G. (1954-07-15). "Surface Studies of Solids by Total Reflection of X-Rays". Physical Review. American Physical Society (APS). 95 (2): 359â369. doi:10.1103/physrev.95.359. ISSN 0031-899X.
 Kiessig, Heinz (1931). "Untersuchungen zur Totalreflexion von RÃ¶ntgenstrahlen". Annalen der Physik (in German). Wiley. 402 (6): 715â768. doi:10.1002/andp.19314020607. ISSN 0003-3804.
 Granville, V.; Krivanek, M.; Rasson, J.-P. (1994). "Simulated annealing: a proof of convergence". IEEE Transactions on Pattern Analysis and Machine Intelligence. Institute of Electrical and Electronics Engineers (IEEE). 16 (6): 652â656. doi:10.1109/34.295910. ISSN 0162-8828.
 Dane, A.D.; Veldhuis, A.; Boer, D.K.G.de; Leenaers, A.J.G.; Buydens, L.M.C. (1998). "Application of genetic algorithms for characterization of thin layered materials by glancing incidence X-ray reflectometry". Physica B: Condensed Matter. Elsevier BV. 253 (3â4): 254â268. doi:10.1016/s0921-4526(98)00398-6. ISSN 0921-4526.
 Bjorck, Matts. "GenX - Home". genx.sourceforge.net.
 "Main Page - Motofit". motofit.sourceforge.net.
 "jmtilli/micronovaxrr". GitHub. 2017-07-25.
 "Main Page - Reflex". reflex.irdl.fr/Reflex/reflex.html.
Categories: X-ray scattering
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Deutsch
ÙØ§Ø±Ø³Û
Bahasa Indonesia
PortuguÃªs
Edit links
This page was last edited on 7 August 2020, at 04:59 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Fresnel equations
From Wikipedia, the free encyclopedia
  (Redirected from Fresnel reflectivity)
Jump to navigationJump to search
This article is about the Fresnel equations describing reflection and refraction of light at uniform planar interfaces. For the diffraction of light through an aperture, see Fresnel diffraction. For the thin lens and mirror technology, see Fresnel lens.

Partial transmission and reflection of a pulse travelling from a low to a high refractive index medium.


At near-grazing incidence, media interfaces appear mirror-like especially due to reflection of the s polarization, despite being poor reflectors at normal incidence. Polarized sunglasses block the s polarization, greatly reducing glare from horizontal surfaces.
The Fresnel equations (or Fresnel coefficients) describe the reflection and transmission of light (or electromagnetic radiation in general) when incident on an interface between different optical media. They were deduced by Augustin-Jean Fresnel (/freÉªËnÉl/) who was the first to understand that light is a transverse wave, even though no one realized that the "vibrations" of the wave were electric and magnetic fields. For the first time, polarization could be understood quantitatively, as Fresnel's equations correctly predicted the differing behaviour of waves of the s and p polarizations incident upon a material interface.


Contents
1	Overview
1.1	S and P polarizations
2	Power (intensity) reflection and transmission coefficients
2.1	Special cases
2.1.1	Normal incidence
2.1.2	Brewster's angle
2.1.3	Total internal reflection
3	Complex amplitude reflection and transmission coefficients
3.1	Alternative forms
4	Multiple surfaces
5	History
6	Theory
6.1	Material parameters
6.2	Electromagnetic plane waves
6.3	The wave vectors
6.4	The s components
6.5	The p components
6.6	Power ratios (reflectivity and transmissivity)
6.7	Equal refractive indices
6.8	Non-magnetic media
6.9	Brewster's angle
6.10	Equal permittivities
7	See also
8	Notes
9	References
10	Sources
11	Further reading
12	External links
Overview
When light strikes the interface between a medium with refractive index n1 and a second medium with refractive index n2, both reflection and refraction of the light may occur. The Fresnel equations describe the ratios of the reflected and transmitted waves' electric fields to the incident wave's electric field (the waves' magnetic fields can also be related using similar coefficients). Since these are complex ratios, they describe not only the relative amplitude, but phase shifts between the waves.

The equations assume the interface between the media is flat and that the media are homogeneous and isotropic.[1] The incident light is assumed to be a plane wave, which is sufficient to solve any problem since any incident light field can be decomposed into plane waves and polarizations.

S and P polarizations
Main article: Plane of incidence

The plane of incidence is defined by the incoming radiation's propagation vector and the normal vector of the surface.
There are two sets of Fresnel coefficients for two different linear polarization components of the incident wave. Since any polarization state can be resolved into a combination of two orthogonal linear polarizations, this is sufficient for any problem. Likewise, unpolarized (or "randomly polarized") light has an equal amount of power in each of two linear polarizations.

The s polarization refers to polarization of a wave's electric field normal to the plane of incidence (the z direction in the derivation below); then the magnetic field is in the plane of incidence. The p polarization refers to polarization of the electric field in the plane of incidence (the xy plane in the derivation below); then the magnetic field is normal to the plane of incidence.

Although the reflectivity and transmission are dependent on polarization, at normal incidence (Î¸ = 0) there is no distinction between them so all polarization states are governed by a single set of Fresnel coefficients (and another special case is mentioned below in which that is true).

Power (intensity) reflection and transmission coefficients

Variables used in the Fresnel equations

Power coefficients: air to glass

Power coefficients: glass to air
In the diagram on the right, an incident plane wave in the direction of the ray IO strikes the interface between two media of refractive indices n1 and n2 at point O. Part of the wave is reflected in the direction OR, and part refracted in the direction OT. The angles that the incident, reflected and refracted rays make to the normal of the interface are given as Î¸i, Î¸r and Î¸t, respectively.

The relationship between these angles is given by the law of reflection:

{\displaystyle \theta _{\mathrm {i} }=\theta _{\mathrm {r} },}\theta _{\mathrm {i} }=\theta _{\mathrm {r} },
and Snell's law:

{\displaystyle n_{1}\sin \theta _{\mathrm {i} }=n_{2}\sin \theta _{\mathrm {t} }.}n_{1}\sin \theta _{\mathrm {i} }=n_{2}\sin \theta _{\mathrm {t} }.
The behavior of light striking the interface is solved by considering the electric and magnetic fields that constitute an electromagnetic wave, and the laws of electromagnetism, as shown below. The ratio of waves' electric field (or magnetic field) amplitudes are obtained, but in practice one is more often interested in formulae which determine power coefficients, since power (or irradiance) is what can be directly measured at optical frequencies. The power of a wave is generally proportional to the square of the electric (or magnetic) field amplitude.

We call the fraction of the incident power that is reflected from the interface the reflectance (or "reflectivity", or "power reflection coefficient") R, and the fraction that is refracted into the second medium is called the transmittance (or "transmissivity", or "power transmission coefficient") Tâ. Note that these are what would be measured right at each side of an interface and do not account for attenuation of a wave in an absorbing medium following transmission or reflection.[2]

The reflectance for s-polarized light is

{\displaystyle R_{\mathrm {s} }=\left|{\frac {Z_{2}\cos \theta _{\mathrm {i} }-Z_{1}\cos \theta _{\mathrm {t} }}{Z_{2}\cos \theta _{\mathrm {i} }+Z_{1}\cos \theta _{\mathrm {t} }}}\right|^{2},}{\displaystyle R_{\mathrm {s} }=\left|{\frac {Z_{2}\cos \theta _{\mathrm {i} }-Z_{1}\cos \theta _{\mathrm {t} }}{Z_{2}\cos \theta _{\mathrm {i} }+Z_{1}\cos \theta _{\mathrm {t} }}}\right|^{2},}
while the reflectance for p-polarized light is

{\displaystyle R_{\mathrm {p} }=\left|{\frac {Z_{2}\cos \theta _{\mathrm {t} }-Z_{1}\cos \theta _{\mathrm {i} }}{Z_{2}\cos \theta _{\mathrm {t} }+Z_{1}\cos \theta _{\mathrm {i} }}}\right|^{2},}{\displaystyle R_{\mathrm {p} }=\left|{\frac {Z_{2}\cos \theta _{\mathrm {t} }-Z_{1}\cos \theta _{\mathrm {i} }}{Z_{2}\cos \theta _{\mathrm {t} }+Z_{1}\cos \theta _{\mathrm {i} }}}\right|^{2},}
where Z1 and Z2 are the wave impedances of media 1 and 2, respectively.

We assume that the media are non-magnetic (i.e., Î¼1 = Î¼2 = Î¼0), which is typically a good approximation at optical frequencies (and for transparent media at other frequencies).[3] Then the wave impedances are determined solely by the refractive indices n1 and n2:

{\displaystyle Z_{i}={\frac {Z_{0}}{n_{i}}}\,,}{\displaystyle Z_{i}={\frac {Z_{0}}{n_{i}}}\,,}
where Z0 is the impedance of free space and iâ¯=â¯1,â2. Making this substitution, we obtain equations using the refractive indices:

{\displaystyle R_{\mathrm {s} }=\left|{\frac {n_{1}\cos \theta _{\mathrm {i} }-n_{2}\cos \theta _{\mathrm {t} }}{n_{1}\cos \theta _{\mathrm {i} }+n_{2}\cos \theta _{\mathrm {t} }}}\right|^{2}=\left|{\frac {n_{1}\cos \theta _{\mathrm {i} }-n_{2}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}}{n_{1}\cos \theta _{\mathrm {i} }+n_{2}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}}}\right|^{2}\!,}{\displaystyle R_{\mathrm {s} }=\left|{\frac {n_{1}\cos \theta _{\mathrm {i} }-n_{2}\cos \theta _{\mathrm {t} }}{n_{1}\cos \theta _{\mathrm {i} }+n_{2}\cos \theta _{\mathrm {t} }}}\right|^{2}=\left|{\frac {n_{1}\cos \theta _{\mathrm {i} }-n_{2}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}}{n_{1}\cos \theta _{\mathrm {i} }+n_{2}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}}}\right|^{2}\!,}
{\displaystyle R_{\mathrm {p} }=\left|{\frac {n_{1}\cos \theta _{\mathrm {t} }-n_{2}\cos \theta _{\mathrm {i} }}{n_{1}\cos \theta _{\mathrm {t} }+n_{2}\cos \theta _{\mathrm {i} }}}\right|^{2}=\left|{\frac {n_{1}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}-n_{2}\cos \theta _{\mathrm {i} }}{n_{1}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}+n_{2}\cos \theta _{\mathrm {i} }}}\right|^{2}\!.}{\displaystyle R_{\mathrm {p} }=\left|{\frac {n_{1}\cos \theta _{\mathrm {t} }-n_{2}\cos \theta _{\mathrm {i} }}{n_{1}\cos \theta _{\mathrm {t} }+n_{2}\cos \theta _{\mathrm {i} }}}\right|^{2}=\left|{\frac {n_{1}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}-n_{2}\cos \theta _{\mathrm {i} }}{n_{1}{\sqrt {1-\left({\frac {n_{1}}{n_{2}}}\sin \theta _{\mathrm {i} }\right)^{2}}}+n_{2}\cos \theta _{\mathrm {i} }}}\right|^{2}\!.}
The second form of each equation is derived from the first by eliminating Î¸t using Snell's law and trigonometric identities.

As a consequence of conservation of energy, one can find the transmitted power (or more correctly, irradiance: power per unit area) simply as the portion of the incident power that isn't reflected:â[4]

{\displaystyle T_{\mathrm {s} }=1-R_{\mathrm {s} }}T_{\mathrm {s} }=1-R_{\mathrm {s} }
and

{\displaystyle T_{\mathrm {p} }=1-R_{\mathrm {p} }}T_{\mathrm {p} }=1-R_{\mathrm {p} }
Note that all such intensities are measured in terms of a wave's irradiance in the direction normal to the interface; this is also what is measured in typical experiments. That number could be obtained from irradiances in the direction of an incident or reflected wave (given by the magnitude of a wave's Poynting vector) multiplied by cosâ¯Î¸ for a wave at an angle Î¸ to the normal direction (or equivalently, taking the dot product of the Poynting vector with the unit vector normal to the interface). This complication can be ignored in the case of the reflection coefficient, since cosâ¯Î¸i = cosâ¯Î¸r, so that the ratio of reflected to incident irradiance in the wave's direction is the same as in the direction normal to the interface.

Although these relationships describe the basic physics, in many practical applications one is concerned with "natural light" that can be described as unpolarized. That means that there is an equal amount of power in the s and p polarizations, so that the effective reflectivity of the material is just the average of the two reflectivities:

{\displaystyle R_{\mathrm {eff} }={\frac {1}{2}}\left(R_{\mathrm {s} }+R_{\mathrm {p} }\right).}{\displaystyle R_{\mathrm {eff} }={\frac {1}{2}}\left(R_{\mathrm {s} }+R_{\mathrm {p} }\right).}
For low-precision applications involving unpolarized light, such as computer graphics, rather than rigorously computing the effective reflection coefficient for each angle, Schlick's approximation is often used.

Special cases
Normal incidence
For the case of normal incidence, {\displaystyle \theta _{\mathrm {i} }=\theta _{\mathrm {t} }=0}{\displaystyle \theta _{\mathrm {i} }=\theta _{\mathrm {t} }=0}, and there is no distinction between s and p polarization. Thus, the reflectance simplifies to

{\displaystyle R=\left|{\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}\right|^{2}}{\displaystyle R=\left|{\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}\right|^{2}}.
For common glass (n2 â 1.5) surrounded by air (n1â¯=â¯1), the power reflectance at normal incidence can be seen to be about 4%, or 8% accounting for both sides of a glass pane.

Brewster's angle
Main article: Brewster's angle
At a dielectric interface from n1 to n2, there is a particular angle of incidence at which Rp goes to zero and a p-polarised incident wave is purely refracted. This angle is known as Brewster's angle, and is around 56Â° for n1â¯=â¯1 and n2â¯=â¯1.5 (typical glass).

Total internal reflection
Main article: Total internal reflection
When light travelling in a denser medium strikes the surface of a less dense medium (i.e., n1 > n2), beyond a particular incidence angle known as the critical angle, all light is reflected and Rs = Rp = 1. This phenomenon, known as total internal reflection, occurs at incidence angles for which Snell's law predicts that the sine of the angle of refraction would exceed unity (whereas in fact sinâ¯Î¸ â¤ 1 for all real Î¸). For glass with nâ¯=â¯1.5 surrounded by air, the critical angle is approximately 41Â°.

Complex amplitude reflection and transmission coefficients
The above equations relating powers (which could be measured with a photometer for instance) are derived from the Fresnel equations which solve the physical problem in terms of electromagnetic field complex amplitudes, i.e., considering phase in addition to power (which is important in multipath propagation for instance). Those underlying equations supply generally complex-valued ratios of those EM fields and may take several different forms, depending on formalisms used. The complex amplitude coefficients are usually represented by lower case r and t (whereas the power coefficients are capitalized).


Amplitude coefficients: air to glass

Amplitude coefficients: glass to air
In the following, the reflection coefficient r is the ratio of the reflected wave's electric field complex amplitude to that of the incident wave. The transmission coefficient t is the ratio of the transmitted wave's electric field complex amplitude to that of the incident wave. We require separate formulae for the s and p polarizations. In each case we assume an incident plane wave at an angle of incidence {\displaystyle \theta _{\mathrm {i} }}{\displaystyle \theta _{\mathrm {i} }} on a plane interface, reflected at an angle {\displaystyle \theta _{\mathrm {r} }}{\displaystyle \theta _{\mathrm {r} }}, and with a transmitted wave at an angle {\displaystyle \theta _{\mathrm {t} }}{\displaystyle \theta _{\mathrm {t} }}, corresponding to the above figure. Note that in the cases of an interface into an absorbing material (where n is complex) or total internal reflection, the angle of transmission might not evaluate to a real number.

We consider the sign of a wave's electric field in relation to a wave's direction. Consequently, for p polarization at normal incidence, the positive direction of electric field for an incident wave (to the left) is opposite that of a reflected wave (also to its left); for s polarization both are the same (upward).[Note 1]

Using these conventions,[5][6]

{\displaystyle {\begin{aligned}r_{\text{s}}&={\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}},\\[3pt]t_{\text{s}}&={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}},\\[3pt]r_{\text{p}}&={\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}},\\[3pt]t_{\text{p}}&={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}.\end{aligned}}}{\displaystyle {\begin{aligned}r_{\text{s}}&={\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}},\\[3pt]t_{\text{s}}&={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}},\\[3pt]r_{\text{p}}&={\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}},\\[3pt]t_{\text{p}}&={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}.\end{aligned}}}
One can see that ts = rs + 1[7] and
n2
/
n1
tp=rp+1. One can write similar equations applying to the ratio of magnetic fields of the waves, but these are usually not required.

Because the reflected and incident waves propagate in the same medium and make the same angle with the normal to the surface, the power reflection coefficient R is just the squared magnitude of r:â[8]

{\displaystyle R=|r|^{2}.}R=|r|^{2}.
On the other hand, calculation of the power transmission coefficient T is less straightforward, since the light travels in different directions in the two media. What's more, the wave impedances in the two media differ; power is only proportional to the square of the amplitude when the media's impedances are the same (as they are for the reflected wave). This results in:[9]

{\displaystyle T={\frac {n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}}}|t|^{2}.}{\displaystyle T={\frac {n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}}}|t|^{2}.}
The factor of n2/n1 is the reciprocal of the ratio of the media's wave impedances (since we assume Î¼â¯=â¯Î¼0). The factor of cos(Î¸t)/cos(Î¸i) is from expressing power in the direction normal to the interface, for both the incident and transmitted waves.

In the case of total internal reflection where the power transmission T is zero, t nevertheless describes the electric field (including its phase) just beyond the interface. This is an evanescent field which does not propagate as a wave (thus Tâ¯=â¯0) but has nonzero values very close to the interface. The phase shift of the reflected wave on total internal reflection can similarly be obtained from the phase angles of rp and rs (whose magnitudes are unity). These phase shifts are different for s and p waves, which is the well-known principle by which total internal reflection is used to effect polarization transformations.

Alternative forms
In the above formula for rs, if we put {\displaystyle n_{2}=n_{1}\sin \theta _{\text{i}}/\sin \theta _{\text{t}}}{\displaystyle n_{2}=n_{1}\sin \theta _{\text{i}}/\sin \theta _{\text{t}}} (Snell's law) and multiply the numerator and denominator by
1
/
n1
âsinâÎ¸t, we obtainâ[10][11]

{\displaystyle r_{\text{s}}=-{\frac {\sin(\theta _{\text{i}}-\theta _{\text{t}})}{\sin(\theta _{\text{i}}+\theta _{\text{t}})}}.}{\displaystyle r_{\text{s}}=-{\frac {\sin(\theta _{\text{i}}-\theta _{\text{t}})}{\sin(\theta _{\text{i}}+\theta _{\text{t}})}}.}
If we do likewise with the formula for rp, the result is easily shown to be equivalent toâ[12][13]

{\displaystyle r_{\text{p}}={\frac {\tan(\theta _{\text{i}}-\theta _{\text{t}})}{\tan(\theta _{\text{i}}+\theta _{\text{t}})}}.}{\displaystyle r_{\text{p}}={\frac {\tan(\theta _{\text{i}}-\theta _{\text{t}})}{\tan(\theta _{\text{i}}+\theta _{\text{t}})}}.}
These formulasâ[14][15][16] are known respectively as Fresnel's sine law and Fresnel's tangent law.[17] Although at normal incidence these expressions reduce to 0/0, one can see that they yield the correct results in the limit as Î¸i â 0.

Multiple surfaces
When light makes multiple reflections between two or more parallel surfaces, the multiple beams of light generally interfere with one another, resulting in net transmission and reflection amplitudes that depend on the light's wavelength. The interference, however, is seen only when the surfaces are at distances comparable to or smaller than the light's coherence length, which for ordinary white light is few micrometers; it can be much larger for light from a laser.

An example of interference between reflections is the iridescent colours seen in a soap bubble or in thin oil films on water. Applications include FabryâPÃ©rot interferometers, antireflection coatings, and optical filters. A quantitative analysis of these effects is based on the Fresnel equations, but with additional calculations to account for interference.

The transfer-matrix method, or the recursive Rouard methodâ[18] can be used to solve multiple-surface problems.

History
Further information: Augustin-Jean Fresnel
In 1808, Ãtienne-Louis Malus discovered that when a ray of light was reflected off a non-metallic surface at the appropriate angle, it behaved like one of the two rays emerging from a doubly-refractive calcite crystal.[19] He later coined the term polarization to describe this behavior.  In 1815, the dependence of the polarizing angle on the refractive index was determined experimentally by David Brewster.[20] But the reason for that dependence was such a deep mystery that in late 1817, Thomas Young was moved to write:

[T]he great difficulty of all, which is to assign a sufficient reason for the reflection or nonreflection of a polarised ray, will probably long remain, to mortify the vanity of an ambitious philosophy, completely unresolved by any theory.[21]

In 1821, however, Augustin-Jean Fresnel derived results equivalent to his sine and tangent laws (above), by modeling light waves as transverse elastic waves with vibrations perpendicular to what had previously been called the plane of polarization. Fresnel promptly confirmed by experiment that the equations correctly predicted the direction of polarization of the reflected beam when the incident beam was polarized at 45Â° to the plane of incidence, for light incident from air onto glass or water; in particular, the equations gave the correct polarization at Brewster's angle.[22] The experimental confirmation was reported in a "postscript" to the work in which Fresnel first revealed his theory that light waves, including "unpolarized" waves, were purely transverse.[23]

Details of Fresnel's derivation, including the modern forms of the sine law and tangent law, were given later, in a memoir read to the French Academy of Sciences in January 1823.[24] That derivation combined conservation of energy with continuity of the tangential vibration at the interface, but failed to allow for any condition on the normal component of vibration.[25] The first derivation from electromagnetic principles was given by Hendrik Lorentz in 1875.[26]

In the same memoir of January 1823,[24] Fresnel found that for angles of incidence greater than the critical angle, his formulas for the reflection coefficients (rs and rp) gave complex values with unit magnitudes. Noting that the magnitude, as usual, represented the ratio of peak amplitudes, he guessed that the argument represented the phase shift, and verified the hypothesis experimentally.[27] The verification involved

calculating the angle of incidence that would introduce a total phase difference of 90Â° between the s and p components, for various numbers of total internal reflections at that angle (generally there were two solutions),
subjecting light to that number of total internal reflections at that angle of incidence, with an initial linear polarization at 45Â° to the plane of incidence, and
checking that the final polarization was circular.[28]
Thus he finally had a quantitative theory for what we now call the Fresnel rhomb â a device that he had been using in experiments, in one form or another, since 1817 (see Fresnel rhombâ Â§âHistory).

The success of the complex reflection coefficient inspired James MacCullagh and Augustin-Louis Cauchy, beginning in 1836, to analyze reflection from metals by using the Fresnel equations with a complex refractive index.[29]

Four weeks before he presented his completed theory of total internal reflection and the rhomb, Fresnel submitted a memoirâ[30] in which he introduced the needed terms linear polarization, circular polarization, and elliptical polarization,[31] and in which he explained optical rotation as a species of birefringence: linearly-polarized light can be resolved into two circularly-polarized components rotating in opposite directions, and if these propagate at different speeds, the phase difference between them â hence the orientation of their linearly-polarized resultant â will vary continuously with distance.[32]

Thus Fresnel's interpretation of the complex values of his reflection coefficients marked the confluence of several streams of his research and, arguably, the essential completion of his reconstruction of physical optics on the transverse-wave hypothesis (see Augustin-Jean Fresnel).

Theory
Here we systematically derive the above relations from electromagnetic premises.

Material parameters
In order to compute meaningful Fresnel coefficients, we must assume that the medium is (approximately) linear and homogeneous. If the medium is also isotropic, the four field vectors E,â¯B,â¯D,â¯Hâ are related by

Dâ=âÏµE
Bâ=âÎ¼Hâ¯,
where Ïµ and Î¼ are scalars, known respectively as the (electric) permittivity and the (magnetic) permeability of the medium. For a vacuum, these have the values Ïµ0 and Î¼0, respectively. Hence we define the relative permittivity (or dielectric constant) Ïµrelâ¯=â¯Ïµ/Ïµ0â, and the relative permeability Î¼relâ¯=â¯Î¼/Î¼0.

In optics it is common to assume that the medium is non-magnetic, so that Î¼relâ¯=â¯1. For ferromagnetic materials at radio/microwave frequencies, larger values of Î¼rel must be taken into account. But, for optically transparent media, and for all other materials at optical frequencies (except possible metamaterials), Î¼rel is indeed very close to 1; that is, Î¼â¯ââ¯Î¼0.

In optics, one usually knows the refractive index n of the medium, which is the ratio of the speed of light in a vacuum (c) to the speed of light in the medium. In the analysis of partial reflection and transmission, one is also interested in the electromagnetic wave impedance Z, which is the ratio of the amplitude of E to the amplitude of H. It is therefore desirable to express n and Z in terms of Ïµ and Î¼, and thence to relate Z to n. The last-mentioned relation, however, will make it convenient to derive the reflection coefficients in terms of the wave admittance Y, which is the reciprocal of the wave impedance Z.

In the case of uniform plane sinusoidal waves, the wave impedance or admittance is known as the intrinsic impedance or admittance of the medium. This case is the one for which the Fresnel coefficients are to be derived.

Electromagnetic plane waves
In a uniform plane sinusoidal electromagnetic wave, the electric field E has the form

{\displaystyle \mathbf {E_{k}} e^{i(\mathbf {k\cdot r} -\omega t)},}{\displaystyle \mathbf {E_{k}} e^{i(\mathbf {k\cdot r} -\omega t)},}








(1)

where Ek is the (constant) complex amplitude vector,  i is the imaginary unit,  k is the wave vector (whose magnitude k is the angular wavenumber),  r is the position vector,  Ï is the angular frequency,  t is time, and it is understood that the real part of the expression is the physical field.[Note 2]  The value of the expression is unchanged if the position r varies in a direction normal to k; hence k is normal to the wavefronts.

To advance the phase by the angle Ï, we replace Ït by Ït+Ïâ (that is, we replace âÏt by âÏtâÏ),â with the result that the (complex) field is multiplied by eâiÏ. So a phase advance is equivalent to multiplication by a complex constant with a negative argument. This becomes more obvious when the field (1) is factored as EkâeikâreâiÏt,â where the last factor contains the time-dependence. That factor also implies that differentiation w.r.t. time corresponds to multiplication by âiÏ.â[Note 3]

If â is the component of r in the direction of kâ, the field (1) can be written Ekâei(kââÏt).  If the argument of ei(â¯) is to be constant,  â must increase at the velocity {\displaystyle \omega /k\,,\,}{\displaystyle \omega /k\,,\,} known as the phase velocityâ (vp). This in turn is equal to {\displaystyle c/n}c/n. Solving for k gives

{\displaystyle k=n\omega /c\,}{\displaystyle k=n\omega /c\,}.








(2)

As usual, we drop the time-dependent factor eâiÏt which is understood to multiply every complex field quantity. The electric field for a uniform plane sine wave will then be represented by the location-dependent phasor

{\displaystyle \mathbf {E_{k}} e^{i\mathbf {k\cdot r} }}{\displaystyle \mathbf {E_{k}} e^{i\mathbf {k\cdot r} }}.








(3)

For fields of that form, Faraday's law and the Maxwell-AmpÃ¨re law respectively reduce toâ[33]

{\displaystyle {\begin{aligned}\omega \mathbf {B} &=\mathbf {k} \times \mathbf {E} \\\omega \mathbf {D} &=-\mathbf {k} \times \mathbf {H} \,.\end{aligned}}}{\displaystyle {\begin{aligned}\omega \mathbf {B} &=\mathbf {k} \times \mathbf {E} \\\omega \mathbf {D} &=-\mathbf {k} \times \mathbf {H} \,.\end{aligned}}}
Puttingâ Bâ=âÎ¼Hâ andâ Dâ=âÏµE, as above, we can eliminate B and D to obtain equations in only E and H:

{\displaystyle {\begin{aligned}\omega \mu \mathbf {H} &=\mathbf {k} \times \mathbf {E} \\\omega \epsilon \mathbf {E} &=-\mathbf {k} \times \mathbf {H} \,.\end{aligned}}}{\displaystyle {\begin{aligned}\omega \mu \mathbf {H} &=\mathbf {k} \times \mathbf {E} \\\omega \epsilon \mathbf {E} &=-\mathbf {k} \times \mathbf {H} \,.\end{aligned}}}
If the material parameters Ïµ and Î¼ are real (as in a lossless dielectric), these equations show that kâ,âEâ,âHâ form a right-handed orthogonal triad, so that the same equations apply to the magnitudes of the respective vectors. Taking the magnitude equations and substituting from (2), we obtain

{\displaystyle {\begin{aligned}\mu cH&=nE\\\epsilon cE&=nH\,,\end{aligned}}}{\displaystyle {\begin{aligned}\mu cH&=nE\\\epsilon cE&=nH\,,\end{aligned}}}
where H and E are the magnitudes of H and E.â Multiplying the last two equations gives

{\displaystyle n=c\,{\sqrt {\mu \epsilon }}\,.}{\displaystyle n=c\,{\sqrt {\mu \epsilon }}\,.}








(4)

Dividing (or cross-multiplying) the same two equations givesâ Hâ=âYE, where

{\displaystyle Y={\sqrt {\epsilon /\mu }}\,}{\displaystyle Y={\sqrt {\epsilon /\mu }}\,}.








(5)

This is the intrinsic admittance.

From (4) we obtain the phase velocity {\displaystyle c/n=1{\big /}\!{\sqrt {\mu \epsilon \,}}}{\displaystyle c/n=1{\big /}\!{\sqrt {\mu \epsilon \,}}}.â For a vacuum this reduces to {\displaystyle c=1{\big /}\!{\sqrt {\mu _{0}\epsilon _{0}}}}{\displaystyle c=1{\big /}\!{\sqrt {\mu _{0}\epsilon _{0}}}}.â Dividing the second result by the first gives

{\displaystyle n={\sqrt {\mu _{\text{rel}}\epsilon _{\text{rel}}}}\,}{\displaystyle n={\sqrt {\mu _{\text{rel}}\epsilon _{\text{rel}}}}\,}.
For a non-magnetic medium (the usual case), this becomes {\displaystyle n={\sqrt {\epsilon _{\text{rel}}}}}{\displaystyle n={\sqrt {\epsilon _{\text{rel}}}}}.

(Taking the reciprocal of (5), we find that the intrinsic impedance is {\displaystyle Z={\sqrt {\mu /\epsilon }}}{\displaystyle Z={\sqrt {\mu /\epsilon }}}.  In a vacuum this takes the value {\displaystyle Z_{0}={\sqrt {\mu _{0}/\epsilon _{0}}}\,\approx 377\,\Omega \,,}{\displaystyle Z_{0}={\sqrt {\mu _{0}/\epsilon _{0}}}\,\approx 377\,\Omega \,,} known as the impedance of free space. By division, {\displaystyle Z/Z_{0}={\sqrt {\mu _{\text{rel}}/\epsilon _{\text{rel}}}}}{\displaystyle Z/Z_{0}={\sqrt {\mu _{\text{rel}}/\epsilon _{\text{rel}}}}}.â For a non-magnetic medium, this becomes {\displaystyle Z=Z_{0}{\big /}\!{\sqrt {\epsilon _{\text{rel}}}}=Z_{0}/n.}{\displaystyle Z=Z_{0}{\big /}\!{\sqrt {\epsilon _{\text{rel}}}}=Z_{0}/n.})

The wave vectors

Incident, reflected, and transmitted wave vectors (ki, kr, and ktâ), for incidence from a medium with refractive index n1 to a medium with refractive index n2. The red arrows are perpendicular to the wave vectors.
In Cartesian coordinates (x,ây,z), let the region yâ<â0 have refractive index n1â, intrinsic admittance Y1â, etc., and let the region yâ>â0 have refractive index n2â, intrinsic admittance Y2â, etc. Then the xz plane is the interface, and the y axis is normal to the interface (see diagram). Let i and j (in bold roman type) be the unit vectors in the x and y directions, respectively. Let the plane of incidence be the xy plane (the plane of the page), with the angle of incidence Î¸i measured from j towards i. Let the angle of refraction, measured in the same sense, be Î¸tâ, where the subscript t stands for transmitted (reserving r for reflected).

In the absence of Doppler shifts, Ï does not change on reflection or refraction. Hence, by (2), the magnitude of the wave vector is proportional to the refractive index.

So, for a given Ï, if we redefine k as the magnitude of the wave vector in the reference medium (for which nâ=1), then the wave vector has magnitude n1k in the first medium (region yâ<â0 in the diagram) and magnitude n2k in the second medium. From the magnitudes and the geometry, we find that the wave vectors are

{\displaystyle {\begin{aligned}\mathbf {k} _{\text{i}}&=n_{1}k(\mathbf {i} \sin \theta _{\text{i}}+\mathbf {j} \cos \theta _{\text{i}})\\[.5ex]\mathbf {k} _{\text{r}}&=n_{1}k(\mathbf {i} \sin \theta _{\text{i}}-\mathbf {j} \cos \theta _{\text{i}})\\[.5ex]\mathbf {k} _{\text{t}}&=n_{2}k(\mathbf {i} \sin \theta _{\text{t}}+\mathbf {j} \cos \theta _{\text{t}})\\&=k(\mathbf {i} \,n_{1}\sin \theta _{\text{i}}+\mathbf {j} \,n_{2}\cos \theta _{\text{t}})\,,\end{aligned}}}{\displaystyle {\begin{aligned}\mathbf {k} _{\text{i}}&=n_{1}k(\mathbf {i} \sin \theta _{\text{i}}+\mathbf {j} \cos \theta _{\text{i}})\\[.5ex]\mathbf {k} _{\text{r}}&=n_{1}k(\mathbf {i} \sin \theta _{\text{i}}-\mathbf {j} \cos \theta _{\text{i}})\\[.5ex]\mathbf {k} _{\text{t}}&=n_{2}k(\mathbf {i} \sin \theta _{\text{t}}+\mathbf {j} \cos \theta _{\text{t}})\\&=k(\mathbf {i} \,n_{1}\sin \theta _{\text{i}}+\mathbf {j} \,n_{2}\cos \theta _{\text{t}})\,,\end{aligned}}}
where the last step uses Snell's law. The corresponding dot products in the phasor form (3) are

{\displaystyle {\begin{aligned}\mathbf {k} _{\text{i}}\mathbf {\cdot r} &=n_{1}k(x\sin \theta _{\text{i}}+y\cos \theta _{\text{i}})\\\mathbf {k} _{\text{r}}\mathbf {\cdot r} &=n_{1}k(x\sin \theta _{\text{i}}-y\cos \theta _{\text{i}})\\\mathbf {k} _{\text{t}}\mathbf {\cdot r} &=k(n_{1}x\sin \theta _{\text{i}}+n_{2}y\cos \theta _{\text{t}})\,.\end{aligned}}}{\displaystyle {\begin{aligned}\mathbf {k} _{\text{i}}\mathbf {\cdot r} &=n_{1}k(x\sin \theta _{\text{i}}+y\cos \theta _{\text{i}})\\\mathbf {k} _{\text{r}}\mathbf {\cdot r} &=n_{1}k(x\sin \theta _{\text{i}}-y\cos \theta _{\text{i}})\\\mathbf {k} _{\text{t}}\mathbf {\cdot r} &=k(n_{1}x\sin \theta _{\text{i}}+n_{2}y\cos \theta _{\text{t}})\,.\end{aligned}}}








(6)

Hence:

At  {\displaystyle y=0\,,~~~\mathbf {k} _{\text{i}}\mathbf {\cdot r} =\mathbf {k} _{\text{r}}\mathbf {\cdot r} =\mathbf {k} _{\text{t}}\mathbf {\cdot r} =n_{1}kx\sin \theta _{\text{i}}\,}{\displaystyle y=0\,,~~~\mathbf {k} _{\text{i}}\mathbf {\cdot r} =\mathbf {k} _{\text{r}}\mathbf {\cdot r} =\mathbf {k} _{\text{t}}\mathbf {\cdot r} =n_{1}kx\sin \theta _{\text{i}}\,}.








(7)

The s components
For the s polarization, the E field is parallel to the z axis and may therefore be described by its component in the z direction. Let the reflection and transmission coefficients be rs and tsâ, respectively. Then, if the incident E field is taken to have unit amplitude, the phasor form (3) of its z component is

{\displaystyle E_{\text{i}}=e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} },}{\displaystyle E_{\text{i}}=e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} },}








(8)

and the reflected and transmitted fields, in the same form, are

{\displaystyle {\begin{aligned}E_{\text{r}}&=r_{s\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\E_{\text{t}}&=t_{s\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}{\displaystyle {\begin{aligned}E_{\text{r}}&=r_{s\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\E_{\text{t}}&=t_{s\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}








(9)

Under the sign convention used in this article, a positive reflection or transmission coefficient is one that preserves the direction of the transverse field, meaning (in this context) the field normal to the plane of incidence. For the s polarization, that means the E field. If the incident, reflected, and transmitted E fields (in the above equations) are in the z direction ("out of the page"), then the respective H fields are in the directions of the red arrows, sinceâ kâ,âEâ,âHâ form a right-handed orthogonal triad. The H fields may therefore be described by their components in the directions of those arrows, denoted byâ Hiâ,âHr,âHtâ.  Then, sinceâ Hâ=âYE,

{\displaystyle {\begin{aligned}H_{\text{i}}&=\,Y_{1}e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\H_{\text{r}}&=\,Y_{1}r_{s\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\H_{\text{t}}&=\,Y_{2}t_{s\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}{\displaystyle {\begin{aligned}H_{\text{i}}&=\,Y_{1}e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\H_{\text{r}}&=\,Y_{1}r_{s\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\H_{\text{t}}&=\,Y_{2}t_{s\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}








(10)

At the interface, by the usual interface conditions for electromagnetic fields, the tangential components of the E and H fields must be continuous; that is,

{\displaystyle \left.{\begin{aligned}E_{\text{i}}+E_{\text{r}}&=E_{\text{t}}\\H_{\text{i}}\cos \theta _{\text{i}}-H_{\text{r}}\cos \theta _{\text{i}}&=H_{\text{t}}\cos \theta _{\text{t}}\end{aligned}}~~\right\}~~~{\text{at}}~~y=0\,}{\displaystyle \left.{\begin{aligned}E_{\text{i}}+E_{\text{r}}&=E_{\text{t}}\\H_{\text{i}}\cos \theta _{\text{i}}-H_{\text{r}}\cos \theta _{\text{i}}&=H_{\text{t}}\cos \theta _{\text{t}}\end{aligned}}~~\right\}~~~{\text{at}}~~y=0\,}.








(11)

When we substitute from equations (8) to (10) and then from (7), the exponential factors cancel out, so that the interface conditions reduce to the simultaneous equations

{\displaystyle {\begin{aligned}1+r_{\text{s}}&=\,t_{\text{s}}\\Y_{1}\cos \theta _{\text{i}}-Y_{1}r_{\text{s}}\cos \theta _{\text{i}}&=\,Y_{2}t_{\text{s}}\cos \theta _{\text{t}}\,,\end{aligned}}}{\displaystyle {\begin{aligned}1+r_{\text{s}}&=\,t_{\text{s}}\\Y_{1}\cos \theta _{\text{i}}-Y_{1}r_{\text{s}}\cos \theta _{\text{i}}&=\,Y_{2}t_{\text{s}}\cos \theta _{\text{t}}\,,\end{aligned}}}








(12)

which are easily solved for rs and ts, yielding

{\displaystyle r_{\text{s}}={\frac {Y_{1}\cos \theta _{\text{i}}-Y_{2}\cos \theta _{\text{t}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}}{\displaystyle r_{\text{s}}={\frac {Y_{1}\cos \theta _{\text{i}}-Y_{2}\cos \theta _{\text{t}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}}








(13)

and

{\displaystyle t_{\text{s}}={\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\,}{\displaystyle t_{\text{s}}={\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\,}.








(14)

At normal incidence (Î¸iâ= Î¸tâ= 0), indicated by an additional subscript 0, these results become

{\displaystyle r_{\text{s0}}={\frac {Y_{1}-Y_{2}}{Y_{1}+Y_{2}}}}{\displaystyle r_{\text{s0}}={\frac {Y_{1}-Y_{2}}{Y_{1}+Y_{2}}}}








(15)

and

{\displaystyle t_{\text{s0}}={\frac {2Y_{1}}{Y_{1}+Y_{2}}}\,}{\displaystyle t_{\text{s0}}={\frac {2Y_{1}}{Y_{1}+Y_{2}}}\,}.








(16)

At grazing incidence (Î¸iââ 90Â°), we haveâ cosâÎ¸iââ 0, henceâ rsââââ1â andâ tsââ 0.

The p components
For the p polarization, the incident, reflected, and transmitted E fields are parallel to the red arrows and may therefore be described by their components in the directions of those arrows. Let those components beâ Eiâ,âEr,âEtâ (redefining the symbols for the new context). Let the reflection and transmission coefficients be rp and tp. Then, if the incident E field is taken to have unit amplitude, we have

{\displaystyle {\begin{aligned}E_{\text{i}}&=e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\E_{\text{r}}&=r_{p\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\E_{\text{t}}&=t_{p\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}{\displaystyle {\begin{aligned}E_{\text{i}}&=e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\E_{\text{r}}&=r_{p\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\E_{\text{t}}&=t_{p\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}








(17)

If the E fields are in the directions of the red arrows, then, in order forâ kâ,âEâ,âHâ to form a right-handed orthogonal triad, the respective H fields must be in the âz direction ("into the page") and may therefore be described by their components in that direction. This is consistent with the adopted sign convention, namely that a positive reflection or transmission coefficient is one that preserves the direction of the transverse field (the H field in the case of the p polarization). The agreement of the other field with the red arrows reveals an alternative definition of the sign convention: that a positive reflection or transmission coefficient is one for which the field vector in the plane of incidence points towards the same medium before and after reflection or transmission.[34]

So, for the incident, reflected, and transmitted H fields, let the respective components in the âz direction beâ Hiâ,âHr,âHtâ.  Then, sinceâ Hâ=âYE,

{\displaystyle {\begin{aligned}H_{\text{i}}&=\,Y_{1}e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\H_{\text{r}}&=\,Y_{1}r_{p\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\H_{\text{t}}&=\,Y_{2}t_{p\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}{\displaystyle {\begin{aligned}H_{\text{i}}&=\,Y_{1}e^{i\mathbf {k} _{\text{i}}\mathbf {\cdot r} }\\H_{\text{r}}&=\,Y_{1}r_{p\,}e^{i\mathbf {k} _{\text{r}}\mathbf {\cdot r} }\\H_{\text{t}}&=\,Y_{2}t_{p\,}e^{i\mathbf {k} _{\text{t}}\mathbf {\cdot r} }.\end{aligned}}}








(18)

At the interface, the tangential components of the E and H fields must be continuous; that is,

{\displaystyle \left.{\begin{aligned}E_{\text{i}}\cos \theta _{\text{i}}-E_{\text{r}}\cos \theta _{\text{i}}&=E_{\text{t}}\cos \theta _{\text{t}}\\H_{\text{i}}+H_{\text{r}}&=H_{\text{t}}\end{aligned}}~~\right\}~~~{\text{at}}~~y=0\,}{\displaystyle \left.{\begin{aligned}E_{\text{i}}\cos \theta _{\text{i}}-E_{\text{r}}\cos \theta _{\text{i}}&=E_{\text{t}}\cos \theta _{\text{t}}\\H_{\text{i}}+H_{\text{r}}&=H_{\text{t}}\end{aligned}}~~\right\}~~~{\text{at}}~~y=0\,}.








(19)

When we substitute from equations (17) and (18) and then from (7), the exponential factors again cancel out, so that the interface conditions reduce to

{\displaystyle {\begin{aligned}\cos \theta _{\text{i}}-r_{\text{p}}\cos \theta _{\text{i}}&=\,t_{\text{p}}\cos \theta _{\text{t}}\\Y_{1}+Y_{1}r_{\text{p}}&=\,Y_{2}t_{\text{p}}\,.\end{aligned}}}{\displaystyle {\begin{aligned}\cos \theta _{\text{i}}-r_{\text{p}}\cos \theta _{\text{i}}&=\,t_{\text{p}}\cos \theta _{\text{t}}\\Y_{1}+Y_{1}r_{\text{p}}&=\,Y_{2}t_{\text{p}}\,.\end{aligned}}}








(20)

Solving for rp and tp, we find

{\displaystyle r_{\text{p}}={\frac {Y_{2}\cos \theta _{\text{i}}-Y_{1}\cos \theta _{\text{t}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}}{\displaystyle r_{\text{p}}={\frac {Y_{2}\cos \theta _{\text{i}}-Y_{1}\cos \theta _{\text{t}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}}








(21)

and

{\displaystyle t_{\text{p}}={\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\,}{\displaystyle t_{\text{p}}={\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\,}.








(22)

At normal incidence (Î¸iâ= Î¸tâ= 0), indicated by an additional subscript 0, these results become

{\displaystyle r_{\text{p0}}={\frac {Y_{2}-Y_{1}}{Y_{2}+Y_{1}}}}{\displaystyle r_{\text{p0}}={\frac {Y_{2}-Y_{1}}{Y_{2}+Y_{1}}}}








(23)

and

{\displaystyle t_{\text{p0}}={\frac {2Y_{1}}{Y_{2}+Y_{1}}}\,}{\displaystyle t_{\text{p0}}={\frac {2Y_{1}}{Y_{2}+Y_{1}}}\,}.








(24)

At grazing incidence (Î¸iââ 90Â°), we again haveâ cosâÎ¸iââ 0, henceâ rpââââ1â andâ tpââ 0.

Comparing (23) and (24) with (15) and (16), we see that at normal incidence, under the adopted sign convention, the transmission coefficients for the two polarizations are equal, whereas the reflection coefficients have equal magnitudes but opposite signs. While this clash of signs is a disadvantage of the convention, the attendant advantage is that the signs agree at grazing incidence.

Power ratios (reflectivity and transmissivity)
The Poynting vector for a wave is a vector whose component in any direction is the irradiance (power per unit area) of that wave on a surface perpendicular to that direction. For a plane sinusoidal wave the Poynting vector is
1
/
2
Re{EâÃâHâ}, where E and H are due only to the wave in question, and the asterisk denotes complex conjugation. Inside a lossless dielectric (the usual case), E and H are in phase, and at right angles to each other and to the wave vector kâ; so, for s polarization, using the z and xy components of E and H respectively (or for p polarization, using the xy and -z components of E and H), the irradiance in the direction of k is given simply by EH/2â, which is âE2â2Zâ in a medium of intrinsic impedance Zâ¯=â¯1/Y. To compute the irradiance in the direction normal to the interface, as we shall require in the definition of the power transmission coefficient, we could use only the x component (rather than the full xy component) of H or E or, equivalently, simply multiply EH/2 by the proper geometric factor, obtaining (âE2â2Zâ)â¯cosâ¯Î¸.

From equations (13) and (21), taking squared magnitudes, we find that the reflectivity (ratio of reflected power to incident power) is

{\displaystyle R_{\text{s}}=\left|{\frac {Y_{1}\cos \theta _{\text{i}}-Y_{2}\cos \theta _{\text{t}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\right|^{2}}{\displaystyle R_{\text{s}}=\left|{\frac {Y_{1}\cos \theta _{\text{i}}-Y_{2}\cos \theta _{\text{t}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\right|^{2}}








(25)

for the s polarization, and

{\displaystyle R_{\text{p}}=\left|{\frac {Y_{2}\cos \theta _{\text{i}}-Y_{1}\cos \theta _{\text{t}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\right|^{2}}{\displaystyle R_{\text{p}}=\left|{\frac {Y_{2}\cos \theta _{\text{i}}-Y_{1}\cos \theta _{\text{t}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\right|^{2}}








(26)

for the p polarization. Note that when comparing the powers of two such waves in the same medium and with the same âcosâÎ¸, the impedance and geometric factors mentioned above are identical and cancel out. But in computing the power transmission (below), these factors must be taken into account.

The simplest way to obtain the power transmission coefficient (transmissivity, the ratio of transmitted power to incident power in the direction normal to the interface, i.e. the y direction) is to use Râ¯+â¯Tâ¯=â¯1 (conservation of energy). In this way we find

{\displaystyle T_{\text{s}}=1-R_{\text{s}}=\,{\frac {4\,{\text{Re}}\{Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}\}}{\left|Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}\right|^{2}}}}{\displaystyle T_{\text{s}}=1-R_{\text{s}}=\,{\frac {4\,{\text{Re}}\{Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}\}}{\left|Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}\right|^{2}}}}








(25T)

for the s polarization, and

{\displaystyle T_{\text{p}}=1-R_{\text{p}}=\,{\frac {4\,{\text{Re}}\{Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}\}}{\left|Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}\right|^{2}}}}{\displaystyle T_{\text{p}}=1-R_{\text{p}}=\,{\frac {4\,{\text{Re}}\{Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}\}}{\left|Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}\right|^{2}}}}








(26T)

for the p polarization.

In the case of an interface between two lossless media (for which Ïµ and Î¼ are real and positive), one can obtain these results directly using the squared magnitudes of the amplitude transmission coefficients that we found earlier in equations (14) and (22). But, for given amplitude (as noted above), the component of the Poynting vector in the y direction is proportional to the geometric factor cosâ¯Î¸ and inversely proportional to the wave impedance Z. Applying these corrections to each wave, we obtain two ratios multiplying the square of the amplitude transmission coefficient:

{\displaystyle T_{\text{s}}=\left({\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\right)^{2}{\frac {\,Y_{2}\,}{Y_{1}}}\,{\frac {\cos \theta _{\text{t}}}{\cos \theta _{\text{i}}}}={\frac {4Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}}{\left(Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}\right)^{2}}}}{\displaystyle T_{\text{s}}=\left({\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}}}\right)^{2}{\frac {\,Y_{2}\,}{Y_{1}}}\,{\frac {\cos \theta _{\text{t}}}{\cos \theta _{\text{i}}}}={\frac {4Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}}{\left(Y_{1}\cos \theta _{\text{i}}+Y_{2}\cos \theta _{\text{t}}\right)^{2}}}}








(27)

for the s polarization, and

{\displaystyle T_{\text{p}}=\left({\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\right)^{2}{\frac {\,Y_{2}\,}{Y_{1}}}\,{\frac {\cos \theta _{\text{t}}}{\cos \theta _{\text{i}}}}={\frac {4Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}}{\left(Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}\right)^{2}}}}{\displaystyle T_{\text{p}}=\left({\frac {2Y_{1}\cos \theta _{\text{i}}}{Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}}}\right)^{2}{\frac {\,Y_{2}\,}{Y_{1}}}\,{\frac {\cos \theta _{\text{t}}}{\cos \theta _{\text{i}}}}={\frac {4Y_{1}Y_{2}\cos \theta _{\text{i}}\cos \theta _{\text{t}}}{\left(Y_{2}\cos \theta _{\text{i}}+Y_{1}\cos \theta _{\text{t}}\right)^{2}}}}








(28)

for the p polarization. The last two equations apply only to lossless dielectrics, and only at incidence angles smaller than the critical angle (beyond which, of course, Tâ¯=â¯0â).

Equal refractive indices
From equations (4) and (5), we see that two dissimilar media will have the same refractive index, but different admittances, if the ratio of their permeabilities is the inverse of the ratio of their permittivities. In that unusual situation we haveâ Î¸tâ= Î¸i (that is, the transmitted ray is undeviated), so that the cosines in equations (13), (14), (21), (22), and (25) to (28) cancel out, and all the reflection and transmission ratios become independent of the angle of incidence; in other words, the ratios for normal incidence become applicable to all angles of incidence.[35]. When extended to spherical reflection or scattering, this results in the Kerker effect for Mie scattering.

Non-magnetic media
Since the Fresnel equations were developed for optics, they are usually given for non-magnetic materials. Dividing (4) by (5)) yields

{\displaystyle Y={\frac {n}{\,c\mu \,}}}{\displaystyle Y={\frac {n}{\,c\mu \,}}}.
For non-magnetic media we can substitute the vacuum permeability Î¼0 for Î¼, so that

{\displaystyle Y_{1}={\frac {n_{1}}{\,c\mu _{0}}}~~;~~~Y_{2}={\frac {n_{2}}{\,c\mu _{0}}}\,}{\displaystyle Y_{1}={\frac {n_{1}}{\,c\mu _{0}}}~~;~~~Y_{2}={\frac {n_{2}}{\,c\mu _{0}}}\,};
that is, the admittances are simply proportional to the corresponding refractive indices. When we make these substitutions in equations (13) to (16) and equations (21) to (26), the factor cÎ¼0 cancels out. For the amplitude coefficients we obtain:[5][6]

{\displaystyle r_{\text{s}}={\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}}{\displaystyle r_{\text{s}}={\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}}








(29)

{\displaystyle t_{\text{s}}={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}\,}{\displaystyle t_{\text{s}}={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}\,}








(30)

{\displaystyle r_{\text{p}}={\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}}{\displaystyle r_{\text{p}}={\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}}








(31)

{\displaystyle t_{\text{p}}={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}\,}{\displaystyle t_{\text{p}}={\frac {2n_{1}\cos \theta _{\text{i}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}\,}.








(32)

For the case of normal incidence these reduce to:

{\displaystyle r_{\text{s0}}={\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}}{\displaystyle r_{\text{s0}}={\frac {n_{1}-n_{2}}{n_{1}+n_{2}}}}








(33)

{\displaystyle t_{\text{s0}}={\frac {2n_{1}}{n_{1}+n_{2}}}}{\displaystyle t_{\text{s0}}={\frac {2n_{1}}{n_{1}+n_{2}}}}








(34)

{\displaystyle r_{\text{p0}}={\frac {n_{2}-n_{1}}{n_{2}+n_{1}}}}{\displaystyle r_{\text{p0}}={\frac {n_{2}-n_{1}}{n_{2}+n_{1}}}}








(35)

{\displaystyle t_{\text{p0}}={\frac {2n_{1}}{n_{2}+n_{1}}}\,}{\displaystyle t_{\text{p0}}={\frac {2n_{1}}{n_{2}+n_{1}}}\,}.








(36)

The power reflection coefficients become:

{\displaystyle R_{\text{s}}=\left|{\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}\right|^{2}}{\displaystyle R_{\text{s}}=\left|{\frac {n_{1}\cos \theta _{\text{i}}-n_{2}\cos \theta _{\text{t}}}{n_{1}\cos \theta _{\text{i}}+n_{2}\cos \theta _{\text{t}}}}\right|^{2}}








(37)

{\displaystyle R_{\text{p}}=\left|{\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}\right|^{2}}{\displaystyle R_{\text{p}}=\left|{\frac {n_{2}\cos \theta _{\text{i}}-n_{1}\cos \theta _{\text{t}}}{n_{2}\cos \theta _{\text{i}}+n_{1}\cos \theta _{\text{t}}}}\right|^{2}}.








(38)

The power transmissions can then be found from Tâ¯=â¯1â¯-â¯R.

Brewster's angle
For equal permeabilities (e.g., non-magnetic media), if Î¸i and Î¸t are complementary, we can substitute sinâÎ¸t for cosâÎ¸i, and sinâÎ¸i for cosâÎ¸t, so that the numerator in equation (31) becomes n2sinâÎ¸tââ n1sinâÎ¸i,â which is zero (by Snell's law). Henceâ rpâ= 0â and only the s-polarized component is reflected. This is what happens at the Brewster angle. Substituting cosâÎ¸i for sinâÎ¸t in Snell's law, we readily obtain

{\displaystyle \theta _{\text{i}}=\arctan(n_{2}/n_{1})}{\displaystyle \theta _{\text{i}}=\arctan(n_{2}/n_{1})}








(39)

for Brewster's angle.

Equal permittivities
Although it is not encountered in practice, the equations can also apply to the case of two media with a common permittivity but different refractive indices due to different permeabilities. From equations (4) and (5), if Ïµ is fixed instead of Î¼, then Y becomes inversely proportional to n, with the result that the subscripts 1 and 2 in equations (29) to (38) are interchanged (due to the additional step of multiplying the numerator and denominator by n1n2). Hence, in (29) and (31), the expressions for rs and rp in terms of refractive indices will be interchanged, so that Brewster's angle (39) will giveâ rsâ= 0â instead ofâ rpâ= 0, and any beam reflected at that angle will be p-polarized instead of s-polarized.[36] Similarly, Fresnel's sine law will apply to the p polarization instead of the s polarization, and his tangent law to the s polarization instead of the p polarization.

This switch of polarizations has an analog in the old mechanical theory of light waves (see Â§â¯History, above). One could predict reflection coefficients that agreed with observation by supposing (like Fresnel) that different refractive indices were due to different densities and that the vibrations were normal to what was then called the plane of polarization, or by supposing (like MacCullagh and Neumann) that different refractive indices were due to different elasticities and that the vibrations were parallel to that plane.[37] Thus the condition of equal permittivities and unequal permeabilities, although not realistic, is of some historical interest.

See also
Jones calculus
Polarization mixing
Index-matching material
Field and power quantities
Fresnel rhomb, Fresnel's apparatus to produce circularly polarised light
Specular reflection
Schlick's approximation
Snell's window
X-ray reflectivity
Plane of incidence
Reflections of signals on conducting lines
Notes
 Some authors use the opposite sign convention for rp, so that rp is positive when the incoming and reflected magnetic fields are anti-parallel, and negative when they are parallel. This latter convention has the convenient advantage that the s and p sign conventions are the same at normal incidence. However, either convention, when used consistently, gives the right answers.
 The above form (1) is typically used by physicists. Electrical engineers typically prefer the formâ Ekâej(Ïtâkâr); that is, they not only use j instead of i for the imaginary unit, but also change the sign of the exponent, with the result that the whole expression is replaced by its complex conjugate, leaving the real part unchanged [Cf. (e.g.) Collin, 1966, p.â41, eq.(2.81)]. The electrical engineers' form and the formulas derived therefrom may be converted to the physicists' convention by substituting âi for j.
 In the electrical engineering convention, the time-dependent factor is ejÏt, so that a phase advance corresponds to multiplication by a complex constant with a positive argument, and differentiation w.r.t. time corresponds to multiplication by +jÏ. This article, however, uses the physics convention, whose time-dependent factor is eâiÏt. Although the imaginary unit does not appear explicitly in the results given here, the time-dependent factor affects the interpretation of any results that turn out to be complex.
References
 Born & Wolf, 1970, p.â38.
 Hecht, 1987, p.â100.
 Driggers, Ronald G.; Hoffman, Craig; Driggers, Ronald (2011). Encyclopedia of Optical Engineering. doi:10.1081/E-EOE. ISBN 978-0-8247-0940-2.
 Hecht, 1987, p.â102.
 Lecture notes by Bo Sernelius, main site, see especially Lecture 12.
 Born & Wolf, 1970, p.â40, eqs.â(20),â(21).
 Hecht, 2002, p.â116, eqs.â(4.49),â(4.50).
 Hecht, 2002, p.â120, eq.â(4.56).
 Hecht, 2002, p.â120, eq.â(4.57).
 Fresnel, 1866, p.â773.
 Hecht, 2002, p.â115, eq.â(4.42).
 Fresnel, 1866, p.â757.
 Hecht, 2002, p.â115, eq.â(4.43).
 E. Verdet, in Fresnel, 1866, p.â789n.
 Born & Wolf, 1970, p.â40, eqs.â(21a).
 Jenkins & White, 1976, p.â524, eqs.â(25a).
 Whittaker, 1910, p.â134; Darrigol, 2012, p.213.
 Heavens, O. S. (1955). Optical Properties of Thin Films. Academic Press. chapt. 4.
 Darrigol, 2012, pp.â191â2.
 D. Brewster, "On the laws which regulate the polarisation of light by reflexion from transparent bodies", Philosophical Transactions of the Royal Society, vol.â105, pp.â125â59, read 16 March 1815.
 T. Young, "Chromatics" (written Sep.ââOct.1817), Supplement to the Fourth, Fifth, and Sixth Editions of the EncyclopÃ¦dia Britannica, vol.â3 (first half, issued February 1818), pp.â141â63, concluding sentence.
 Buchwald, 1989, pp.â390â91; Fresnel, 1866, pp.â646â8.
 A. Fresnel, "Note sur le calcul des teintes que la polarisation dÃ©veloppe dans les lames cristallisÃ©es" et seq., Annales de Chimie et de Physique, vol. 17, pp. 102â11 (May 1821), 167â96 (June 1821), 312â15 ("Postscript", July 1821); reprinted in Fresnel, 1866, pp. 609â48; translated as "On the calculation of the hues that polarization develops in crystalline plates (& postscript)", Zenodo: 4058004 / doi:10.5281/zenodo.4058004, 2020.
 A. Fresnel, "MÃ©moire sur la loi des modifications que la rÃ©flexion imprime Ã  la lumiÃ¨re polarisÃ©e" ("Memoir on the law of the modifications that reflection impresses on polarized light"), read 7 January 1823; reprinted in Fresnel, 1866, pp.â767â99 (full text, published 1831), pp.â753â62 (extract, published 1823). See especially pp.â773 (sine law), 757 (tangent law), 760â61 and 792â6 (angles of total internal reflection for given phase differences).
 Buchwald, 1989, pp.â391â3; Whittaker, 1910, pp.â133â5.
 Buchwald, 1989, p.â392.
 Lloyd, 1834, pp.â369â70; Buchwald, 1989, pp.â393â4,â453; Fresnel, 1866, pp.â781â96.
 Fresnel, 1866, pp.â760â61,â792â6; Whewell, 1857, p.â359.
 Whittaker, 1910, pp.â177â9.
 A. Fresnel, "MÃ©moire sur la double rÃ©fraction que les rayons lumineux Ã©prouvent en traversant les aiguilles de cristal de roche suivant les directions parallÃ¨les Ã  l'axe" ("Memoir on the double refraction that light rays undergo in traversing the needles of rock crystal [quartz] in directions parallel to the axis"), signed & submitted 9 December 1822; reprinted in Fresnel, 1866, pp.â731â51 (full text, published 1825), pp.â719â29 (extract, published 1823). On the publication dates, see also Buchwald, 1989, p.â462, ref.â1822b.
 Buchwald, 1989, pp.â230â31; Fresnel, 1866, p.â744.
 Buchwald, 1989, p.â442; Fresnel, 1866, pp.â737â9,â749.  Cf. Whewell, 1857, pp.â356â8; Jenkins & White, 1976, pp.â589â90.
 Compare M.V. Berry and M.R. Jeffrey, "Conical diffraction: Hamilton's diabolical point at the heart of crystal optics", in E. Wolf (ed.), Progress in Optics, vol.â50, Amsterdam: Elsevier, 2007, pp.â13â50, at p.â18, eq.(2.2).
 This agrees with Born & Wolf, 1970, p.â38, Fig.â1.10.
 Giles, C.L.; Wild, W.J. (1982). "Fresnel Reflection and Transmission at a Planar Boundary from Media of Equal Refractive Indices". Applied Physics Letters. 40 (3): 210â212. doi:10.1063/1.93043.
 More general Brewster angles, for which the angles of incidence and refraction are not necessarily complementary, are discussed in C.L. Giles and W.J. Wild, "Brewster angles for magnetic media", International Journal of Infrared and Millimeter Waves, vol.â6, no.3 (March 1985), pp.â187â97.
 Whittaker, 1910, pp.â133,â148â9; Darrigol, 2012, pp.â212,â229â31.
Sources
M. Born and E. Wolf, 1970, Principles of Optics, 4th Ed., Oxford: Pergamon Press.
J.Z. Buchwald, 1989, The Rise of the Wave Theory of Light: Optical Theory and Experiment in the Early Nineteenth Century, University of Chicago Press, ISBN 0-226-07886-8.
R.E. Collin, 1966, Foundations for Microwave Engineering, Tokyo: McGraw-Hill.
O. Darrigol, 2012, A History of Optics: From Greek Antiquity to the Nineteenth Century, Oxford, ISBN 978-0-19-964437-7.
A. Fresnel, 1866â (ed.â H. de Senarmont, E. Verdet, and L. Fresnel), Oeuvres complÃ¨tes d'Augustin Fresnel, Paris: Imprimerie ImpÃ©riale (3 vols., 1866â70), vol.â1 (1866).
E. Hecht, 1987, Optics, 2nd Ed., Addison Wesley, ISBN 0-201-11609-X.
E. Hecht, 2002, Optics, 4th Ed., Addison Wesley, ISBN 0-321-18878-0.
F.A. Jenkins and H.E. White, 1976, Fundamentals of Optics, 4th Ed., New York: McGraw-Hill, ISBN 0-07-032330-5.
H. Lloyd, 1834, "Report on the progress and present state of physical optics", Report of the Fourth Meeting of the British Association for the Advancement of Science (held at Edinburgh in 1834), London: J. Murray, 1835, pp.â295â413.
W. Whewell, 1857, History of the Inductive Sciences: From the Earliest to the Present Time, 3rd Ed., London: J.W. Parker & Son, vol.â2.
E. T. Whittaker, 1910, A History of the Theories of Aether and Electricity: From the Age of Descartes to the Close of the Nineteenth Century, London: Longmans, Green, & Co.
Further reading

This further reading section may contain inappropriate or excessive suggestions that may not follow Wikipedia's guidelines. Please ensure that only a reasonable number of balanced, topical, reliable, and notable further reading suggestions are given; removing less relevant or redundant publications with the same point of view where appropriate. Consider utilising appropriate texts as inline sources or creating a separate bibliography article. (October 2014) (Learn how and when to remove this template message)
Woan, G. (2010). The Cambridge Handbook of Physics Formulas. Cambridge University Press. ISBN 978-0-521-57507-2.
Griffiths, David J. (2017). "Chapter 9.3: Electromagnetic Waves in Matter". Introduction to Electrodynamics (4th ed.). Cambridge University Press. ISBN 978-1-108-42041-9.
Band, Y. B. (2010). Light and Matter: Electromagnetism, Optics, Spectroscopy and Lasers. John Wiley & Sons. ISBN 978-0-471-89931-0.
Kenyon, I. R. (2008). The Light Fantastic â Introduction to Classic and Quantum Optics. Oxford University Press. ISBN 978-0-19-856646-5.
Encyclopaedia of Physics (2nd Edition), R.G. Lerner, G.L. Trigg, VHC publishers, 1991, ISBN (Verlagsgesellschaft) 3-527-26954-1, ISBN (VHC Inc.) 0-89573-752-3
McGraw Hill Encyclopaedia of Physics (2nd Edition), C.B. Parker, 1994, ISBN 0-07-051400-3
External links
Fresnel Equations â Wolfram.
Fresnel equations calculator
FreeSnell â Free software computes the optical properties of multilayer materials.
Thinfilm â Web interface for calculating optical properties of thin films and multilayer materials (reflection & transmission coefficients, ellipsometric parameters Psi & Delta).
Simple web interface for calculating single-interface reflection and refraction angles and strengths.
Reflection and transmittance for two dielectrics[permanent dead link] â Mathematica interactive webpage that shows the relations between index of refraction and reflection.
A self-contained first-principles derivation of the transmission and reflection probabilities from a multilayer with complex indices of refraction.
Stylised atom with three Bohr model orbits and stylised nucleus.svgPhysics portal
Categories: LightGeometrical opticsPhysical opticsPolarization (waves)Equations of physicsHistory of physics
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Bahasa Indonesia
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
ä¸­æ
12 more
Edit links
This page was last edited on 27 October 2020, at 08:48 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Polarization (waves)
From Wikipedia, the free encyclopedia
  (Redirected from Polarized sunglasses)
Jump to navigationJump to search
For other uses, see Polarization.

Circular polarization on rubber thread, converted to linear polarization
Polarization (also polarisation) is a property applying to transverse waves that specifies the geometrical orientation of the oscillations.[1][2][3][4][5] In a transverse wave, the direction of the oscillation is perpendicular to the direction of motion of the wave.[4] A simple example of a polarized transverse wave is vibrations traveling along a taut string (see image); for example, in a musical instrument like a guitar string. Depending on how the string is plucked, the vibrations can be in a vertical direction, horizontal direction, or at any angle perpendicular to the string. In contrast, in longitudinal waves, such as sound waves in a liquid or gas, the displacement of the particles in the oscillation is always in the direction of propagation, so these waves do not exhibit polarization. Transverse waves that exhibit polarization include electromagnetic waves such as light and radio waves, gravitational waves,[6] and transverse sound waves (shear waves) in solids.

An electromagnetic wave such as light consists of a coupled oscillating electric field and magnetic field which are always perpendicular to each other; by convention, the "polarization" of electromagnetic waves refers to the direction of the electric field. In linear polarization, the fields oscillate in a single direction. In circular or elliptical polarization, the fields rotate at a constant rate in a plane as the wave travels. The rotation can have two possible directions; if the fields rotate in a right hand sense with respect to the direction of wave travel, it is called right circular polarization, while if the fields rotate in a left hand sense, it is called left circular polarization.

Light or other electromagnetic radiation from many sources, such as the sun, flames, and incandescent lamps, consists of short wave trains with an equal mixture of polarizations; this is called unpolarized light. Polarized light can be produced by passing unpolarized light through a polarizer, which allows waves of only one polarization to pass through. The most common optical materials do not affect the polarization of light, however, some materialsâthose that exhibit birefringence, dichroism, or optical activityâaffect light differently depending on its polarization. Some of these are used to make polarizing filters. Light is also partially polarized when it reflects from a surface.

According to quantum mechanics, electromagnetic waves can also be viewed as streams of particles called photons. When viewed in this way, the polarization of an electromagnetic wave is determined by a quantum mechanical property of photons called their spin.[7][8] A photon has one of two possible spins: it can either spin in a right hand sense or a left hand sense about its direction of travel. Circularly polarized electromagnetic waves are composed of photons with only one type of spin, either right- or left-hand. Linearly polarized waves consist of photons that are in a superposition of right and left circularly polarized states, with equal amplitude and phases synchronized to give oscillation in a plane.[8]

Polarization is an important parameter in areas of science dealing with transverse waves, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.


Contents
1	Introduction
1.1	Wave propagation and polarization
1.1.1	Transverse electromagnetic waves
1.1.2	Non-transverse waves
2	Polarization state
2.1	Polarization ellipse
2.2	Jones vector
2.3	Coordinate frame
2.3.1	s and p designations
2.4	Unpolarized and partially polarized light
2.4.1	Definition
2.4.2	Motivation
2.4.3	Coherency matrix
2.4.4	Stokes parameters
2.4.5	PoincarÃ© sphere
3	Implications for reflection and propagation
3.1	Polarization in wave propagation
3.1.1	Birefringence
3.1.2	Dichroism
3.2	Specular reflection
4	Measurement techniques involving polarization
4.1	Measurement of stress
4.2	Ellipsometry
4.3	Geology
4.4	Chemistry
4.5	Astronomy
5	Applications and examples
5.1	Polarized sunglasses
5.2	Sky polarization and photography
5.3	Display technologies
5.4	Radio transmission and reception
5.5	Polarization and vision
5.6	Angular momentum using circular polarization
6	See also
7	References
7.1	Cited references
7.2	General references
8	External links
Introduction

Wave propagation and polarization

cross linear polarized
Most sources of light are classified as incoherent and unpolarized (or only "partially polarized") because they consist of a random mixture of waves having different spatial characteristics, frequencies (wavelengths), phases, and polarization states. However, for understanding electromagnetic waves and polarization in particular, it is easier to just consider coherent plane waves; these are sinusoidal waves of one particular direction (or wavevector), frequency, phase, and polarization state. Characterizing an optical system in relation to a plane wave with those given parameters can then be used to predict its response to a more general case, since a wave with any specified spatial structure can be decomposed into a combination of plane waves (its so-called angular spectrum). Incoherent states can be modeled stochastically as a weighted combination of such uncorrelated waves with some distribution of frequencies (its spectrum), phases, and polarizations.

Transverse electromagnetic waves

A "vertically polarized" electromagnetic wave of wavelength Î» has its electric field vector E (red) oscillating in the vertical direction. The magnetic field B (or H) is always at right angles to it (blue), and both are perpendicular to the direction of propagation (z).
Electromagnetic waves (such as light), traveling in free space or another homogeneous isotropic non-attenuating medium, are properly described as transverse waves, meaning that a plane wave's electric field vector E and magnetic field H are in directions perpendicular to (or "transverse" to) the direction of wave propagation; E and H are also perpendicular to each other. By convention, the "polarization" direction of an electromagnetic wave is given by its electric field vector. Considering a monochromatic plane wave of optical frequency f (light of vacuum wavelength Î» has a frequency of f = c/Î» where c is the speed of light), let us take the direction of propagation as the z axis. Being a transverse wave the E and H fields must then contain components only in the x and y directions whereas Ez = Hz = 0. Using complex (or phasor) notation, the instantaneous physical electric and magnetic fields are given by the real parts of the complex quantities occurring in the following equations. As a function of time t and spatial position z (since for a plane wave in the +z direction the fields have no dependence on x or y) these complex fields can be written as:

{\displaystyle {\vec {E}}(z,t)={\begin{bmatrix}e_{x}\\e_{y}\\0\end{bmatrix}}\;e^{i2\pi \left({\frac {z}{\lambda }}-{\frac {t}{T}}\right)}={\begin{bmatrix}e_{x}\\e_{y}\\0\end{bmatrix}}\;e^{i(kz-\omega t)}}{\displaystyle {\vec {E}}(z,t)={\begin{bmatrix}e_{x}\\e_{y}\\0\end{bmatrix}}\;e^{i2\pi \left({\frac {z}{\lambda }}-{\frac {t}{T}}\right)}={\begin{bmatrix}e_{x}\\e_{y}\\0\end{bmatrix}}\;e^{i(kz-\omega t)}}
and

{\displaystyle {\vec {H}}(z,t)={\begin{bmatrix}h_{x}\\h_{y}\\0\end{bmatrix}}\;e^{i2\pi \left({\frac {z}{\lambda }}-{\frac {t}{T}}\right)}={\begin{bmatrix}h_{x}\\h_{y}\\0\end{bmatrix}}\;e^{i(kz-\omega t)}}{\displaystyle {\vec {H}}(z,t)={\begin{bmatrix}h_{x}\\h_{y}\\0\end{bmatrix}}\;e^{i2\pi \left({\frac {z}{\lambda }}-{\frac {t}{T}}\right)}={\begin{bmatrix}h_{x}\\h_{y}\\0\end{bmatrix}}\;e^{i(kz-\omega t)}}
where Î» = Î»0/n is the wavelength in the medium (whose refractive index is n) and T = 1/f is the period of the wave. Here ex, ey, hx, and hy are complex numbers. In the second more compact form, as these equations are customarily expressed, these factors are described using the wavenumber {\displaystyle k=2\pi n/\lambda _{0}}{\displaystyle k=2\pi n/\lambda _{0}} and angular frequency (or "radian frequency") {\displaystyle \omega =2\pi f}\omega =2\pi f. In a more general formulation with propagation not restricted to the +z direction, then the spatial dependence kz is replaced by {\displaystyle {\vec {k}}\cdot {\vec {r}}}\vec{k} \cdot \vec{r} where {\displaystyle {\vec {k}}}{\vec {k}} is called the wave vector, the magnitude of which is the wavenumber.

Thus the leading vectors e and h each contain up to two nonzero (complex) components describing the amplitude and phase of the wave's x and y polarization components (again, there can be no z polarization component for a transverse wave in the +z direction). For a given medium with a characteristic impedance {\displaystyle \eta }\eta , h is related to e by:

{\displaystyle h_{y}={\frac {e_{x}}{\eta }}}{\displaystyle h_{y}={\frac {e_{x}}{\eta }}}
and

{\displaystyle h_{x}=-{\frac {e_{y}}{\eta }}}{\displaystyle h_{x}=-{\frac {e_{y}}{\eta }}}.
In a dielectric, Î· is real and has the value Î·0/n, where n is the refractive index and Î·0 is the impedance of free space. The impedance will be complex in a conducting medium.[clarification needed] Note that given that relationship, the dot product of E and H must be zero:[dubious â discuss]

{\displaystyle {\vec {E}}\left({\vec {r}},t\right)\cdot {\vec {H}}\left({\vec {r}},t\right)=e_{x}h_{x}+e_{y}h_{y}+e_{z}h_{z}=e_{x}\left(-{\frac {e_{y}}{\eta }}\right)+e_{y}\left({\frac {e_{x}}{\eta }}\right)+0\cdot 0=0}{\displaystyle {\vec {E}}\left({\vec {r}},t\right)\cdot {\vec {H}}\left({\vec {r}},t\right)=e_{x}h_{x}+e_{y}h_{y}+e_{z}h_{z}=e_{x}\left(-{\frac {e_{y}}{\eta }}\right)+e_{y}\left({\frac {e_{x}}{\eta }}\right)+0\cdot 0=0}
indicating that these vectors are orthogonal (at right angles to each other), as expected.

So knowing the propagation direction (+z in this case) and Î·, one can just as well specify the wave in terms of just ex and ey describing the electric field. The vector containing ex and ey (but without the z component which is necessarily zero for a transverse wave) is known as a Jones vector. In addition to specifying the polarization state of the wave, a general Jones vector also specifies the overall magnitude and phase of that wave. Specifically, the intensity of the light wave is proportional to the sum of the squared magnitudes of the two electric field components:

{\displaystyle I=\left(\left|e_{x}\right|^{2}+\left|e_{y}\right|^{2}\right)\,{\frac {1}{2\eta }}}{\displaystyle I=\left(\left|e_{x}\right|^{2}+\left|e_{y}\right|^{2}\right)\,{\frac {1}{2\eta }}}
however the wave's state of polarization is only dependent on the (complex) ratio of ey to ex. So let us just consider waves whose |ex|2 + |ey|2 = 1; this happens to correspond to an intensity of about .00133 watts per square meter in free space (where {\displaystyle \eta =}\eta =  {\displaystyle \eta _{0}}\eta _{0}). And since the absolute phase of a wave is unimportant in discussing its polarization state, let us stipulate that the phase of ex is zero, in other words ex is a real number while ey may be complex. Under these restrictions, ex and ey can be represented as follows:

{\displaystyle e_{x}={\sqrt {\frac {1+Q}{2}}}}{\displaystyle e_{x}={\sqrt {\frac {1+Q}{2}}}}
{\displaystyle e_{y}={\sqrt {\frac {1-Q}{2}}}\,e^{i\phi }}{\displaystyle e_{y}={\sqrt {\frac {1-Q}{2}}}\,e^{i\phi }}
where the polarization state is now fully parameterized by the value of Q (such that â1 < Q < 1) and the relative phase {\displaystyle \phi }\phi .

Non-transverse waves
In addition to transverse waves, there are many wave motions where the oscillation is not limited to directions perpendicular to the direction of propagation. These cases are far beyond the scope of the current article which concentrates on transverse waves (such as most electromagnetic waves in bulk media), however one should be aware of cases where the polarization of a coherent wave cannot be described simply using a Jones vector, as we have just done.

Just considering electromagnetic waves, we note that the preceding discussion strictly applies to plane waves in a homogeneous isotropic non-attenuating medium, whereas in an anisotropic medium (such as birefringent crystals as discussed below) the electric or magnetic field may have longitudinal as well as transverse components. In those cases the electric displacement D and magnetic flux density B[clarification needed] still obey the above geometry but due to anisotropy in the electric susceptibility (or in the magnetic permeability), now given by a tensor, the direction of E (or H) may differ from that of D (or B). Even in isotropic media, so-called inhomogeneous waves can be launched into a medium whose refractive index has a significant imaginary part (or "extinction coefficient") such as metals;[clarification needed] these fields are also not strictly transverse.[9]:179â184[10]:51â52 Surface waves or waves propagating in a waveguide (such as an optical fiber) are generally not transverse waves, but might be described as an electric or magnetic transverse mode, or a hybrid mode.

Even in free space, longitudinal field components can be generated in focal regions, where the plane wave approximation breaks down. An extreme example is radially or tangentially polarized light, at the focus of which the electric or magnetic field respectively is entirely longitudinal (along the direction of propagation).[11]

For longitudinal waves such as sound waves in fluids, the direction of oscillation is by definition along the direction of travel, so the issue of polarization is not normally even mentioned. On the other hand, sound waves in a bulk solid can be transverse as well as longitudinal, for a total of three polarization components. In this case, the transverse polarization is associated with the direction of the shear stress and displacement in directions perpendicular to the propagation direction, while the longitudinal polarization describes compression of the solid and vibration along the direction of propagation. The differential propagation of transverse and longitudinal polarizations is important in seismology.

Polarization state
Further information: Linear polarization, Circular polarization, and Elliptical polarization

Electric field oscillation
Polarization is best understood by initially considering only pure polarization states, and only a coherent sinusoidal wave at some optical frequency. The vector in the adjacent diagram might describe the oscillation of the electric field emitted by a single-mode laser (whose oscillation frequency would be typically 1015 times faster). The field oscillates in the x-y plane, along the page, with the wave propagating in the z direction, perpendicular to the page. The first two diagrams below trace the electric field vector over a complete cycle for linear polarization at two different orientations; these are each considered a distinct state of polarization (SOP). Note that the linear polarization at 45Â° can also be viewed as the addition of a horizontally linearly polarized wave (as in the leftmost figure) and a vertically polarized wave of the same amplitude in the same phase.

Polarisation state - Linear polarization parallel to x axis.svg
Polarisation state - Linear polarization oriented at +45deg.svg
Polarisation state - Right-elliptical polarization A.svg

Polarisation state - Right-circular polarization.svg

Polarisation state - Left-circular polarization.svg


Animation showing four different polarization states and two orthogonal projections.

A circularly polarized wave as a sum of two linearly polarized components 90Â° out of phase
Now if one were to introduce a phase shift in between those horizontal and vertical polarization components, one would generally obtain elliptical polarization[12] as is shown in the third figure. When the phase shift is exactly Â±90Â°, then circular polarization is produced (fourth and fifth figures). Thus is circular polarization created in practice, starting with linearly polarized light and employing a quarter-wave plate to introduce such a phase shift. The result of two such phase-shifted components in causing a rotating electric field vector is depicted in the animation on the right. Note that circular or elliptical polarization can involve either a clockwise or counterclockwise rotation of the field. These correspond to distinct polarization states, such as the two circular polarizations shown above.

Of course the orientation of the x and y axes used in this description is arbitrary. The choice of such a coordinate system and viewing the polarization ellipse in terms of the x and y polarization components, corresponds to the definition of the Jones vector (below) in terms of those basis polarizations. One would typically choose axes to suit a particular problem such as x being in the plane of incidence. Since there are separate reflection coefficients for the linear polarizations in and orthogonal to the plane of incidence (p and s polarizations, see below), that choice greatly simplifies the calculation of a wave's reflection from a surface.

Moreover, one can use as basis functions any pair of orthogonal polarization states, not just linear polarizations. For instance, choosing right and left circular polarizations as basis functions simplifies the solution of problems involving circular birefringence (optical activity) or circular dichroism.


Polarization ellipse
Main article: Polarization ellipse
Polarisation ellipse2.svg
Consider a purely polarized monochromatic wave. If one were to plot the electric field vector over one cycle of oscillation, an ellipse would generally be obtained, as is shown in the figure, corresponding to a particular state of elliptical polarization. Note that linear polarization and circular polarization can be seen as special cases of elliptical polarization.

A polarization state can then be described in relation to the geometrical parameters of the ellipse, and its "handedness", that is, whether the rotation around the ellipse is clockwise or counter clockwise. One parameterization of the elliptical figure specifies the orientation angle Ï, defined as the angle between the major axis of the ellipse and the x-axis[13] along with the ellipticity Îµ=a/b, the ratio of the ellipse's major to minor axis.[14][15][16] (also known as the axial ratio). The ellipticity parameter is an alternative parameterization of an ellipse's eccentricity {\displaystyle e={\sqrt {1-b^{2}/a^{2}}}}{\displaystyle e={\sqrt {1-b^{2}/a^{2}}}}, or the ellipticity angle, Ï = arctan b/a= arctan 1/Îµ as is shown in the figure.[13] The angle Ï is also significant in that the latitude (angle from the equator) of the polarization state as represented on the PoincarÃ© sphere (see below) is equal to Â±2Ï. The special cases of linear and circular polarization correspond to an ellipticity Îµ of infinity and unity (or Ï of zero and 45Â°) respectively.

Jones vector
Main article: Jones vector
Full information on a completely polarized state is also provided by the amplitude and phase of oscillations in two components of the electric field vector in the plane of polarization. This representation was used above to show how different states of polarization are possible. The amplitude and phase information can be conveniently represented as a two-dimensional complex vector (the Jones vector):

{\displaystyle \mathbf {e} ={\begin{bmatrix}a_{1}e^{i\theta _{1}}\\a_{2}e^{i\theta _{2}}\end{bmatrix}}.}{\displaystyle \mathbf {e} ={\begin{bmatrix}a_{1}e^{i\theta _{1}}\\a_{2}e^{i\theta _{2}}\end{bmatrix}}.}
Here {\displaystyle a_{1}}a_{1} and {\displaystyle a_{2}}a_{2} denote the amplitude of the wave in the two components of the electric field vector, while {\displaystyle \theta _{1}}\theta _{1} and {\displaystyle \theta _{2}}\theta _{2} represent the phases. The product of a Jones vector with a complex number of unit modulus gives a different Jones vector representing the same ellipse, and thus the same state of polarization. The physical electric field, as the real part of the Jones vector, would be altered but the polarization state itself is independent of absolute phase. The basis vectors used to represent the Jones vector need not represent linear polarization states (i.e. be real). In general any two orthogonal states can be used, where an orthogonal vector pair is formally defined as one having a zero inner product. A common choice is left and right circular polarizations, for example to model the different propagation of waves in two such components in circularly birefringent media (see below) or signal paths of coherent detectors sensitive to circular polarization.

Coordinate frame
Regardless of whether polarization state is represented using geometric parameters or Jones vectors, implicit in the parameterization is the orientation of the coordinate frame. This permits a degree of freedom, namely rotation about the propagation direction. When considering light that is propagating parallel to the surface of the Earth, the terms "horizontal" and "vertical" polarization are often used, with the former being associated with the first component of the Jones vector, or zero azimuth angle. On the other hand, in astronomy the equatorial coordinate system is generally used instead, with the zero azimuth (or position angle, as it is more commonly called in astronomy to avoid confusion with the horizontal coordinate system) corresponding to due north.

s and p designations
File:E xy deformation.webm
Electromagnetic vectors for {\textstyle {\textbf {E}}}{\textstyle {\textbf {E}}}, {\textstyle {\textbf {B}}}{\textstyle {\textbf {B}}} and {\textstyle {\textbf {k}}}{\textstyle {\textbf {k}}} with {\textstyle {\textbf {E}}={\textbf {E}}(x,y)}{\textstyle {\textbf {E}}={\textbf {E}}(x,y)} along with 3 planar projections and a deformation surface of total electric field. The light is always s-polarized in the xy plane. {\textstyle \theta }{\textstyle \theta } is the polar angle of {\textstyle {\textbf {k}}}{\textstyle {\textbf {k}}} and {\textstyle \varphi _{E}}{\textstyle \varphi _{E}} is the azimuthal angle of {\textstyle {\textbf {E}}}{\textstyle {\textbf {E}}}.
Another coordinate system frequently used relates to the plane of incidence. This is the plane made by the incoming propagation direction and the vector perpendicular to the plane of an interface, in other words, the plane in which the ray travels before and after reflection or refraction. The component of the electric field parallel to this plane is termed p-like (parallel) and the component perpendicular to this plane is termed s-like (from senkrecht, German for perpendicular). Polarized light with its electric field along the plane of incidence is thus denoted p-polarized, while light whose electric field is normal to the plane of incidence is called s-polarized. P polarization is commonly referred to as transverse-magnetic (TM), and has also been termed pi-polarized or tangential plane polarized. S polarization is also called transverse-electric (TE), as well as sigma-polarized or sagittal plane polarized.

Unpolarized and partially polarized light

This section may contain content that is repetitive or redundant of text elsewhere in the article. Please help improve it by merging similar text or removing repeated statements. (July 2014)
Definition
Natural light, like most other common sources of visible light, is incoherent: radiation is produced independently by a large number of atoms or molecules whose emissions are uncorrelated and generally of random polarizations. In this case the light is said to be unpolarized. This term is somewhat inexact, since at any instant of time at one location there is a definite direction to the electric and magnetic fields, however it implies that the polarization changes so quickly in time that it will not be measured or relevant to the outcome of an experiment. A so-called depolarizer acts on a polarized beam to create one which is actually fully polarized at every point, but in which the polarization varies so rapidly across the beam that it may be ignored in the intended applications.

Unpolarized light can be described as a mixture of two independent oppositely polarized streams, each with half the intensity.[17][18] Light is said to be partially polarized when there is more power in one of these streams than the other. At any particular wavelength, partially polarized light can be statistically described as the superposition of a completely unpolarized component and a completely polarized one.[19]:330 One may then describe the light in terms of the degree of polarization and the parameters of the polarized component. That polarized component can be described in terms of a Jones vector or polarization ellipse, as is detailed above. However, in order to also describe the degree of polarization, one normally employs Stokes parameters (see below) to specify a state of partial polarization.[19]:351,374â375

Motivation
The transmission of plane waves through a homogeneous medium are fully described in terms of Jones vectors and 2Ã2 Jones matrices. However, in practice there are cases in which all of the light cannot be viewed in such a simple manner due to spatial inhomogeneities or the presence of mutually incoherent waves. So-called depolarization, for instance, cannot be described using Jones matrices. For these cases it is usual instead to use a 4Ã4 matrix that acts upon the Stokes 4-vector. Such matrices were first used by Paul Soleillet in 1929, although they have come to be known as Mueller matrices. While every Jones matrix has a Mueller matrix, the reverse is not true. Mueller matrices are then used to describe the observed polarization effects of the scattering of waves from complex surfaces or ensembles of particles, as shall now be presented.[19]:377â379

Coherency matrix
The Jones vector perfectly describes the state of polarization and phase of a single monochromatic wave, representing a pure state of polarization as described above. However any mixture of waves of different polarizations (or even of different frequencies) do not correspond to a Jones vector. In so-called partially polarized radiation the fields are stochastic, and the variations and correlations between components of the electric field can only be described statistically. One such representation is the coherency matrix:[20]:137â142

{\displaystyle {\begin{aligned}\mathbf {\Psi } &=\left\langle \mathbf {e} \mathbf {e} ^{\dagger }\right\rangle \\&=\left\langle {\begin{bmatrix}e_{1}e_{1}^{*}&e_{1}e_{2}^{*}\\e_{2}e_{1}^{*}&e_{2}e_{2}^{*}\end{bmatrix}}\right\rangle \\&=\left\langle {\begin{bmatrix}a_{1}^{2}&a_{1}a_{2}e^{i\left(\theta _{1}-\theta _{2}\right)}\\a_{1}a_{2}e^{-i\left(\theta _{1}-\theta _{2}\right)}&a_{2}^{2}\end{bmatrix}}\right\rangle \end{aligned}}}{\displaystyle {\begin{aligned}\mathbf {\Psi } &=\left\langle \mathbf {e} \mathbf {e} ^{\dagger }\right\rangle \\&=\left\langle {\begin{bmatrix}e_{1}e_{1}^{*}&e_{1}e_{2}^{*}\\e_{2}e_{1}^{*}&e_{2}e_{2}^{*}\end{bmatrix}}\right\rangle \\&=\left\langle {\begin{bmatrix}a_{1}^{2}&a_{1}a_{2}e^{i\left(\theta _{1}-\theta _{2}\right)}\\a_{1}a_{2}e^{-i\left(\theta _{1}-\theta _{2}\right)}&a_{2}^{2}\end{bmatrix}}\right\rangle \end{aligned}}}
where angular brackets denote averaging over many wave cycles. Several variants of the coherency matrix have been proposed: the Wiener coherency matrix and the spectral coherency matrix of Richard Barakat measure the coherence of a spectral decomposition of the signal, while the Wolf coherency matrix averages over all time/frequencies.

The coherency matrix contains all second order statistical information about the polarization. This matrix can be decomposed into the sum of two idempotent matrices, corresponding to the eigenvectors of the coherency matrix, each representing a polarization state that is orthogonal to the other. An alternative decomposition is into completely polarized (zero determinant) and unpolarized (scaled identity matrix) components. In either case, the operation of summing the components corresponds to the incoherent superposition of waves from the two components. The latter case gives rise to the concept of the "degree of polarization"; i.e., the fraction of the total intensity contributed by the completely polarized component.

Stokes parameters
Main article: Stokes parameters
The coherency matrix is not easy to visualize, and it is therefore common to describe incoherent or partially polarized radiation in terms of its total intensity (I), (fractional) degree of polarization (p), and the shape parameters of the polarization ellipse. An alternative and mathematically convenient description is given by the Stokes parameters, introduced by George Gabriel Stokes in 1852. The relationship of the Stokes parameters to intensity and polarization ellipse parameters is shown in the equations and figure below.

{\displaystyle S_{0}=I\,}S_0 = I \,
{\displaystyle S_{1}=Ip\cos 2\psi \cos 2\chi \,}{\displaystyle S_{1}=Ip\cos 2\psi \cos 2\chi \,}
{\displaystyle S_{2}=Ip\sin 2\psi \cos 2\chi \,}{\displaystyle S_{2}=Ip\sin 2\psi \cos 2\chi \,}
{\displaystyle S_{3}=Ip\sin 2\chi \,}{\displaystyle S_{3}=Ip\sin 2\chi \,}
Here Ip, 2Ï and 2Ï are the spherical coordinates of the polarization state in the three-dimensional space of the last three Stokes parameters. Note the factors of two before Ï and Ï corresponding respectively to the facts that any polarization ellipse is indistinguishable from one rotated by 180Â°, or one with the semi-axis lengths swapped accompanied by a 90Â° rotation. The Stokes parameters are sometimes denoted I, Q, U and V.

PoincarÃ© sphere
Neglecting the first Stokes parameter S0 (or I), the three other Stokes parameters can be plotted directly in three-dimensional Cartesian coordinates. For a given power in the polarized component given by

{\displaystyle P={\sqrt {S_{1}^{2}+S_{2}^{2}+S_{3}^{2}}}}{\displaystyle P={\sqrt {S_{1}^{2}+S_{2}^{2}+S_{3}^{2}}}}
the set of all polarization states are then mapped to points on the surface of the so-called PoincarÃ© sphere (but of radius P), as shown in the accompanying diagram.


PoincarÃ© sphere, on or beneath which the three Stokes parameters [S1, S2, S3] (or [Q,â¯U,â¯V]) are plotted in Cartesian coordinates

Depiction of the polarization states on PoincarÃ© sphere
Often the total beam power is not of interest, in which case a normalized Stokes vector is used by dividing the Stokes vector by the total intensity S0:

{\displaystyle \mathbf {S'} ={\frac {1}{S_{0}}}{\begin{bmatrix}S_{0}\\S_{1}\\S_{2}\\S_{3}\end{bmatrix}}.}{\displaystyle \mathbf {S'} ={\frac {1}{S_{0}}}{\begin{bmatrix}S_{0}\\S_{1}\\S_{2}\\S_{3}\end{bmatrix}}.}
The normalized Stokes vector {\displaystyle \mathbf {S'} }\mathbf{S'} then has unity power ({\displaystyle S'_{0}=1}{\displaystyle S'_{0}=1}) and the three significant Stokes parameters plotted in three dimensions will lie on the unity-radius PoincarÃ© sphere for pure polarization states (where {\displaystyle P'_{0}=1}{\displaystyle P'_{0}=1}). Partially polarized states will lie inside the PoincarÃ© sphere at a distance of {\displaystyle P'={\sqrt {S_{1}'^{2}+S_{2}'^{2}+S_{3}'^{2}}}}{\displaystyle P'={\sqrt {S_{1}'^{2}+S_{2}'^{2}+S_{3}'^{2}}}} from the origin. When the non-polarized component is not of interest, the Stokes vector can be further normalized to obtain

{\displaystyle \mathbf {S''} ={\frac {1}{P'}}{\begin{bmatrix}1\\S'_{1}\\S'_{2}\\S'_{3}\end{bmatrix}}={\frac {1}{P}}{\begin{bmatrix}S_{0}\\S_{1}\\S_{2}\\S_{3}\end{bmatrix}}.}{\displaystyle \mathbf {S''} ={\frac {1}{P'}}{\begin{bmatrix}1\\S'_{1}\\S'_{2}\\S'_{3}\end{bmatrix}}={\frac {1}{P}}{\begin{bmatrix}S_{0}\\S_{1}\\S_{2}\\S_{3}\end{bmatrix}}.}
When plotted, that point will lie on the surface of the unity-radius PoincarÃ© sphere and indicate the state of polarization of the polarized component.

Any two antipodal points on the PoincarÃ© sphere refer to orthogonal polarization states. The overlap between any two polarization states is dependent solely on the distance between their locations along the sphere. This property, which can only be true when pure polarization states are mapped onto a sphere, is the motivation for the invention of the PoincarÃ© sphere and the use of Stokes parameters, which are thus plotted on (or beneath) it.

Note that the IEEE defines RHCP and LHCP the opposite as those used by Physicists. The IEEE 1979 Antenna Standard will show RHCP on the South Pole of the Poincare Sphere. The IEEE defines RHCP using the right hand with thumb pointing in the direction of transmit, and the fingers showing the direction of rotation of the E field with time. The rationale for the opposite conventions used by Physicists and Engineers is that Astronomical Observations are always done with the incoming wave traveling toward the observer, where as for most engineers, they are assumed to be standing behind the transmitter watching the wave traveling away from them. This article is not using the IEEE 1979 Antenna Standard and is not using the +t convention typically used in IEEE work.

Implications for reflection and propagation
Polarization in wave propagation
In a vacuum, the components of the electric field propagate at the speed of light, so that the phase of the wave varies in space and time while the polarization state does not. That is, the electric field vector e of a plane wave in the +z direction follows:

{\displaystyle \mathbf {e} (z+\Delta z,t+\Delta t)=\mathbf {e} (z,t)e^{ik(c\Delta t-\Delta z)},}{\displaystyle \mathbf {e} (z+\Delta z,t+\Delta t)=\mathbf {e} (z,t)e^{ik(c\Delta t-\Delta z)},}
where k is the wavenumber. As noted above, the instantaneous electric field is the real part of the product of the Jones vector times the phase factor {\displaystyle e^{-i\omega t}}e^{{-i\omega t}}. When an electromagnetic wave interacts with matter, its propagation is altered according to the material's (complex) index of refraction. When the real or imaginary part of that refractive index is dependent on the polarization state of a wave, properties known as birefringence and polarization dichroism (or diattenuation) respectively, then the polarization state of a wave will generally be altered.

In such media, an electromagnetic wave with any given state of polarization may be decomposed into two orthogonally polarized components that encounter different propagation constants. The effect of propagation over a given path on those two components is most easily characterized in the form of a complex 2Ã2 transformation matrix J known as a Jones matrix:

{\displaystyle \mathbf {e'} =\mathbf {J} \mathbf {e} .}\mathbf{e'} = \mathbf{J}\mathbf{e}.
The Jones matrix due to passage through a transparent material is dependent on the propagation distance as well as the birefringence. The birefringence (as well as the average refractive index) will generally be dispersive, that is, it will vary as a function of optical frequency (wavelength). In the case of non-birefringent materials, however, the 2Ã2 Jones matrix is the identity matrix (multiplied by a scalar phase factor and attenuation factor), implying no change in polarization during propagation.

For propagation effects in two orthogonal modes, the Jones matrix can be written as

{\displaystyle \mathbf {J} =\mathbf {T} {\begin{bmatrix}g_{1}&0\\0&g_{2}\end{bmatrix}}\mathbf {T} ^{-1},}{\displaystyle \mathbf {J} =\mathbf {T} {\begin{bmatrix}g_{1}&0\\0&g_{2}\end{bmatrix}}\mathbf {T} ^{-1},}
where g1 and g2 are complex numbers describing the phase delay and possibly the amplitude attenuation due to propagation in each of the two polarization eigenmodes. T is a unitary matrix representing a change of basis from these propagation modes to the linear system used for the Jones vectors; in the case of linear birefringence or diattenuation the modes are themselves linear polarization states so T and Tâ1 can be omitted if the coordinate axes have been chosen appropriately.

Birefringence
Main article: Birefringence
In media termed birefringent, in which the amplitudes are unchanged but a differential phase delay occurs, the Jones matrix is a unitary matrix: |g1| = |g2| = 1. Media termed diattenuating (or dichroic in the sense of polarization), in which only the amplitudes of the two polarizations are affected differentially, may be described using a Hermitian matrix (generally multiplied by a common phase factor). In fact, since any matrix may be written as the product of unitary and positive Hermitian matrices, light propagation through any sequence of polarization-dependent optical components can be written as the product of these two basic types of transformations.


Color pattern of a plastic box showing stress-induced birefringence when placed in between two crossed polarizers.
In birefringent media there is no attenuation, but two modes accrue a differential phase delay. Well known manifestations of linear birefringence (that is, in which the basis polarizations are orthogonal linear polarizations) appear in optical wave plates/retarders and many crystals. If linearly polarized light passes through a birefringent material, its state of polarization will generally change, unless its polarization direction is identical to one of those basis polarizations. Since the phase shift, and thus the change in polarization state, is usually wavelength-dependent, such objects viewed under white light in between two polarizers may give rise to colorful effects, as seen in the accompanying photograph.

Circular birefringence is also termed optical activity, especially in chiral fluids, or Faraday rotation, when due to the presence of a magnetic field along the direction of propagation. When linearly polarized light is passed through such an object, it will exit still linearly polarized, but with the axis of polarization rotated. A combination of linear and circular birefringence will have as basis polarizations two orthogonal elliptical polarizations; however, the term "elliptical birefringence" is rarely used.


Paths taken by vectors in the PoincarÃ© sphere under birefringence. The propagation modes (rotation axes) are shown with red, blue, and yellow lines, the initial vectors by thick black lines, and the paths they take by colored ellipses (which represent circles in three dimensions).
One can visualize the case of linear birefringence (with two orthogonal linear propagation modes) with an incoming wave linearly polarized at a 45Â° angle to those modes. As a differential phase starts to accrue, the polarization becomes elliptical, eventually changing to purely circular polarization (90Â° phase difference), then to elliptical and eventually linear polarization (180Â° phase) perpendicular to the original polarization, then through circular again (270Â° phase), then elliptical with the original azimuth angle, and finally back to the original linearly polarized state (360Â° phase) where the cycle begins anew. In general the situation is more complicated and can be characterized as a rotation in the PoincarÃ© sphere about the axis defined by the propagation modes. Examples for linear (blue), circular (red), and elliptical (yellow) birefringence are shown in the figure on the left. The total intensity and degree of polarization are unaffected. If the path length in the birefringent medium is sufficient, the two polarization components of a collimated beam (or ray) can exit the material with a positional offset, even though their final propagation directions will be the same (assuming the entrance face and exit face are parallel). This is commonly viewed using calcite crystals, which present the viewer with two slightly offset images, in opposite polarizations, of an object behind the crystal. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669.

Dichroism
Media in which transmission of one polarization mode is preferentially reduced are called dichroic or diattenuating. Like birefringence, diattenuation can be with respect to linear polarization modes (in a crystal) or circular polarization modes (usually in a liquid).

Devices that block nearly all of the radiation in one mode are known as polarizing filters or simply "polarizers". This corresponds to g2=0 in the above representation of the Jones matrix. The output of an ideal polarizer is a specific polarization state (usually linear polarization) with an amplitude equal to the input wave's original amplitude in that polarization mode. Power in the other polarization mode is eliminated. Thus if unpolarized light is passed through an ideal polarizer (where g1=1 and g2=0) exactly half of its initial power is retained. Practical polarizers, especially inexpensive sheet polarizers, have additional loss so that g1 < 1. However, in many instances the more relevant figure of merit is the polarizer's degree of polarization or extinction ratio, which involve a comparison of g1 to g2. Since Jones vectors refer to waves' amplitudes (rather than intensity), when illuminated by unpolarized light the remaining power in the unwanted polarization will be (g2/g1)2 of the power in the intended polarization.

Specular reflection
In addition to birefringence and dichroism in extended media, polarization effects describable using Jones matrices can also occur at (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected; for a given material those proportions (and also the phase of reflection) are dependent on the angle of incidence and are different for the s and p polarizations. Therefore, the polarization state of reflected light (even if initially unpolarized) is generally changed.


A stack of plates at Brewster's angle to a beam reflects off a fraction of the s-polarized light at each surface, leaving (after many such plates) a mainly p-polarized beam.
Any light striking a surface at a special angle of incidence known as Brewster's angle, where the reflection coefficient for p polarization is zero, will be reflected with only the s-polarization remaining. This principle is employed in the so-called "pile of plates polarizer" (see figure) in which part of the s polarization is removed by reflection at each Brewster angle surface, leaving only the p polarization after transmission through many such surfaces. The generally smaller reflection coefficient of the p polarization is also the basis of polarized sunglasses; by blocking the s (horizontal) polarization, most of the glare due to reflection from a wet street, for instance, is removed.[19]:348â350

In the important special case of reflection at normal incidence (not involving anisotropic materials) there is no particular s or p polarization. Both the x and y polarization components are reflected identically, and therefore the polarization of the reflected wave is identical to that of the incident wave. However, in the case of circular (or elliptical) polarization, the handedness of the polarization state is thereby reversed, since by convention this is specified relative to the direction of propagation. The circular rotation of the electric field around the x-y axes called "right-handed" for a wave in the +z direction is "left-handed" for a wave in the -z direction. But in the general case of reflection at a nonzero angle of incidence, no such generalization can be made. For instance, right-circularly polarized light reflected from a dielectric surface at a grazing angle, will still be right-handed (but elliptically) polarized. Linear polarized light reflected from a metal at non-normal incidence will generally become elliptically polarized. These cases are handled using Jones vectors acted upon by the different Fresnel coefficients for the s and p polarization components.

Measurement techniques involving polarization
Some optical measurement techniques are based on polarization. In many other optical techniques polarization is crucial or at least must be taken into account and controlled; such examples are too numerous to mention.

Measurement of stress

Stress in plastic glasses
In engineering, the phenomenon of stress induced birefringence allows for stresses in transparent materials to be readily observed. As noted above and seen in the accompanying photograph, the chromaticity of birefringence typically creates colored patterns when viewed in between two polarizers. As external forces are applied, internal stress induced in the material is thereby observed. Additionally, birefringence is frequently observed due to stresses "frozen in" at the time of manufacture. This is famously observed in cellophane tape whose birefringence is due to the stretching of the material during the manufacturing process.

Ellipsometry
Main article: Ellipsometry
Ellipsometry is a powerful technique for the measurement of the optical properties of a uniform surface. It involves measuring the polarization state of light following specular reflection from such a surface. This is typically done as a function of incidence angle or wavelength (or both). Since ellipsometry relies on reflection, it is not required for the sample to be transparent to light or for its back side to be accessible.

Ellipsometry can be used to model the (complex) refractive index of a surface of a bulk material. It is also very useful in determining parameters of one or more thin film layers deposited on a substrate. Due to their reflection properties, not only are the predicted magnitude of the p and s polarization components, but their relative phase shifts upon reflection, compared to measurements using an ellipsometer. A normal ellipsometer does not measure the actual reflection coefficient (which requires careful photometric calibration of the illuminating beam) but the ratio of the p and s reflections, as well as change of polarization ellipticity (hence the name) induced upon reflection by the surface being studied. In addition to use in science and research, ellipsometers are used in situ to control production processes for instance.[21]:585ff[22]:632

Geology

Photomicrograph of a volcanic sand grain; upper picture is plane-polarized light, bottom picture is cross-polarized light, scale box at left-center is 0.25 millimeter.
The property of (linear) birefringence is widespread in crystalline minerals, and indeed was pivotal in the initial discovery of polarization. In mineralogy, this property is frequently exploited using polarization microscopes, for the purpose of identifying minerals. See optical mineralogy for more details.[23]:163â164

Sound waves in solid materials exhibit polarization. Differential propagation of the three polarizations through the earth is a crucial in the field of seismology. Horizontally and vertically polarized seismic waves (shear waves)are termed SH and SV, while waves with longitudinal polarization (compressional waves) are termed P-waves.[24]:48â50[25]:56â57

Chemistry
We have seen (above) that the birefringence of a type of crystal is useful in identifying it, and thus detection of linear birefringence is especially useful in geology and mineralogy. Linearly polarized light generally has its polarization state altered upon transmission through such a crystal, making it stand out when viewed in between two crossed polarizers, as seen in the photograph, above. Likewise, in chemistry, rotation of polarization axes in a liquid solution can be a useful measurement. In a liquid, linear birefringence is impossible, however there may be circular birefringence when a chiral molecule is in solution. When the right and left handed enantiomers of such a molecule are present in equal numbers (a so-called racemic mixture) then their effects cancel out. However, when there is only one (or a preponderance of one), as is more often the case for organic molecules, a net circular birefringence (or optical activity) is observed, revealing the magnitude of that imbalance (or the concentration of the molecule itself, when it can be assumed that only one enantiomer is present). This is measured using a polarimeter in which polarized light is passed through a tube of the liquid, at the end of which is another polarizer which is rotated in order to null the transmission of light through it.[19]:360â365[26]

Astronomy
Main article: Polarization in astronomy
In many areas of astronomy, the study of polarized electromagnetic radiation from outer space is of great importance. Although not usually a factor in the thermal radiation of stars, polarization is also present in radiation from coherent astronomical sources (e.g. hydroxyl or methanol masers), and incoherent sources such as the large radio lobes in active galaxies, and pulsar radio radiation (which may, it is speculated, sometimes be coherent), and is also imposed upon starlight by scattering from interstellar dust. Apart from providing information on sources of radiation and scattering, polarization also probes the interstellar magnetic field via Faraday rotation.[27]:119,124[28]:336â337 The polarization of the cosmic microwave background is being used to study the physics of the very early universe.[29][30] Synchrotron radiation is inherently polarised. It has been suggested that astronomical sources caused the chirality of biological molecules on Earth.[31]

Applications and examples
Polarized sunglasses

Effect of a polarizer on reflection from mud flats. In the picture on the left, the horizontally oriented polarizer preferentially transmits those reflections; rotating the polarizer by 90Â° (right) as one would view using polarized sunglasses blocks almost all specularly reflected sunlight.

One can test whether sunglasses are polarized by looking through two pairs, with one perpendicular to the other. If both are polarized, all light will be blocked.
Unpolarized light, after being reflected by a specular (shiny) surface, generally obtains a degree of polarization. This phenomenon was observed in 1808 by the mathematician Ãtienne-Louis Malus, after whom Malus's law is named. Polarizing sunglasses exploit this effect to reduce glare from reflections by horizontal surfaces, notably the road ahead viewed at a grazing angle.

Wearers of polarized sunglasses will occasionally observe inadvertent polarization effects such as color-dependent birefringent effects, for example in toughened glass (e.g., car windows) or items made from transparent plastics, in conjunction with natural polarization by reflection or scattering. The polarized light from LCD monitors (see below) is very conspicuous when these are worn.

Sky polarization and photography
Further information: Polarizing filter (Photography)

The effects of a polarizing filter (right image) on the sky in a photograph
Polarization is observed in the light of the sky, as this is due to sunlight scattered by aerosols as it passes through Earth's atmosphere. The scattered light produces the brightness and color in clear skies. This partial polarization of scattered light can be used to darken the sky in photographs, increasing the contrast. This effect is most strongly observed at points on the sky making a 90Â° angle to the Sun. Polarizing filters use these effects to optimize the results of photographing scenes in which reflection or scattering by the sky is involved.[19]:346â347[32]:495â499

Sky polarization has been used for orientation in navigation. The Pfund sky compass was used in the 1950s when navigating near the poles of the Earth's magnetic field when neither the sun nor stars were visible (e.g., under daytime cloud or twilight). It has been suggested, controversially, that the Vikings exploited a similar device (the "sunstone") in their extensive expeditions across the North Atlantic in the 9thâ11th centuries, before the arrival of the magnetic compass from Asia to Europe in the 12th century. Related to the sky compass is the "polar clock", invented by Charles Wheatstone in the late 19th century.[33]:67â69

Display technologies
The principle of liquid-crystal display (LCD) technology relies on the rotation of the axis of linear polarization by the liquid crystal array. Light from the backlight (or the back reflective layer, in devices not including or requiring a backlight) first passes through a linear polarizing sheet. That polarized light passes through the actual liquid crystal layer which may be organized in pixels (for a TV or computer monitor) or in another format such as a seven-segment display or one with custom symbols for a particular product. The liquid crystal layer is produced with a consistent right (or left) handed chirality, essentially consisting of tiny helices. This causes circular birefringence, and is engineered so that there is a 90 degree rotation of the linear polarization state. However, when a voltage is applied across a cell, the molecules straighten out, lessening or totally losing the circular birefringence. On the viewing side of the display is another linear polarizing sheet, usually oriented at 90 degrees from the one behind the active layer. Therefore, when the circular birefringence is removed by the application of a sufficient voltage, the polarization of the transmitted light remains at right angles to the front polarizer, and the pixel appears dark. With no voltage, however, the 90 degree rotation of the polarization causes it to exactly match the axis of the front polarizer, allowing the light through. Intermediate voltages create intermediate rotation of the polarization axis and the pixel has an intermediate intensity. Displays based on this principle are widespread, and now are used in the vast majority of televisions, computer monitors and video projectors, rendering the previous CRT technology essentially obsolete. The use of polarization in the operation of LCD displays is immediately apparent to someone wearing polarized sunglasses, often making the display unreadable.

In a totally different sense, polarization encoding has become the leading (but not sole) method for delivering separate images to the left and right eye in stereoscopic displays used for 3D movies. This involves separate images intended for each eye either projected from two different projectors with orthogonally oriented polarizing filters or, more typically, from a single projector with time multiplexed polarization (a fast alternating polarization device for successive frames). Polarized 3D glasses with suitable polarizing filters ensure that each eye receives only the intended image. Historically such systems used linear polarization encoding because it was inexpensive and offered good separation. However circular polarization makes separation of the two images insensitive to tilting of the head, and is widely used in 3-D movie exhibition today, such as the system from RealD. Projecting such images requires screens that maintain the polarization of the projected light when viewed in reflection (such as silver screens); a normal diffuse white projection screen causes depolarization of the projected images, making it unsuitable for this application.

Although now obsolete, CRT computer displays suffered from reflection by the glass envelope, causing glare from room lights and consequently poor contrast. Several anti-reflection solutions were employed to ameliorate this problem. One solution utilized the principle of reflection of circularly polarized light. A circular polarizing filter in front of the screen allows for the transmission of (say) only right circularly polarized room light. Now, right circularly polarized light (depending on the convention used) has its electric (and magnetic) field direction rotating clockwise while propagating in the +z direction. Upon reflection, the field still has the same direction of rotation, but now propagation is in the âz direction making the reflected wave left circularly polarized. With the right circular polarization filter placed in front of the reflecting glass, the unwanted light reflected from the glass will thus be in very polarization state that is blocked by that filter, eliminating the reflection problem. The reversal of circular polarization on reflection and elimination of reflections in this manner can be easily observed by looking in a mirror while wearing 3-D movie glasses which employ left- and right-handed circular polarization in the two lenses. Closing one eye, the other eye will see a reflection in which it cannot see itself; that lens appears black. However the other lens (of the closed eye) will have the correct circular polarization allowing the closed eye to be easily seen by the open one.

Radio transmission and reception
See also: Antenna (radio) Â§ Polarization
All radio (and microwave) antennas used for transmitting or receiving are intrinsically polarized. They transmit in (or receive signals from) a particular polarization, being totally insensitive to the opposite polarization; in certain cases that polarization is a function of direction. Most antennas are nominally linearly polarized, but elliptical and circular polarization is a possibility. As is the convention in optics, the "polarization" of a radio wave is understood to refer to the polarization of its electric field, with the magnetic field being at a 90 degree rotation with respect to it for a linearly polarized wave.

The vast majority of antennas are linearly polarized. In fact it can be shown from considerations of symmetry that an antenna that lies entirely in a plane which also includes the observer, can only have its polarization in the direction of that plane. This applies to many cases, allowing one to easily infer such an antenna's polarization at an intended direction of propagation. So a typical rooftop Yagi or log-periodic antenna with horizontal conductors, as viewed from a second station toward the horizon, is necessarily horizontally polarized. But a vertical "whip antenna" or AM broadcast tower used as an antenna element (again, for observers horizontally displaced from it) will transmit in the vertical polarization. A turnstile antenna with its four arms in the horizontal plane, likewise transmits horizontally polarized radiation toward the horizon. However, when that same turnstile antenna is used in the "axial mode" (upwards, for the same horizontally-oriented structure) its radiation is circularly polarized. At intermediate elevations it is elliptically polarized.

Polarization is important in radio communications because, for instance, if one attempts to use a horizontally polarized antenna to receive a vertically polarized transmission, the signal strength will be substantially reduced (or under very controlled conditions, reduced to nothing). This principle is used in satellite television in order to double the channel capacity over a fixed frequency band. The same frequency channel can be used for two signals broadcast in opposite polarizations. By adjusting the receiving antenna for one or the other polarization, either signal can be selected without interference from the other.

Especially due to the presence of the ground, there are some differences in propagation (and also in reflections responsible for TV ghosting) between horizontal and vertical polarizations. AM and FM broadcast radio usually use vertical polarization, while television uses horizontal polarization. At low frequencies especially, horizontal polarization is avoided. That is because the phase of a horizontally polarized wave is reversed upon reflection by the ground. A distant station in the horizontal direction will receive both the direct and reflected wave, which thus tend to cancel each other. This problem is avoided with vertical polarization. Polarization is also important in the transmission of radar pulses and reception of radar reflections by the same or a different antenna. For instance, back scattering of radar pulses by rain drops can be avoided by using circular polarization. Just as specular reflection of circularly polarized light reverses the handedness of the polarization, as discussed above, the same principle applies to scattering by objects much smaller than a wavelength such as rain drops. On the other hand, reflection of that wave by an irregular metal object (such as an airplane) will typically introduce a change in polarization and (partial) reception of the return wave by the same antenna.

The effect of free electrons in the ionosphere, in conjunction with the earth's magnetic field, causes Faraday rotation, a sort of circular birefringence. This is the same mechanism which can rotate the axis of linear polarization by electrons in interstellar space as mentioned below. The magnitude of Faraday rotation caused by such a plasma is greatly exaggerated at lower frequencies, so at the higher microwave frequencies used by satellites the effect is minimal. However medium or short wave transmissions received following refraction by the ionosphere are strongly affected. Since a wave's path through the ionosphere and the earth's magnetic field vector along such a path are rather unpredictable, a wave transmitted with vertical (or horizontal) polarization will generally have a resulting polarization in an arbitrary orientation at the receiver.


Circular polarization through an airplane plastic window, 1989
Polarization and vision
Many animals are capable of perceiving some of the components of the polarization of light, e.g., linear horizontally polarized light. This is generally used for navigational purposes, since the linear polarization of sky light is always perpendicular to the direction of the sun. This ability is very common among the insects, including bees, which use this information to orient their communicative dances.[33]:102â103 Polarization sensitivity has also been observed in species of octopus, squid, cuttlefish, and mantis shrimp.[33]:111â112 In the latter case, one species measures all six orthogonal components of polarization, and is believed to have optimal polarization vision.[34] The rapidly changing, vividly colored skin patterns of cuttlefish, used for communication, also incorporate polarization patterns, and mantis shrimp are known to have polarization selective reflective tissue. Sky polarization was thought to be perceived by pigeons, which was assumed to be one of their aids in homing, but research indicates this is a popular myth.[35]

The naked human eye is weakly sensitive to polarization, without the need for intervening filters. Polarized light creates a very faint pattern near the center of the visual field, called Haidinger's brush. This pattern is very difficult to see, but with practice one can learn to detect polarized light with the naked eye.[33]:118

Angular momentum using circular polarization
It is well known that electromagnetic radiation carries a certain linear momentum in the direction of propagation. In addition, however, light carries a certain angular momentum if it is circularly polarized (or partially so). In comparison with lower frequencies such as microwaves, the amount of angular momentum in light, even of pure circular polarization, compared to the same wave's linear momentum (or radiation pressure) is very small and difficult to even measure. However it was utilized in an experiment to achieve speeds of up to 600 million revolutions per minute.[36][37]

See also
Depolarizer (optics)
Fluorescence anisotropy
GlanâTaylor prism
Kerr effect
Nicol prism
Pockels effect
Polarization rotator
Polarized light microscopy
Polarizer
Polaroid (polarizer)
Radial polarization
Rayleigh sky model
Waveplate
References
Cited references
 Shipman, James; Wilson, Jerry D.; Higgins, Charles A. (2015). An Introduction to Physical Science, 14th Ed. Cengage Learning. p. 187. ISBN 978-1-305-54467-3.
 Muncaster, Roger (1993). A-level Physics. Nelson Thornes. pp. 465â467. ISBN 0-7487-1584-3.
 Singh, Devraj (2015). Fundamentals of Optics, 2nd Ed. PHI Learning Pvt. Ltd. p. 453. ISBN 978-8120351462.
 Avadhanulu, M. N. (1992). A Textbook of Engineering Physics. S. Chand Publishing. pp. 198â199. ISBN 8121908175.
 Desmarais, Louis (1997). Applied Electro Optics. Pearson Education. pp. 162â163. ISBN 0-13-244182-9.
 Le Tiec, A.; Novak, J. (July 2016). "Theory of Gravitational Waves". An Overview of Gravitational Waves. pp. 1â41. arXiv:1607.04202. doi:10.1142/9789813141766_0001. ISBN 978-981-314-175-9. S2CID 119283594.
 Lipson, Stephen G.; Lipson, Henry; Tannhauser, David Stefan (1995). Optical Physics. Cambridge University Press. pp. 125â127. ISBN 978-0-521-43631-1.
 Waldman, Gary (2002). Introduction to Light: The Physics of Light, Vision, and Color. Courier Corporation. pp. 79â80. ISBN 978-0-486-42118-6.
 Griffiths, David J. (1998). Introduction to Electrodynamics (3rd ed.). Prentice Hall. ISBN 0-13-805326-X.
 Geoffrey New (7 April 2011). Introduction to Nonlinear Optics. Cambridge University Press. ISBN 978-1-139-50076-0.
 Dorn, R.; Quabis, S. & Leuchs, G. (Dec 2003). "Sharper Focus for a Radially Polarized Light Beam". Physical Review Letters. 91 (23): 233901. Bibcode:2003PhRvL..91w3901D. doi:10.1103/PhysRevLett.91.233901. PMID 14683185.
 Chandrasekhar, Subrahmanyan (1960). Radiative Transfer. Dover. p. 27. ISBN 0-486-60590-6. OCLC 924844798.
 Sletten, Mark A.; Mc Laughlin, David J. (2005-04-15). "Radar Polarimetry". In Chang, Kai (ed.). Encyclopedia of RF and Microwave Engineering. John Wiley & Sons, Inc. doi:10.1002/0471654507.eme343. ISBN 978-0-471-65450-6.
 Schrank, Helmut E.; Evans, Gary E.; Davis, Daniel (1990). "6 Reflector Antennas" (PDF). In Skolnik, Merrill Ivan (ed.). Radar Handbook (PDF). McGraw-Hill. pp. 6.30, Fig 6.25. ISBN 978-0-07-057913-2.
 Ishii, T. Koryu, ed. (1995). Handbook of Microwave Technology. Vol 2: Applications. Elsevier. p. 177. ISBN 978-0-08-053410-7.
 Volakis, John (2007). Antenna Engineering Handbook, Fourth Edition. McGraw-Hill. Sec. 26.1. ISBN 9780071475747: Note: in contrast with other authors, this source initially defines ellipticity reciprocally, as the minor-to-major-axis ratio, but then goes on to say that "Although [it] is less than unity, when expressing ellipticity in decibels, the minus sign is frequently omitted for convenience", which essentially reverts back to the definition adopted by other authors.
 Prakash, Hari; Chandra, Naresh (1971). "Density Operator of Unpolarized Radiation". Physical Review A. 4 (2): 796â799. Bibcode:1971PhRvA...4..796P. doi:10.1103/PhysRevA.4.796.
 Chandrasekhar, Subrahmanyan (2013). Radiative transfer. Courier. p. 30.
 Hecht, Eugene (2002). Optics (4th ed.). United States of America: Addison Wesley. ISBN 0-8053-8566-5.
 Edward L. O'Neill (January 2004). Introduction to Statistical Optics. Courier Dover Publications. ISBN 978-0-486-43578-7.
 Dennis Goldstein; Dennis H. Goldstein (3 January 2011). Polarized Light, Revised and Expanded. CRC Press. ISBN 978-0-203-91158-7.
 Masud Mansuripur (2009). Classical Optics and Its Applications. Cambridge University Press. ISBN 978-0-521-88169-2.
 Randy O. Wayne (16 December 2013). Light and Video Microscopy. Academic Press. ISBN 978-0-12-411536-1.
 Peter M. Shearer (2009). Introduction to Seismology. Cambridge University Press. ISBN 978-0-521-88210-1.
 Seth Stein; Michael Wysession (1 April 2009). An Introduction to Seismology, Earthquakes, and Earth Structure. John Wiley & Sons. ISBN 978-1-4443-1131-0.
 Vollhardt, K. Peter C.; Schore, Neil E. (2003). Organic Chemistry: Structure and Function (4th ed.). W. H. Freeman. pp. 169â172. ISBN 978-0-7167-4374-3.
 Vlemmings, W. H. T. (Mar 2007). "A review of maser polarization and magnetic fields". Proceedings of the International Astronomical Union. 3 (S242): 37â46. arXiv:0705.0885. Bibcode:2007IAUS..242...37V. doi:10.1017/s1743921307012549.
 Hannu Karttunen; Pekka KrÃ¶ger; Heikki Oja (27 June 2007). Fundamental Astronomy. Springer. ISBN 978-3-540-34143-7.
 Boyle, Latham A.; Steinhardt, PJ; Turok, N (2006). "Inflationary predictions for scalar and tensor fluctuations reconsidered". Physical Review Letters. 96 (11): 111301. arXiv:astro-ph/0507455. Bibcode:2006PhRvL..96k1301B. doi:10.1103/PhysRevLett.96.111301. PMID 16605810. S2CID 10424288.
 Tegmark, Max (2005). "What does inflation really predict?". Journal of Cosmology and Astroparticle Physics. 0504 (4): 001. arXiv:astro-ph/0410281. Bibcode:2005JCAP...04..001T. doi:10.1088/1475-7516/2005/04/001. S2CID 17250080.
 Clark, S. (1999). "Polarised starlight and the handedness of Life". American Scientist. 97 (4): 336â43. Bibcode:1999AmSci..87..336C. doi:10.1511/1999.4.336.
 Bekefi, George; Barrett, Alan (1977). Electromagnetic Vibrations, Waves, and Radiation. USA: MIT Press. ISBN 0-262-52047-8.
 J. David Pye (13 February 2001). Polarised Light in Science and Nature. CRC Press. ISBN 978-0-7503-0673-7.
 Sonja Kleinlogel; Andrew White (2008). "The secret world of shrimps: polarisation vision at its best". PLOS ONE. 3 (5): e2190. arXiv:0804.2162. Bibcode:2008PLoSO...3.2190K. doi:10.1371/journal.pone.0002190. PMC 2377063. PMID 18478095.
 Nuboer, J. F. W.; Coemans, M. a. J. M.; Vos Hzn, J. J. (1995-02-01). "No evidence for polarization sensitivity in the pigeon electroretinogram". Journal of Experimental Biology. 198 (2): 325â335. ISSN 0022-0949. PMID 9317897.
 "'Fastest spinning object' created". BBC News. 2013-08-28. Retrieved 2019-08-27.
 Dholakia, Kishan; Mazilu, Michael; Arita, Yoshihiko (August 28, 2013). "Laser-induced rotation and cooling of a trapped microgyroscope in vacuum". Nature Communications. 4: 2374. Bibcode:2013NatCo...4.2374A. doi:10.1038/ncomms3374. hdl:10023/4019. PMC 3763500. PMID 23982323.
General references
Principles of Optics, 7th edition, M. Born & E. Wolf, Cambridge University, 1999, ISBN 0-521-64222-1.
Fundamentals of polarized light: a statistical optics approach, C. Brosseau, Wiley, 1998, ISBN 0-471-14302-2.
Polarized Light, second edition, Dennis Goldstein, Marcel Dekker, 2003, ISBN 0-8247-4053-X
Field Guide to Polarization, Edward Collett, SPIE Field Guides vol. FG05, SPIE, 2005, ISBN 0-8194-5868-6.
Polarization Optics in Telecommunications, Jay N. Damask, Springer 2004, ISBN 0-387-22493-9.
Polarized Light in Nature, G. P. KÃ¶nnen, Translated by G. A. Beerling, Cambridge University, 1985, ISBN 0-521-25862-6.
Polarised Light in Science and Nature, D. Pye, Institute of Physics, 2001, ISBN 0-7503-0673-4.
Polarized Light, Production and Use, William A. Shurcliff, Harvard University, 1962.
Ellipsometry and Polarized Light, R. M. A. Azzam and N. M. Bashara, North-Holland, 1977, ISBN 0-444-87016-4
Secrets of the Viking NavigatorsâHow the Vikings used their amazing sunstones and other techniques to cross the open oceans, Leif Karlsen, One Earth Press, 2003.
External links
Feynman's lecture on polarization
Polarized Light in Nature and Technology
Polarized Light Digital Image Gallery: Microscopic images made using polarization effects
Polarization by the University of Colorado Physics 2000: Animated explanation of polarization
MathPages: The relationship between photon spin and polarization
A virtual polarization microscope
Polarization angle in satellite dishes.
Using polarizers in photography
Molecular Expressions: Science, Optics and You â Polarization of Light: Interactive Java tutorial
SPIE technical group on polarization
Antenna Polarization
Animations of Linear, Circular and Elliptical Polarizations on YouTube
Authority control Edit this at Wikidata
GND: 4380737-9NDL: 00563102
Categories: Polarization (waves)Electromagnetic radiationAntennas (radio)Broadcast engineering
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Polski
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
41 more
Edit links
This page was last edited on 19 October 2020, at 23:17 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

August Herman Pfund
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
August Herman Pfund
Born	December 28, 1879
Madison, Wisconsin
Died	January 4, 1949 (aged 69)
Nationality	American
Alma mater
Johns Hopkins UniversityUniversity of WisconsinâMadison
Known for
Pfund linePfund telescopePfund sky compass
Awards
Edward Longstreth Medal (1922)
Frederic Ives Medal (1939)
Scientific career
Fields
PhysicsSpectroscopyCalorimetry
Institutions	Johns Hopkins University
Doctoral advisor	Robert W. Wood
August Herman Pfund (December 28, 1879 â January 4, 1949) was an American physicist, spectroscopist, and inventor.


Contents
1	Early life
2	Career
3	See also
4	References
5	Further reading
6	External links
Early life
Pfund was born in Madison, Wisconsin and attended Wisconsin public schools until his entry into the University of WisconsinâMadison, where he earned a B.S. degree in physics and studied under Robert W. Wood.

Career
Both Wood and Pfund left Wisconsin for Johns Hopkins University in 1903. From 1903 to 1905 Pfund was a Carnegie research assistant and continued to work under Wood. In 1906 Pfund earned his Ph.D. in physics and was a Johnston scholar from 1907 to 1909. He remained at Hopkins for the remainder of his career, eventually becoming a full professor and later chair of the physics department. From 1943 to 1944 Pfund served as the president of the Optical Society of America.

Within the hydrogen spectral series Pfund discovered the fifth series, where an electron jumps up from or drops down to the fifth fundamental level. This Series is known as the "Pfund series". He also invented the Pfund telescope, which is a method for achieving a fixed telescope focal point regardless of where the telescope line of sight is positioned, and the Pfund sky compass,[1] which arose from Pfund's studies of the polarization of scattered light from the sky in 1944, and which greatly helped transpolar flights by allowing the determination of the Sun's direction in twilight.[2] Pfund is also noted for his work into the area of infrared gas analysis.

See also
Current and past presidents of the Optical Society of America
Medieval sunstone
References
 Moody, Lieutenant Commander Alton B., "The Pfund Sky Compass", Navigation, Journal of The Institute of Navigation, Vol. 2, No. 7, 1950.
 John Howard, "Presidents of the Late 1940s", Optics and Photonics News, June 2010.
Further reading
Dr. John Andraos, Named Concepts in Chemistry (L-Z), York University, 2001
Pelletier, Paul A. (ed.), Prominent Scientists: An index to collective biographies, 2nd ed., Neal-Schuman Publishing, Inc.: New York, 1985 (citations only)
Cattell, J.M.; Cattell, J. American Men of Science, 6th ed., The Science Press: New York, 1938
Journal of the Optical Society of America, 39:4 (April, 1949) 325. Obit.
External links
Articles Published by early OSA Presidents Journal of the Optical Society of America
Authority control Edit this at Wikidata
ISNI: 0000 0000 2814 5472LCCN: n89103800SNAC: w62f7pv2VIAF: 11433043WorldCat Identities: lccn-n89103800
Categories: 1879 births1949 deathsAmerican physicistsUniversity of WisconsinâMadison alumniSpectroscopists
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
ØªÛØ±Ú©Ø¬Ù
à¦¬à¦¾à¦à¦²à¦¾
ÄeÅ¡tina
Deutsch
ÙØ§Ø±Ø³Û
FranÃ§ais
PortuguÃªs
SlovenÅ¡Äina
ä¸­æ
Edit links
This page was last edited on 8 February 2020, at 22:21 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Polar route
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Polar route" â news Â· newspapers Â· books Â· scholar Â· JSTOR (September 2014) (Learn how and when to remove this template message)
A polar route is an aircraft route across the uninhabited polar ice cap regions. The term "polar route" was originally applied to great circle navigation routes between Europe and the west coast of North America in the 1950s.[1]


Contents
1	Maps
2	The Arctic
2.1	Early years
2.2	The Cold War
2.3	After the Cold War
2.4	Today
3	Antarctica
4	Operational considerations
5	See also
6	References
7	External links
Maps
Maps showing arctic routes under and after the cold war together with antarctic routes.

General arrangement of polar routes in the late 20th century (left) and in the 2000s (center) and in the Southern Hemisphere (right).
The Arctic
Early years
The Soviet pilot Valery Chkalov was the first to fly non-stop from Europe to the American Pacific Coast. His flight from Moscow, Russian SFSR, Soviet Union to Vancouver, Washington, United States, via the North Pole on a Tupolev ANT-25 single-engine plane (June 18â20, 1937) took 63 hours to complete. The distance covered was 8,811 kilometres (5,475 mi).[2]

In October 1946, a modified B-29 flew 15,163 kilometres (9,422 mi) nonstop from Oahu, Hawaii, to Cairo, Egypt, in less than 40 hours, further proving the capability of routing airlines over the polar icecap.[3]

The Cold War
Of the commercial airlines, SAS was first: their Douglas DC-6B flights between Los Angeles and Copenhagen, via Sondre Stromfjord and Winnipeg, started on November 15, 1954.[4] Canadian Pacific DC-6Bs started VancouverâAmsterdam in 1955, then Pan Am and TWA started West Coast to Paris/London in 1957. SAS was first again, flying Europe to Tokyo via Anchorage with Douglas DC-7Cs in February 1957; Air France Lockheed L-1649 Starliner (which was the final version of the Lockheed Constellation) and KLM DC-7C aircraft followed in 1958.[5]

During the Cold War, the Arctic region was a buffer zone between the Soviet Union and North America. Civilian flights from Europe to the Asian Far East were prohibited from crossing the Eastern Bloc countries, Soviet Union or China, and either had to fly via the Middle East or fly across Arctic North America and Greenland with a refueling stop in Anchorage. These Cold War tracks extended from the northern Alaskan coast across Greenland to Europe. In 1978, Korean Air Lines Flight 902 operated with a Boeing 707 was shot down over the USSR by a Soviet Air Force fighter aircraft after the flight crew made gross navigational errors attempting to fly the assigned polar route.

In April 1967 Japan Air Lines (JAL) began an experimental service between Tokyo and Europe via Moscow across Siberia. This service used an Aeroflot Tupolev Tu-114, with one JAL flight crew and mixed JAL and Aeroflot cabin crew. However, Japan Air Lines dropped the service in 1969.

During the Cold War, Anchorage International Airport (ANC) in Alaska was a technical stop for a number of airlines flying the polar route between western Europe and Tokyo. According to the July 1, 1983 edition of the Worldwide Official Airline Guide (OAG), Air France, British Airways, Japan Air Lines (JAL), KLM Royal Dutch Airlines, Lufthansa, Sabena and Scandinavian Airlines (SAS) were all operating flights between Japan and western Europe which included a stop in Anchorage.[6] Most of these international airlines were operating earlier model Boeing 747 aircraft on the route at this time, although Sabena and SAS were instead operating McDonnell Douglas DC-10-30 aircraft on their respective polar route services via Anchorage. U.S. based air carrier Western Airlines also flew a polar route during the early 1980s between London Gatwick Airport and Honolulu using DC-10-30 aircraft, with these flights also making a stop in Anchorage.[7]

The only airline that still flies this type of route between Europe and Anchorage is Condor Airlines, seasonal service between Frankfurt (EDDF) and Anchorage.[8] This will soon be joined by a flight by Eurowings, also from Frankfurt, using an Airbus A330.[needs update]

Finnair was the first airline to fly non-stop via the Arctic Ocean - Alaska polar route without a technical stop. This service began in 1983 and was flown with a McDonnell Douglas DC-10-30 wide body jetliner between Tokyo and Helsinki.[9][10]

United States Boeing B-52 aircraft operated in the Arctic Ocean region almost continuously in the 1960s as part of Operation Chrome Dome and in later decades as part of readiness exercises. A number of Western reconnaissance aircraft also conducted missions regularly along the Soviet Union's northern coast. Russian Long-Range Aviation now perform some of the same types of training flights, testing the readiness of Alaskan Command and Royal Canadian Air Force interceptors.[11]

After the Cold War
Immediately after the Cold War, a number of direct southern routes had opened up between Europe and Asia over the Black Sea and southern former Soviet republics across Afghanistan, and by the mid-1990s, over China. In Russia's eastern and Arctic regions there were significant problems with lack of English-speaking controllers, lack of radio facilities, poor radar coverage, poor ATC capacity, and a lack of funds. To solve these issues, RACGAT (Russian-American Coordinating Group for Air Traffic) was formed in 1993. By summer 1998, the Russian government worked through these problems and gave permission to open four cross-polar routes, named Polar 1, 2, 3 and 4.[12] Additional routes were opened in subsequent years.

Cathay Pacific Flight 889 from New York John F. Kennedy International Airport, piloted by Captain Paul Horsting on 7 July 1998âthe first arrival to the new Hong Kong International Airport at Chek Lap Kok west of Hong Kongâappears to be the first non-stop flight over the Arctic polar region and over Russian airspace. It was the world's first nonstop transpolar flight from New York to Hong Kong, dubbed Polar One. It took 16 hours to complete, and it was and still is one of the longest flights that Cathay Pacific operates.[13]

Today
The American Federal Aviation Administration now defines the North Polar area of operations as the area north of 78Â° north latitude,[14] which is north of Alaska and most of Siberia.

Aircraft like the Boeing 747-400, 747-8, 777-200ER, 777-200LR, 777-300ER and Boeing 787-8, 787-9, and 787-10, as well as the Airbus A340, A350 and A380, with ranges of around 7,000 nautical miles (8,100 mi; 13,000 km) or more, are required in order to travel the long distances nonstop between suitable airports.[15]

Arctic polar routes are now common on airlines connecting Asian cities to North American cities. Emirates flies nonstop from Dubai to the US West Coast (San Francisco, Seattle and Los Angeles), coming within a few degrees of latitude of the North Pole.[16][17] Air India started operating its non-stop flight: AI-173, between New Delhi (DEL) and San Francisco (SFO), on August 15, 2019, that flies over the North Pole.[18]

Antarctica
Few airlines fly between cities having a great circle route over Antarctica. Hypothetically, flights between South Africa and New Zealand, or between Perth, Australia and certain destinations in South America (including Buenos Aires and SÃ£o Paulo), would overfly Antarctica, but no airline has scheduled such flights.

Flights between Australia and South America and between Australia and South Africa pass near the Antarctic coastline. Depending on the winds, the Qantas flight QFA63 from Sydney to Johannesburg-O. R. Tambo, or the return flight QFA64, sometimes flies over the Antarctic Circle to 71Â° latitude as well and allowing views of the icecap.[19][better source needed] Qantas QFA27 and QFA28 fly nonstop between Sydney and Santiago de Chile, the most southerly polar route. Depending on winds, this flight may reach 55Â° south latitude. Until 2014, AerolÃ­neas Argentinas flew nonstop between Sydney and Buenos Aires[20][better source needed] Previously, QANTAS also operated QFA17 and QFA18 between Sydney, Australia and Buenos Aires, Argentina. Nowadays, LATAM operates LAN804 and LAN805 between Melbourne, Australia, and Santiago, Chile, and Air New Zealand operates ANZ30 and ANZ31 between Auckland, New Zealand and Buenos Aires, Argentina all with similarly south-running routes.

The polar route across the remote southern Pacific Ocean between South America and Oceania was pioneered by Aerolineas Argentinas, which began service between Buenos Aires via Rio Gallegos to Auckland in the 1980s with a Boeing 747-200 aircraft. Aerolineas Argentinas later operated to Sydney, but ended its flights to New Zealand and Australia in 2014.[21]

Because of ETOPS limitations on twin-engined aircraftâthe maximum distance the aircraft can operate from an airport for emergency landingsâonly four-engined aircraft such as the Boeing 747, Airbus A340, and Airbus A380 can or could operate routes near Antarctica. Twin-engined aircraft must fly further north, closer to potential diversion airports; for example, when Virgin Australia operated their VA 15 and VA 16 flights between Melbourne and Johannesburg on twin-engined Boeing 777 aircraft with a 180-minute ETOPS rating, the flight was two hours longer than a Qantas flight from Sydney to Johannesburg.[22] Air New Zealand flight ANZ30 and ANZ31 flies nonstop between Auckland and Buenos Aires-Ezeiza; in 2015, government regulators approved its twin-engined Boeing 777-200ER aircraft that operate the route for a 330-minute ETOPS rating (i.e. its 777 aircraft can fly a maximum 330 minutes away from the nearest diversion airport), an increase from its previous 240-minute ETOPS rating.[23] LATAM Airlines began their LAN800 and LAN801 nonstop flights between Santiago de Chile and Sydney via Auckland in April 2015 with twin-engined Boeing 787 aircraft with a 330-minute ETOPS rating.[24][25] LATAM has announced a nonstop flight between Santiago de Chile and Melbourne (LAN804/LAN805) to begin in October 2017.[26][27] In late 2019, LATAM began direct flights between Santiago and Sydney (LA802/LAN803) competing with the existing QANTAS (QFA27/QFA28) flights on the same route.

The southernmost flight route with plausible airports would be between Buenos Aires and Perth.[28] With a 175Â° (S) heading, the route's great circle exceeds 85Â° S and would be within 500 kilometres (310 mi) of the South Pole. Currently, no commercial airliners operates this 6,800-nautical-mile (7,800 mi; 12,600 km) route. However, in February, 2018, it was stated that Norwegian Air Argentina is considering this "less than 15 hours" trans-polar flight between South America and Asia, with a stop-over in Perth enroute Singapore.[29] They will not fly over the South Pole, but around Antarctica taking advantage of the strong winds which circle that continent in an easterly direction. Hence, the "westbound" flight from Buenos Aires would actually travel south-east south of Cape Town, over the southern Indian Ocean and on to Perth, while the true "eastbound" flight would also head south-east south of Tasmania and New Zealand, over the South Pacific and on to South America. If this route becomes operational, a Buenos AiresâSingapore return flight would possibly be the fastest circumnavigation available with commercial airliners, although PerthâBuenos Aires return would be faster but without passing the Equator.

Operational considerations
The FAA's policy letter Guidance for Polar Operations (March 5, 2001) outlines a number of special requirements for polar flight, which includes two cold-weather suits, special communication capability, designation of Arctic diversion airports and firm recovery plans for stranded passengers, and fuel freeze strategy and monitoring requirements.[14]

Jet fuel freeze temperatures range between â40 and â50 Â°C (â40 and â58 Â°F). These temperatures are frequently encountered at cruise altitude throughout the world with no effect since the fuel retains heat from lower elevations, but the intense cold and extended duration of polar flights may cause fuel temperature to approach its freezing point. Jet A grade with a maximum freeze point of â40 Â°C (â40 Â°F) is used in the U.S., while Jet A1 grade with a maximum freeze point of â47 Â°C (â53 Â°F) is used elsewhere.[30] Modern long-distance airliners are equipped to alert flight crew when fuel temperatures reach 3 Â°C (5.4 Â°F) above these levels. The crew must then change altitude, though in some cases due to the low stratosphere over polar regions and its inversion properties the air may actually be somewhat warmer at higher altitudes.[14]

The alerts are typically set at 3 Â°C (5.4 Â°F) above the specified maximum freeze point. This provides a 3 Â°C (5.4 Â°F) safety margin from the solidification temperature. However, fuels produced at the refineries are often better than the spec values; for example, it is not uncommon to find Jet A fuels that have measured freeze point better (colder) than the specified maximum of â40 Â°C (â40 Â°F). In that way, the safety margin is even larger than 3 Â°C (5.4 Â°F). On the other hand, the temperature probe that delivers fuel temperature information to the flight deck is not located in the coldest part of the fuel tanks. The difference between the recorded and the coldest fuel temperature varies depending on a variety of factors, especially the circulation of fuel in the tanks and duration of cold soak. It is, therefore, prudent to have a safety margin.

For polar flights, FAA allows,[31] under certain conditions, the measured freeze point be used instead of assuming the spec value in ASTM D1655. This gives the airlines more flexibility in flight planning.

See also
icon	Geography portal
Longest flights
Great circle
Antarctica
References
 For instance, Aviation Week 22 July 1957 p47 reports on "polar routes" from California to Europe granted to Pan Am and TWA.
 McCannon, John (1998). Red Arctic : Polar Exploration and the Myth of the North in the Soviet Union, 1932â1939. New York: Oxford University Press. p. 71. ISBN 978-0-19-535420-1.
 "Inside The Dreamboat." Popular Science, December 1946 interview with crew about planning for flight.
 [1]
 "To Tokyo with a DC-7 over the North Pole". KLM Blog. 23 November 2014.
 http://www.departedflights.com, July 1, 1983 Worldwide Official Airline Guide (OAG), Tokyo-Anchorage flight schedules
 http://www.departedflights.com, March 1, 1981 Western Airlines system timetable
 "StackPath (12 August 2020)". www.aviationpros.com (Press release). Retrieved 21 August 2020.
 http://www.departedflights.com, July 1, 1983 Worldwide Official Airline Guide (OAG), Tokyo-Helsinki flight schedules
 Huhtanen, Ann-Mari (7 September 2014). "Perhana, se tulee suoraan kohti. Jouluna 1987 Finnairin lento AY 915 oli matkalla Tokiosta Helsinkiin, kun Huippuvuorten kohdalla konetta lÃ¤hestyi ohjus" [âDamn it, itâs coming straight at us. At Christmas, 1987, Finnair flight AY 915 was en route from Tokyo to Helsinki, when a missile approached it over Svalbardâ]. Helsingin Sanomat (in Finnish). Sanoma: C 6â8. Retrieved 2014-09-21.
 Russian military planes approach Alaska for 4th straight night, Anchorage Daily News, Published April 21, 2017.
 Over the Top: Flying the Polar Routes. Avionics Magazine, April 1, 2002. Retrieved 3-07-12. [2]
 "Cathay Pacific's non-stop New York flight 'strengthens Hong Kong's hub'" (Press release). Cathay Pacific. 11 June 2004. Archived from the original on 27 September 2011. Retrieved 5 July 2009.
 "Aero 16 - Polar Route Operations". www.boeing.com.
 Study Finds Air Route Over North Pole Feasible for Flights to Asia, Matthew L. Wald, New York Times, 10-22-2000. Article retrieved 03-12-09. [3]
 "Flightaware website".
 "Schedule search". Air India. Air India Ltd. Retrieved 12 April 2015.
 "Air India becomes first Indian airline to fly over North Pole - Times of India". The Times of India. Retrieved 2019-08-16.
 "gotravelyourway".
 "Flightaware website".
 Chui, Sam. "Aerolineas Argentinas Transpolar Flight "Vuelo Transpolar" Sydney â Buenos Aires". Sam Chui Aviation & Travel. Retrieved 21 March 2017.
 Freed, Jamie (1 October 2014). "Virgin Australia brings back direct flights to Johannesburg through South African Airways codeshare". The Border Mail. Retrieved 21 March 2017.
 Carey, Bill (2 December 2015). "Air New Zealand 777 Makes First 330-Minute ETOPS Flight". AINonline. The Convention News Company, Inc. Retrieved 21 March 2017.
 Leaman, Aaron (31 July 2016). "Flight test: Auckland to Santiago on LATAM's Boeing 787-9". Stuff. Retrieved 21 March 2017.
 Clark, Peter (22 April 2015). "PICTURE: LAN avails of 787 ETOPS certification". Flight Global. Retrieved 21 March 2017.
 "LATAM to serve Melbourne-Santiago nonstop from October 2017". Australian Aviation. 5 December 2016. Archived from the original on 21 March 2017. Retrieved 21 March 2017.
 Flynn, David (5 December 2016). "LATAM to fly Melbourne-Santiago from October 2017". Australian Business Traveller. Retrieved 21 March 2017.
 "Great Circle Mapper: EZE-PER". Great Circle Mapper.
 "World-first South American flight to boost WA tourism". The West Australian. 25 February 2018.
 [ASTM specification D1655]
https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/73587
19: https://www.lavoz.com.ar/ciudadanos/aerolineas-argentinas-unira-buenos-aires-sydney-sin-escalas

External links
RACGAT website â archived in 2003
Flightradar24 blog page â Why you flew over Greenland
vte
Arctic topics
Categories: Air traffic controlAir navigationPolar regions of the EarthAirline routes
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
íêµ­ì´
Bahasa Indonesia
PortuguÃªs
ä¸­æ
3 more
Edit links
This page was last edited on 17 September 2020, at 20:41 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Great circle
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
This article is about the mathematical notion. For its applications in geodesy, see great-circle distance. For other uses, see The Great Circle.

This article includes a list of references, related reading or external links, but its sources remain unclear because it lacks inline citations. Please help to improve this article by introducing more precise citations. (March 2018) (Learn how and when to remove this template message)

A great circle divides the sphere in two equal hemispheres
A great circle, also known as an orthodrome, of a sphere is the intersection of the sphere and a plane that passes through the center point of the sphere. A great circle is the largest circle that can be drawn on any given sphere. Any diameter of any great circle coincides with a diameter of the sphere, and therefore all great circles have the same center and circumference as each other. This special case of a circle of a sphere is in opposition to a small circle, that is, the intersection of the sphere and a plane that does not pass through the center. Every circle in Euclidean 3-space is a great circle of exactly one sphere.

For most pairs of distinct points on the surface of a sphere, there is a unique great circle through the two points. The exception is a pair of antipodal points, for which there are infinitely many great circles. The minor arc of a great circle between two points is the shortest surface-path between them. In this sense, the minor arc is analogous to âstraight linesâ in Euclidean geometry. The length of the minor arc of a great circle is taken as the distance between two points on a surface of a sphere in Riemannian geometry where such great circles are called Riemannian circles. These great circles are the geodesics of the sphere.

The disk bounded by a great circle is called a great disk: it is the intersection of a ball and a plane passing through its center. In higher dimensions, the great circles on the n-sphere are the intersection of the n-sphere with 2-planes that pass through the origin in the Euclidean space Rn + 1.


Contents
1	Derivation of shortest paths
2	Applications
3	See also
4	External links
Derivation of shortest paths
See also: Great-circle distance
To prove that the minor arc of a great circle is the shortest path connecting two points on the surface of a sphere, one can apply calculus of variations to it.

Consider the class of all regular paths from a point {\displaystyle p}p to another point {\displaystyle q}q. Introduce spherical coordinates so that {\displaystyle p}p coincides with the north pole. Any curve on the sphere that does not intersect either pole, except possibly at the endpoints, can be parametrized by

{\displaystyle \theta =\theta (t),\quad \phi =\phi (t),\quad a\leq t\leq b}\theta =\theta (t),\quad \phi =\phi (t),\quad a\leq t\leq b
provided we allow {\displaystyle \phi }\phi  to take on arbitrary real values. The infinitesimal arc length in these coordinates is

{\displaystyle ds=r{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}\,dt.}ds=r{\sqrt  {\theta '^{2}+\phi '^{{2}}\sin ^{{2}}\theta }}\,dt.
So the length of a curve {\displaystyle \gamma }\gamma  from {\displaystyle p}p to {\displaystyle q}q is a functional of the curve given by

{\displaystyle S[\gamma ]=r\int _{a}^{b}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}\,dt.}S[\gamma ]=r\int _{a}^{b}{\sqrt  {\theta '^{2}+\phi '^{{2}}\sin ^{{2}}\theta }}\,dt.
According to the EulerâLagrange equation, {\displaystyle S[\gamma ]}{\displaystyle S[\gamma ]} is minimized if and only if

{\displaystyle {\frac {\sin ^{2}\theta \phi '}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}=C}{\displaystyle {\frac {\sin ^{2}\theta \phi '}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}=C},
where {\displaystyle C}C is a {\displaystyle t}t-independent constant, and

{\displaystyle {\frac {\sin \theta \cos \theta \phi '^{2}}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}={\frac {d}{dt}}{\frac {\theta '}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}.}{\displaystyle {\frac {\sin \theta \cos \theta \phi '^{2}}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}={\frac {d}{dt}}{\frac {\theta '}{\sqrt {\theta '^{2}+\phi '^{2}\sin ^{2}\theta }}}.}
From the first equation of these two, it can be obtained that

{\displaystyle \phi '={\frac {C\theta '}{\sin \theta {\sqrt {\sin ^{2}\theta -C^{2}}}}}}{\displaystyle \phi '={\frac {C\theta '}{\sin \theta {\sqrt {\sin ^{2}\theta -C^{2}}}}}}.
Integrating both sides and considering the boundary condition, the real solution of {\displaystyle C}C is zero. Thus, {\displaystyle \phi '=0}{\displaystyle \phi '=0} and {\displaystyle \theta }\theta  can be any value between 0 and {\displaystyle \theta _{0}}\theta _{0}, indicating that the curve must lie on a meridian of the sphere. In Cartesian coordinates, this is

{\displaystyle x\sin \phi _{0}-y\cos \phi _{0}=0}x\sin \phi _{0}-y\cos \phi _{0}=0
which is a plane through the origin, i.e., the center of the sphere.

Applications
Some examples of great circles on the celestial sphere include the celestial horizon, the celestial equator, and the ecliptic. Great circles are also used as rather accurate approximations of geodesics on the Earth's surface for air or sea navigation (although it is not a perfect sphere), as well as on spheroidal celestial bodies.

The equator of the idealized earth is a great circle and any meridian and its opposite meridian form a great circle. Another great circle is the one that divides the land and water hemispheres. A great circle divides the earth into two hemispheres and if a great circle passes through a point it must pass through its antipodal point.

The Funk transform integrates a function along all great circles of the sphere.

See also
Great-circle distance
Great-circle navigation
Rhumb line
External links
Great Circle â from MathWorld Great Circle description, figures, and equations. Mathworld, Wolfram Research, Inc. c1999
Great Circles on Mercator's Chart by John Snyder with additional contributions by Jeff Bryant, Pratik Desai, and Carl Woll, Wolfram Demonstrations Project.
Navigational Algorithms Paper: The Sailings.
Chart Work - Navigational Algorithms Chart Work free software: Rhumb line, Great Circle, Composite sailing, Meridional parts. Lines of position Piloting - currents and coastal fix.
Categories: Elementary geometrySpherical trigonometryRiemannian geometryCircles
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
53 more
Edit links
This page was last edited on 7 May 2020, at 11:27 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

This is a good article. Click here for more information.
Ecliptic
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

As seen from the orbiting Earth, the Sun appears to move with respect to the fixed stars, and the ecliptic is the yearly path the Sun follows on the celestial sphere. This process repeats itself in a cycle lasting a little over 365 days.
The ecliptic is the plane of Earth's orbit around the Sun.[1][2][a] From the perspective of an observer on Earth, the Sun's movement around the celestial sphere over the course of a year traces out a path along the ecliptic against the background of stars.[3] The ecliptic is an important reference plane and is the basis of the ecliptic coordinate system.


Contents
1	Sun's apparent motion
2	Relationship to the celestial equator
3	Obliquity of the ecliptic
4	Plane of the Solar System
5	Celestial reference plane
6	Eclipses
7	Equinoxes and solstices
8	In the constellations
9	Astrology
10	See also
11	Notes and references
12	External links
Sun's apparent motion
Because of the movement of Earth around the EarthâMoon center of mass, the apparent path of the Sun wobbles slightly, with a period of about one month. Because of further perturbations by the other planets of the Solar System, the EarthâMoon barycenter wobbles slightly around a mean position in a complex fashion. The ecliptic is actually the apparent path of the Sun throughout the course of a year.[4]

Because Earth takes one year to orbit the Sun, the apparent position of the Sun takes one year to make a complete circuit of the ecliptic. With slightly more than 365 days in one year, the Sun moves a little less than 1Â° eastward[5] every day. This small difference in the Sun's position against the stars causes any particular spot on Earth's surface to catch up with (and stand directly north or south of) the Sun about four minutes later each day than it would if Earth did not orbit; a day on Earth is therefore 24 hours long rather than the approximately 23-hour 56-minute sidereal day. Again, this is a simplification, based on a hypothetical Earth that orbits at uniform speed around the Sun. The actual speed with which Earth orbits the Sun varies slightly during the year, so the speed with which the Sun seems to move along the ecliptic also varies. For example, the Sun is north of the celestial equator for about 185 days of each year, and south of it for about 180 days.[6] The variation of orbital speed accounts for part of the equation of time.[7]

Relationship to the celestial equator
Main article: Axial tilt

The plane of Earth's orbit projected in all directions forms the reference plane known as the ecliptic. Here, it is shown projected outward (gray) to the celestial sphere, along with Earth's equator and polar axis (green). The plane of the ecliptic intersects the celestial sphere along a great circle (black), the same circle on which the Sun seems to move as Earth orbits it. The intersections of the ecliptic and the equator on the celestial sphere are the vernal and autumnal equinoxes (red), where the Sun seems to cross the celestial equator.
Because Earth's rotational axis is not perpendicular to its orbital plane, Earth's equatorial plane is not coplanar with the ecliptic plane, but is inclined to it by an angle of about 23.4Â°, which is known as the obliquity of the ecliptic.[8] If the equator is projected outward to the celestial sphere, forming the celestial equator, it crosses the ecliptic at two points known as the equinoxes. The Sun, in its apparent motion along the ecliptic, crosses the celestial equator at these points, one from south to north, the other from north to south.[5] The crossing from south to north is known as the vernal equinox, also known as the first point of Aries and the ascending node of the ecliptic on the celestial equator.[9] The crossing from north to south is the autumnal equinox or descending node.

Main article: Axial precession (astronomy)
The orientation of Earth's axis and equator are not fixed in space, but rotate about the poles of the ecliptic with a period of about 26,000 years, a process known as lunisolar precession, as it is due mostly to the gravitational effect of the Moon and Sun on Earth's equatorial bulge. Likewise, the ecliptic itself is not fixed. The gravitational perturbations of the other bodies of the Solar System cause a much smaller motion of the plane of Earth's orbit, and hence of the ecliptic, known as planetary precession. The combined action of these two motions is called general precession, and changes the position of the equinoxes by about 50 arc seconds (about 0.014Â°) per year.[10]

Main article: Astronomical nutation
Once again, this is a simplification. Periodic motions of the Moon and apparent periodic motions of the Sun (actually of Earth in its orbit) cause short-term small-amplitude periodic oscillations of Earth's axis, and hence the celestial equator, known as nutation.[11] This adds a periodic component to the position of the equinoxes; the positions of the celestial equator and (vernal) equinox with fully updated precession and nutation are called the true equator and equinox; the positions without nutation are the mean equator and equinox.[12]

Obliquity of the ecliptic
Main article: Obliquity of the ecliptic
Obliquity of the ecliptic is the term used by astronomers for the inclination of Earth's equator with respect to the ecliptic, or of Earth's rotation axis to a perpendicular to the ecliptic. It is about 23.4Â° and is currently decreasing 0.013 degrees (47 arcseconds) per hundred years because of planetary perturbations.[13]

The angular value of the obliquity is found by observation of the motions of Earth and other planets over many years. Astronomers produce new fundamental ephemerides as the accuracy of observation improves and as the understanding of the dynamics increases, and from these ephemerides various astronomical values, including the obliquity, are derived.


Obliquity of the ecliptic for 20,000 years, from Laskar (1986).[14] Note that the obliquity varies only from 24.2Â° to 22.5Â° during this time. The red point represents the year 2000.
Until 1983 the obliquity for any date was calculated from work of Newcomb, who analyzed positions of the planets until about 1895:

Îµ = 23Â° 27â² 08â³.26 â 46â³.845 T â 0â³.0059 T2 + 0â³.00181 T3

where Îµ is the obliquity and T is tropical centuries from B1900.0 to the date in question.[15]

From 1984, the Jet Propulsion Laboratory's DE series of computer-generated ephemerides took over as the fundamental ephemeris of the Astronomical Almanac. Obliquity based on DE200, which analyzed observations from 1911 to 1979, was calculated:

Îµ = 23Â° 26â² 21â³.45 â 46â³.815 T â 0â³.0006 T2 + 0â³.00181 T3

where hereafter T is Julian centuries from J2000.0.[16]

JPL's fundamental ephemerides have been continually updated. The Astronomical Almanac for 2010 specifies:[17]

Îµ = 23Â° 26â² 21â³.406 â 46â³.836769 T â 0â³.0001831 T2 + 0â³.00200340 T3 â 0â³.576Ã10â6 T4 â 4â³.34Ã10â8 T5

These expressions for the obliquity are intended for high precision over a relatively short time span, perhaps several centuries.[18] J. Laskar computed an expression to order T10 good to 0â³.04/1000 years over 10,000 years.[14]

All of these expressions are for the mean obliquity, that is, without the nutation of the equator included. The true or instantaneous obliquity includes the nutation.[19]

Plane of the Solar System
Main article: Solar System
Ecliptic plane top view.gif	Ecliptic plane side view.gif	FourPlanetSunset hao annotated.JPG
Top and side views of the plane of the ecliptic, showing planets Mercury, Venus, Earth, and Mars. Most of the planets orbit the Sun very nearly in the same plane in which Earth orbits, the ecliptic.	Four planets lined up along the ecliptic in July 2010, illustrating how the planets orbit the Sun in nearly the same plane. Photo taken at sunset, looking west over Surakarta, Java, Indonesia.
Most of the major bodies of the Solar System orbit the Sun in nearly the same plane. This is likely due to the way in which the Solar System formed from a protoplanetary disk. Probably the closest current representation of the disk is known as the invariable plane of the Solar System. Earth's orbit, and hence, the ecliptic, is inclined a little more than 1Â° to the invariable plane, Jupiter's orbit is within a little more than â1â2Â° of it, and the other major planets are all within about 6Â°. Because of this, most Solar System bodies appear very close to the ecliptic in the sky.

The invariable plane is defined by the angular momentum of the entire Solar System, essentially the vector sum of all of the orbital and rotational angular momenta of all the bodies of the system; more than 60% of the total comes from the orbit of Jupiter.[20] That sum requires precise knowledge of every object in the system, making it a somewhat uncertain value. Because of the uncertainty regarding the exact location of the invariable plane, and because the ecliptic is well defined by the apparent motion of the Sun, the ecliptic is used as the reference plane of the Solar System both for precision and convenience. The only drawback of using the ecliptic instead of the invariable plane is that over geologic time scales, it will move against fixed reference points in the sky's distant background.[21][22]

Celestial reference plane
Main articles: Celestial equator and Ecliptic coordinate system

The apparent motion of the Sun along the ecliptic (red) as seen on the inside of the celestial sphere. Ecliptic coordinates appear in (red). The celestial equator (blue) and the equatorial coordinates (blue), being inclined to the ecliptic, appear to wobble as the Sun advances.
The ecliptic forms one of the two fundamental planes used as reference for positions on the celestial sphere, the other being the celestial equator. Perpendicular to the ecliptic are the ecliptic poles, the north ecliptic pole being the pole north of the equator. Of the two fundamental planes, the ecliptic is closer to unmoving against the background stars, its motion due to planetary precession being roughly 1/100 that of the celestial equator.[23]

Spherical coordinates, known as ecliptic longitude and latitude or celestial longitude and latitude, are used to specify positions of bodies on the celestial sphere with respect to the ecliptic. Longitude is measured positively eastward[5] 0Â° to 360Â° along the ecliptic from the vernal equinox, the same direction in which the Sun appears to move. Latitude is measured perpendicular to the ecliptic, to +90Â° northward or â90Â° southward to the poles of the ecliptic, the ecliptic itself being 0Â° latitude. For a complete spherical position, a distance parameter is also necessary. Different distance units are used for different objects. Within the Solar System, astronomical units are used, and for objects near Earth, Earth radii or kilometers are used. A corresponding right-handed rectangular coordinate system is also used occasionally; the x-axis is directed toward the vernal equinox, the y-axis 90Â° to the east, and the z-axis toward the north ecliptic pole; the astronomical unit is the unit of measure. Symbols for ecliptic coordinates are somewhat standardized; see the table.[24]

Summary of notation for ecliptic coordinates[25]
 	spherical	rectangular
longitude	latitude	distance
geocentric	Î»	Î²	Î
heliocentric	l	b	r	x, y, z[note 1]
 Occasional use; x, y, z are usually reserved for equatorial coordinates.
Ecliptic coordinates are convenient for specifying positions of Solar System objects, as most of the planets' orbits have small inclinations to the ecliptic, and therefore always appear relatively close to it on the sky. Because Earth's orbit, and hence the ecliptic, moves very little, it is a relatively fixed reference with respect to the stars.


Inclination of the ecliptic over 200,000 years, from Dziobek (1892).[26] This is the inclination to the ecliptic of 101,800 CE. Note that the ecliptic rotates by only about 7Â° during this time, whereas the celestial equator makes several complete cycles around the ecliptic. The ecliptic is a relatively stable reference compared to the celestial equator.
Because of the precessional motion of the equinox, the ecliptic coordinates of objects on the celestial sphere are continuously changing. Specifying a position in ecliptic coordinates requires specifying a particular equinox, that is, the equinox of a particular date, known as an epoch; the coordinates are referred to the direction of the equinox at that date. For instance, the Astronomical Almanac[27] lists the heliocentric position of Mars at 0h Terrestrial Time, 4 January 2010 as: longitude 118Â° 09' 15".8, latitude +1Â° 43' 16".7, true heliocentric distance 1.6302454 AU, mean equinox and ecliptic of date. This specifies the mean equinox of 4 January 2010 0h TT as above, without the addition of nutation.

Eclipses
Main article: Eclipse
Because the orbit of the Moon is inclined only about 5.145Â° to the ecliptic and the Sun is always very near the ecliptic, eclipses always occur on or near it. Because of the inclination of the Moon's orbit, eclipses do not occur at every conjunction and opposition of the Sun and Moon, but only when the Moon is near an ascending or descending node at the same time it is at conjunction (new) or opposition (full). The ecliptic is so named because the ancients noted that eclipses only occur when the Moon is crossing it.[28]

Equinoxes and solstices
Positions of equinoxes and solstices
 	ecliptic	equatorial
longitude	right ascension
March equinox	0Â°	0h
June solstice	90Â°	6h
September equinox	180Â°	12h
December solstice	270Â°	18h
Main article: Equinox (celestial coordinates)
The exact instants of equinoxes and solstices are the times when the apparent ecliptic longitude (including the effects of aberration and nutation) of the Sun is 0Â°, 90Â°, 180Â°, and 270Â°. Because of perturbations of Earth's orbit and anomalies of the calendar, the dates of these are not fixed.[29]

In the constellations

Equirectangular plot of declination vs right ascension of the modern constellations with a dotted line denoting the ecliptic. Constellations are colour-coded by family and year established. (detailed view)
The ecliptic currently passes through the following constellations:

Pisces
Aries
Taurus
Gemini
Cancer
Leo
Virgo
Libra
Scorpius
Ophiuchus[30]
Sagittarius
Capricornus
Aquarius
Astrology
Main article: Astrology
The ecliptic forms the center of the zodiac, a celestial belt about 20Â° wide in latitude through which the Sun, Moon, and planets always appear to move.[31] Traditionally, this region is divided into 12 signs of 30Â° longitude, each of which approximates the Sun's motion in one month.[32] In ancient times, the signs corresponded roughly to 12 of the constellations that straddle the ecliptic.[33] These signs are sometimes still used in modern terminology. The "First Point of Aries" was named when the March equinox Sun was actually in the constellation Aries; it has since moved into Pisces because of precession of the equinoxes.[34]

See also
Formation and evolution of the Solar System
Invariable plane
Protoplanetary disk
Celestial coordinate system
Notes and references
 Strictly, the plane of the mean orbit, with minor variations averaged out.
 USNO Nautical Almanac Office; UK Hydrographic Office, HM Nautical Almanac Office (2008). The Astronomical Almanac for the Year 2010. GPO. p. M5. ISBN 978-0-7077-4082-9.
 "LEVEL 5 Lexicon and Glossary of Terms".
 "The Ecliptic: the Sun's Annual Path on the Celestial Sphere".
 U.S. Naval Observatory Nautical Almanac Office (1992). P. Kenneth Seidelmann (ed.). Explanatory Supplement to the Astronomical Almanac. University Science Books, Mill Valley, CA. ISBN 0-935702-68-7., p. 11
 The directions north and south on the celestial sphere are in the sense toward the north celestial pole and toward the south celestial pole. East is the direction toward which Earth rotates, west is opposite that.
 Astronomical Almanac 2010, sec. C
 Explanatory Supplement (1992), sec. 1.233
 Explanatory Supplement (1992), p. 733
 Astronomical Almanac 2010, p. M2 and M6
 Explanatory Supplement (1992), sec. 1.322 and 3.21
 U.S. Naval Observatory Nautical Almanac Office; H.M. Nautical Almanac Office (1961). Explanatory Supplement to the Astronomical Ephemeris and the American Ephemeris and Nautical Almanac. H.M. Stationery Office, London. , sec. 2C
 Explanatory Supplement (1992), p. 731 and 737
 Chauvenet, William (1906). A Manual of Spherical and Practical Astronomy. I. J.B. Lippincott Co., Philadelphia. , art. 365â367, p. 694â695, at Google books
 Laskar, J. (1986). "Secular Terms of Classical Planetary Theories Using the Results of General Relativity". Bibcode:1986A&A...157...59L. , table 8, at SAO/NASA ADS
 Explanatory Supplement (1961), sec. 2B
 U.S. Naval Observatory, Nautical Almanac Office; H.M. Nautical Almanac Office (1989). The Astronomical Almanac for the Year 1990. U.S. Govt. Printing Office. ISBN 0-11-886934-5. , p. B18
 Astronomical Almanac 2010, p. B52
 Newcomb, Simon (1906). A Compendium of Spherical Astronomy. MacMillan Co., New York. , p. 226-227, at Google books
 Meeus, Jean (1991). Astronomical Algorithms. Willmann-Bell, Inc., Richmond, VA. ISBN 0-943396-35-2. , chap. 21
 "The Mean Plane (Invariable Plane) of the Solar System passing through the barycenter". 3 April 2009. Archived from the original on 3 June 2013. Retrieved 10 April 2009. produced with Vitagliano, Aldo. "Solex 10". Archived from the original (computer program) on 29 April 2009. Retrieved 10 April 2009.
 Danby, J.M.A. (1988). Fundamentals of Celestial Mechanics. Willmann-Bell, Inc., Richmond, VA. section 9.1. ISBN 0-943396-20-4.
 Roy, A.E. (1988). Orbital Motion (third ed.). Institute of Physics Publishing. section 5.3. ISBN 0-85274-229-0.
 Montenbruck, Oliver (1989). Practical Ephemeris Calculations. Springer-Verlag. ISBN 0-387-50704-3. , sec 1.4
 Explanatory Supplement (1961), sec. 2A
 Explanatory Supplement (1961), sec. 1G
 Dziobek, Otto (1892). Mathematical Theories of Planetary Motions. Register Publishing Co., Ann Arbor, Michigan., p. 294, at Google books
 Astronomical Almanac 2010, p. E14
 Ball, Robert S. (1908). A Treatise on Spherical Astronomy. Cambridge University Press. p. 83.
 Meeus (1991), chap. 26
 Serviss, Garrett P. (1908). Astronomy With the Naked Eye. Harper & Brothers, New York and London. pp. 105, 106.
 Bryant, Walter W. (1907). A History of Astronomy. p. 3. ISBN 9781440057922.
 Bryant (1907), p. 4.
 See, for instance, Leo, Alan (1899). Astrology for All. L.N. Fowler & Company. p. 8. astrology.
 Vallado, David A. (2001). Fundamentals of Astrodynamics and Applications (2nd ed.). El Segundo, CA: Microcosm Press. p. 153. ISBN 1-881883-12-4.
External links
The Ecliptic: the Sun's Annual Path on the Celestial Sphere Durham University Department of Physics
Seasons and Ecliptic Simulator University of Nebraska-Lincoln
MEASURING THE SKY A Quick Guide to the Celestial Sphere James B. Kaler, University of Illinois
Earth's Seasons U.S. Naval Observatory
The Basics - the Ecliptic, the Equator, and Coordinate Systems AstrologyClub.Org
Kinoshita, H.; Aoki, S. (1983). "The definition of the ecliptic". Celestial Mechanics. 31 (4): 329â338. Bibcode:1983CeMec..31..329K. doi:10.1007/BF01230290.; comparison of the definitions of LeVerrier, Newcomb, and Standish.
	Look up ecliptic in Wiktionary, the free dictionary.
	Wikiversity has learning resources about Ecliptic at
Quizzes
Categories: Celestial coordinate systemDynamics of the Solar SystemTechnical factors of astrology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Bosanski
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
69 more
Edit links
This page was last edited on 17 September 2020, at 17:20 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

This is a featured article. Click here for more information. Page semi-protected Listen to this article
Earth
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
This article is about the planet. For its human aspects, see World. For other uses, see Earth (disambiguation) and Planet Earth (disambiguation).
Earth Astronomical symbol of Earth
"The Blue Marble" photograph of Earth, taken by the Apollo 17 mission. The Arabian peninsula, Africa and Madagascar lie in the upper half of the disc, whereas Antarctica is at the bottom.
"The Blue Marble" the most widely used photograph of Earth,[1][2] taken by the Apollo 17 mission in 1972.
Designations
Alternative names	Gaia, Gaea, Terra, Tellus, the World, the Globe
Adjectives	Earthly, terrestrial, terran, tellurian
Orbital characteristics
Epoch J2000[n 1]
Aphelion	152.1 Gm (94500000 mi)[n 2]
Perihelion	147.095 Gm (91401000 mi)[n 2]
Semi-major axis	149.598023 Gm (92955902 mi)[3]
Eccentricity	0.0167086[3]
Orbital period	365.256363004 d[4]
(31558.1497635 ks)
Average orbital speed	29.78 km/s[5]
(107200 km/h; 66600 mph)
Mean anomaly	358.617Â°
Inclination
7.155Â° to the Sun's equator;
1.57869Â°[6] to invariable plane;
0.00005Â° to J2000 ecliptic
Longitude of ascending node	â11.26064Â°[5] to J2000 ecliptic
Time of perihelion	2021-Jan-02 13:59[7]
Argument of perihelion	114.20783Â°[5]
Satellites
1 natural satellite: the Moon
5 quasi-satellites
>1â800 operational artificial satellites[8]
>16â000 space debris[n 3]
Physical characteristics
Mean radius	6371.0 km (3958.8 mi)[9]
Equatorial radius	6378.1 km (3963.2 mi)[10][11]
Polar radius	6356.8 km (3949.9 mi)[12]
Flattening	0.0033528[13]
1/298.257222101 (ETRS89)
Circumference
40075.017 km equatorial (24901.461 mi)[11]
40007.86 km meridional (24859.73 mi)[14][n 4]
Surface area
510072000 km2 (196940000 sq mi)[15][n 5]
148940000 km2 land (57510000 sq mi)
361132000 km2 water (139434000 sq mi)
Volume	1.08321Ã1012 km3 (2.59876Ã1011 cu mi)[5]
Mass	5.97237Ã1024 kg (1.31668Ã1025 lb)[16]
(3.0Ã10â6 Mâ)
Mean density	5.514 g/cm3 (0.1992 lb/cu in)[5]
Surface gravity	9.80665 m/s2 (1 g; 32.1740 ft/s2)[17]
Moment of inertia factor	0.3307[18]
Escape velocity	11.186 km/s[5]
(40270 km/h; 25020 mph)
Sidereal rotation period	0.99726968 d[19]
(23h 56m 4.100s)
Equatorial rotation velocity	0.4651 km/s[20]
(1674.4 km/h; 1040.4 mph)
Axial tilt	23.4392811Â°[4]
Albedo
0.367 geometric[5]
0.306 Bond[5]
Surface temp.	min	mean	max
Kelvin	184 K[21]	287.16 K[22] (1961â90)	330 K[23]
Celsius	â89.2 Â°C	14.0 Â°C (1961â90)	56.7 Â°C
Fahrenheit	â128.5 Â°F	57.2 Â°F (1961â90)	134.0 Â°F
Atmosphere
Surface pressure	101.325 kPa (at MSL)
Composition by volume
78.08% nitrogen (N
2; dry air)[5]
20.95% oxygen (O
2)
~â1% water vapor (climate variable)
0.9340% argon
0.0413% carbon dioxide[24]
0.00182% neon[5]
0.00052% helium
0.00019% methane
0.00011% krypton
0.00006% hydrogen
Earth is the third planet from the Sun and the only astronomical object known to harbor life. About 29% of Earth's surface is land consisting of continents and islands. The remaining 71% is covered with water, mostly by oceans but also lakes, rivers and other fresh water, which together constitute the hydrosphere. Much of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's outer layer is divided into several rigid tectonic plates that migrate across the surface over many millions of years. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates Earth's magnetic field, and a convecting mantle that drives plate tectonics.

According to radiometric dating estimation and other evidence, Earth formed over 4.5 billion years ago. Within the first billion years of Earth's history, life appeared in the oceans and began to affect Earth's atmosphere and surface, leading to the proliferation of anaerobic and, later, aerobic organisms. Some geological evidence indicates that life may have arisen as early as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties and geological history have allowed life to evolve and thrive. In the history of life on Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinctions. Over 99% of all species that ever lived on Earth are extinct. Almost 8 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans increasingly impact Earth's hydrology, atmospheric processes and other life.

Earth's atmosphere consists mostly of nitrogen and oxygen. Tropical regions receive more energy from the Sun than polar regions, which is redistributed by atmospheric and ocean circulation. Greenhouse gases also play an important role in regulating the surface temperature. A region's climate is not only determined by latitude, but also by its proximity to moderating oceans and height among other factors. Extreme weather, such as tropical cyclones and heat waves, occurs in most areas and has a large impact on life.

Earth's gravity interacts with other objects in space, especially the Sun and the Moon, which is Earth's only natural satellite. Earth orbits around the Sun in about 365.25 days. Earth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest and most massive of the four rocky planets.


Contents
1	Etymology
2	Chronology
2.1	Formation
2.2	Geological history
2.3	Origin of life and evolution
2.4	Future
3	Physical characteristics
3.1	Shape
3.2	Chemical composition
3.3	Internal structure
3.4	Heat
3.5	Tectonic plates
3.6	Surface
3.7	Gravitational field
3.8	Magnetic field
4	Orbit and rotation
4.1	Rotation
4.2	Orbit
4.3	Axial tilt and seasons
5	Earth-Moon system
5.1	Moon
5.2	Asteroids and artificial satellites
6	Hydrosphere
7	Atmosphere
7.1	Weather and climate
7.2	Upper atmosphere
8	Life on Earth
9	Human geography
9.1	Natural resources and land use
10	Cultural and historical viewpoint
11	See also
12	Notes
13	References
14	Further reading
15	External links
Etymology
The modern English word Earth developed, via Middle English,[n 6] from an Old English noun most often spelled eorÃ°e.[25] It has cognates in every Germanic language, and their ancestral root has been reconstructed as *erÃ¾Å. In its earliest attestation, the word eorÃ°e was already being used to translate the many senses of Latin terra and Greek Î³á¿ gÄ: the ground,[n 7] its soil,[n 8] dry land,[n 9] the human world,[n 10] the surface of the world (including the sea),[n 11] and the globe itself.[n 12] As with Roman Terra/TellÅ«s and Greek Gaia, Earth may have been a personified goddess in Germanic paganism: late Norse mythology included JÃ¶rÃ° ('Earth'), a giantess often given as the mother of Thor.[34]

Historically, earth has been written in lowercase. From early Middle English, its definite sense as "the globe" was expressed as the earth. By Early Modern English, many nouns were capitalized, and the earth was also written the Earth, particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as Earth, by analogy with the names of the other planets, though earth and forms with the remain common.[25] House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes "Earth" when appearing as a name (e.g. "Earth's atmosphere") but writes it in lowercase when preceded by the (e.g. "the atmosphere of the earth"). It almost always appears in lowercase in colloquial expressions such as "what on earth are you doing?"[35]

Occasionally, the name Terra /ËtÉrÉ/ is used in scientific writing and especially in science fiction to distinguish our inhabited planet from others,[36] while in poetry Tellus /ËtÉlÉs/ has been used to denote personification of the Earth.[37] The Greek poetic name Gaea (GÃ¦a) /ËdÊiËÉ/ is rare, though the alternative spelling Gaia has become common due to the Gaia hypothesis, in which case its pronunciation is /ËÉ¡aÉªÉ/ rather than the more Classical /ËÉ¡eÉªÉ/.[38]

There are a number of adjectives for the planet Earth. From Earth itself comes earthly. From Latin Terra come Terran /ËtÉrÉn/,[39] Terrestrial /tÉËrÉstriÉl/,[40] and (via French) Terrene /tÉËriËn/,[41] and from Latin Tellus come Tellurian /tÉËlÊÉriÉn/[42] and, more rarely, Telluric and Tellural. From Greek Gaia and Gaea comes Gaian and Gaean.

Chronology
Main article: History of Earth
Formation

Artist's impression of the early Solar System's planetary disk
The oldest material found in the Solar System is dated to 4.5682+0.0002
â0.0004 Ga (billion years) ago.[43] By 4.54Â±0.04 Ga the primordial Earth had formed.[44] The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth being estimated as likely taking anywhere from 70â100 million years to form.[45]

Estimates of the age of the Moon range from 4.5 Ga to significantly younger.[46] A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object with about 10% of Earth's mass, named Theia, collided with Earth.[47] It hit Earth with a glancing blow and some of its mass merged with Earth.[48][49] Between approximately 4.1 and 3.8 Ga, numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.[50]

Geological history
Main article: Geological history of Earth

Angular unconformity between Carboniferous rocks that were folded, uplifted and eroded during the Variscan orogeny that completed the formation of the Pangaea supercontinent, before deposition of the overlying Triassic strata, Algarve, Portugal
Earth's atmosphere and oceans were formed by volcanic activity and outgassing.[51] Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets.[52] Sufficient water to fill the oceans may have always been on the Earth since the beginning of the planet's formation.[53] In this model, atmospheric greenhouse gases kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity.[54] By 3.5 Ga, Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.[55]

As the molten outer layer of Earth cooled it formed the first solid crust, which is thought to have been mafic in composition. The first continental crust, which was more felsic in composition, formed by the partial melting of this mafic crust. The presence of grains of the mineral zircon of Hadean age in Eoarchean sedimentary rocks suggests that at least some felsic crust existed as early as 4.4 Ga, only 140 Ma after Earth's formation.[56] There are two main models of how this initial small volume of continental crust evolved to reach its current abundance:[57] (1) a relatively steady growth up to the present day,[58] which is supported by the radiometric dating of continental crust globally and (2) an initial rapid growth in the volume of continental crust during the Archean, forming the bulk of the continental crust that now exists,[59][60] which is supported by isotopic evidence from Hafnium in zircons and Neodymium in sedimentary rocks. The two models and the data that support them can be reconciled by large-scale recycling of the continental crust, particularly during the early stages of Earth's history.[61]

New continental crust forms as a result of plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, tectonic forces have caused areas of continental crust to group together to form supercontinents that have subsequently broken apart. At approximately 750 Ma, one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia at 600â540 Ma, then finally Pangaea, which also began to break apart at 180 Ma.[62]

The most recent pattern of ice ages began about 40 Ma,[63] and then intensified during the Pleistocene about 3 Ma.[64] High- and middle-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every 21,000, 41,000 and 100,000 years.[65] The Last Glacial Period, colloquially called the "last ice age", covered large parts of the continents to middle latitudes and ended about 11,700 years ago.

Origin of life and evolution
Life timeline
This box: viewtalkedit
-4500 ââ-4000 ââ-3500 ââ-3000 ââ-2500 ââ-2000 ââ-1500 ââ-1000 ââ-500 ââ0 â
Water
Single-celled
life
Photosynthesis
Eukaryotes
Multicellular
life
Arthropods Molluscs
Plants
Dinosaurs
Mammals
Flowers
Birds
Primates











â
Earth (â4540)
â
Earliest water
â
Earliest life
â
LHB meteorites
â
Earliest oxygen
â
Atmospheric oxygen
â
Oxygen crisis
â
Earliest fungi
â
Sexual reproduction
â
Earliest plants
â
Earliest animals
â
Ediacaran
â
Cambrian
â
Tetrapoda
â
Earliest apes
P
h
a
n
e
r
o
z
o
i
c







P
r
o
t
e
r
o
z
o
i
c



A
r
c
h
e
a
n
H
a
d
e
a
n
Pongola
Huronian
Cryogenian
Andean
Karoo
Quaternary
Ice Ages
(million years ago)
Main articles: Origin of Life and Evolutionary history of life

Phylogenetic tree of life on Earth based on rRNA analysis
Chemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose.[66] The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen (O
2) accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer (O
3) in the upper atmosphere.[67] The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes.[68] True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface.[69] Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia,[70] biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland,[71] and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia.[72][73] The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.[74][75]

During the Neoproterozoic, 1000 to 541 Ma, much of Earth might have been covered in ice. This hypothesis has been termed "Snowball Earth", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity.[76][77] Following the Cambrian explosion, 535 Ma, there have been at least five major mass extinctions and many minor ones.[78][79] Apart from the proposed current Holocene extinction event, the most recent was 66 Ma, when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but largely spared small animals such as insects, mammals, lizards and birds. Mammalian life has diversified over the past 66 Mys, and several million years ago an African ape gained the ability to stand upright.[80] This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.[81]

Future
Main article: Future of Earth
See also: Global catastrophic risk
Because carbon dioxide (CO
2) has a long life time in the atmosphere, moderate human CO
2 emissions may postpone the next glacial inception by 100,000 years.[82] Earth's expected long-term future is tied to that of the Sun. Over the next 1.1 billion years, solar luminosity will increase by 10%, and over the next 3.5 billion years by 40%.[83] Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing CO
2 concentration to levels lethally low for plants (10 ppm for C4 photosynthesis) in approximately 100â900 million years.[84][85] The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible.[86] Due to the increased luminosity, Earth's mean tempearture may reach 100 Â°C (212 Â°F) in 1.5 billion years, and all ocean water will evaporate and be lost to space within an estimated 1.6 to 3 billion years.[87] Even if the Sun were stable, a fraction of the water in the modern oceans will descend to the mantle, due to reduced steam venting from mid-ocean ridges.[87][88]

The Sun will evolve to become a red giant in about 5 billion years. Models predict that the Sun will expand to roughly 1 AU (150 million km; 93 million mi), about 250 times its present radius.[83][89] Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit 1.7 AU (250 million km; 160 million mi) from the Sun when the star reaches its maximum radius.[83]

Physical characteristics
Shape
Main articles: Figure of the Earth, Earth radius, and Earth's circumference

The summit of Chimborazo, the point on the Earth's surface that is farthest from the Earth's center[90]
The shape of Earth is nearly spherical. There is a small flattening at the poles and bulging around the equator due to Earth's rotation.[91] To second order, Earth is approximately an oblate spheroid, whose equatorial diameter is 43 kilometres (27 mi) larger than the pole-to-pole diameter.[92]

The point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador (6,384.4 km or 3,967.1 mi).[93][94][95] The average diameter of the reference spheroid is 12,742 kilometres (7,918 mi). Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: the maximum deviation of only 0.17% is at the Mariana Trench (10,925 metres or 35,843 feet below local sea level),[96] whereas Mount Everest (8,848 metres or 29,029 feet above local sea level) represents a deviation of 0.14%.[n 13][98] In geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.[99]

Chemical composition
See also: Abundance of elements on Earth
Chemical composition of the crust[100][101]
Compound	Formula	Composition
Continental	Oceanic
silica	SiO
2	60.6%	48.6%
alumina	Al
2O
3	15.9%	16.5%
lime	CaO	6.41%	12.3%
magnesia	MgO	4.66%	6.8%
iron oxide	FeOT	6.71%	6.2%
sodium oxide	Na
2O	3.07%	2.6%
potassium oxide	K
2O	1.81%	0.4%
titanium dioxide	TiO
2	0.72%	1.4%
phosphorus pentoxide	P
2O
5	0.13%	0.3%
manganese oxide	MnO	0.10%	1.4%
Total	100.1%	99.9%
Earth's mass is approximately 5.97Ã1024 kg (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulphur (2.9%), nickel (1.8%), calcium (1.5%), and aluminum (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulphur (4.5%), and less than 1% trace elements.[102]

The most common rock constituents of the crust are nearly all oxides: chlorine, sulphur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash and soda.[103][102]

Internal structure
Main article: Structure of the Earth
Geologic layers of Earth[104]
Earth cutaway schematic-en.svg

Earth cutaway from core to exosphere. Not to scale.
Depth[105]
km	Component layer	Density
g/cm3
0â60	Lithosphere[n 14]	â
0â35	Crust[n 15]	2.2â2.9
35â660	Upper mantle	3.4â4.4
  660-2890	Lower mantle	3.4â5.6
100â700	Asthenosphere	â
2890â5100	Outer core	9.9â12.2
5100â6378	Inner core	12.8â13.1
Earth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the MohoroviÄiÄ discontinuity.[106] The thickness of the crust varies from about 6 kilometres (3.7 mi) under the oceans to 30â50 km (19â31 mi) for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, which is divided into independently moving tectonic plates.[107]

Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at 410 and 660 km (250 and 410 mi) below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core.[108] Earth's inner core may be rotating at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1â0.5Â° per year, although both somewhat higher and much lower rates have also been proposed.[109] The radius of the inner core is about one fifth of that of Earth. Density increases with depth, as described in the table below.

Heat
Main article: Earth's internal heat budget
The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232.[110] At the center, the temperature may be up to 6,000 Â°C (10,830 Â°F),[111] and the pressure could reach 360 GPa (52 million psi).[112] Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately 3 Gyr, twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.[113][114]

Present-day major heat-producing isotopes[113]
Isotope	Heat release
W
/
kg isotope
Half-life
years	Mean mantle concentration
kg isotope
/
kg mantle
Heat release
W
/
kg mantle
238U	94.6Ã10â6	4.47Ã109	30.8Ã10â9	2.91Ã10â12
235U	569Ã10â6	0.704Ã109	0.22Ã10â9	0.125Ã10â12
232Th	26.4Ã10â6	14.0Ã109	124Ã10â9	3.27Ã10â12
40K	29.2Ã10â6	1.25Ã109	36.9Ã10â9	1.08Ã10â12
The mean heat loss from Earth is 87 mW mâ2, for a global heat loss of 4.42Ã1013 W.[115] A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts.[116] More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.[117]

Tectonic plates
Main article: Plate tectonics
Earth's major plates[118]
Shows the extent and boundaries of tectonic plates, with superimposed outlines of the continents they support
Plate name	Area
106 km2
  Pacific Plate
103.3
  African Plate[n 16]
78.0
  North American Plate
75.9
  Eurasian Plate
67.8
  Antarctic Plate
60.9
  Indo-Australian Plate
47.2
  South American Plate
43.6
Earth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur.[119] The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.[120]

As the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than 100 Ma old. The oldest oceanic crust is located in the Western Pacific and is estimated to be 200 Ma old.[121][122] By comparison, the oldest dated continental crust is 4,030 Ma,[123] although zircons have been found preserved as clasts within Eoarchean sedimentary rocks that give ages up to 4,400 Ma, indicating that at least some continental crust existed at that time.[56]

The seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between 50 and 55 Ma. The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of 75 mm/a (3.0 in/year)[124] and the Pacific Plate moving 52â69 mm/a (2.0â2.7 in/year). At the other extreme, the slowest-moving plate is the South American Plate, progressing at a typical rate of 10.6 mm/a (0.42 in/year).[125]

Surface
Main articles: Earth's crust, Landform, and Extreme points of Earth
See also: Planetary surface

Current Earth without water, elevation greatly exaggerated (click/enlarge to "spin" 3D-globe).
The total surface area of Earth is about 510 million km2 (197 million sq mi).[15] Of this, 70.8%,[15] or 361.13 million km2 (139.43 million sq mi), is below sea level and covered by ocean water.[126] Below the ocean's surface are much of the continental shelf, mountains, volcanoes,[92] oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or 148.94 million km2 (57.51 million sq mi), not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. The elevation of the land surface varies from the low point of â418 m (â1,371 ft) at the Dead Sea, to a maximum altitude of 8,848 m (29,029 ft) at the top of Mount Everest. The mean height of land above sea level is about 797 m (2,615 ft).[127]

The continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors.[128] Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust.[129] The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine.[130] Common carbonate minerals include calcite (found in limestone) and dolomite.[131]

Erosion and tectonics, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape Earth's surface over geological time.[132][133]

The pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland.[134][135] Close to 40% of Earth's land surface is used for agriculture, or an estimated 16.7 million km2 (6.4 million sq mi) of cropland and 33.5 million km2 (12.9 million sq mi) of pastureland.[136]

Gravitational field
Main article: Gravity of Earth

Earth's gravity measured by NASA's GRACE mission, showing deviations from the theoretical gravity. Red shows where gravity is stronger than the smooth, standard value, and blue shows where it is weaker.
The gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within Earth. Near Earth's surface, gravitational acceleration is approximately 9.8 m/s2 (32 ft/s2). Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in Earth's gravitational field, known as gravity anomalies.[137]

Magnetic field
Main article: Earth's magnetic field
The main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is 3.05Ã10â5 T, with a magnetic dipole moment of 7.79Ã1022 Am2 at epoch 2000, decreasing nearly 6% per century.[138] The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.[139][140]

Magnetosphere
Main article: Magnetosphere
Diagram showing the magnetic field lines of Earth's magnetosphere. The lines are swept back in the anti-solar direction under the influence of the solar wind.
Schematic of Earth's magnetosphere. The solar wind flows from left to right
The extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail.[141] Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bow shock precedes the dayside magnetosphere within the solar wind.[142] Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates.[143][144] The ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field,[145] and the Van Allen radiation belts are formed by high-energy particles whose motion is essentially random, but contained in the magnetosphere.[146][147]

During magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.[148]

Orbit and rotation
Rotation
Main article: Earth's rotation

Earth's rotation imaged by DSCOVR EPIC on 29 May 2016, a few weeks before a solstice.
Earth's rotation period relative to the Sunâits mean solar dayâis 86,400 seconds of mean solar time (86,400.0025 SI seconds).[149] Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between 0 and 2 ms longer than the mean solar day.[150][151]

Earth's rotation period relative to the fixed stars, called its stellar day by the International Earth Rotation and Reference Systems Service (IERS), is 86,164.0989 seconds of mean solar time (UT1), or 23h 56m 4.0989s.[4][n 17] Earth's rotation period relative to the precessing or moving mean March equinox (when the Sun is at 90Â° on the equator), is 86,164.0905 seconds of mean solar time (UT1) (23h 56m 4.0905s).[4] Thus the sidereal day is shorter than the stellar day by about 8.4 ms.[152]

Apart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15Â°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.[153][154]

Orbit
Main article: Earth's orbit

The Pale Blue Dot photo taken in 1990 by the Voyager 1 spacecraft showing Earth (center right) from nearly 6.0 billion km (3.7 billion mi) away, about 5.6 hours at light speed.[155]
Earth orbits the Sun at an average distance of about 150 million km (93 million mi) every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1Â°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hoursâa solar dayâfor Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about 29.78 km/s (107,200 km/h; 66,600 mph), which is fast enough to travel a distance equal to Earth's diameter, about 12,742 km (7,918 mi), in seven minutes, and the distance to the Moon, 384,000 km (239,000 mi), in about 3.5 hours.[5]

The Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the EarthâMoon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the EarthâSun plane (the ecliptic), and the EarthâMoon plane is tilted up to Â±5.1 degrees against the EarthâSun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.[5][156]

The Hill sphere, or the sphere of gravitational influence, of Earth is about 1.5 million km (930,000 mi) in radius.[157][n 18] This is the maximum distance at which Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.[157]

Earth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.[158]

Axial tilt and seasons
Main article: Axial tilt Â§ Earth

Earth's axial tilt (or obliquity) and its relation to the rotation axis and plane of orbit
The axial tilt of Earth is approximately 23.439281Â°[4] with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter.[159] Above the Arctic Circle and below the Antarctic Circle there is no daylight at all for part of the year, causing a polar night, and this night extends for several months at the poles themselves. These same latitudes also experience a midnight sun, where the sun remains visible all day.[160][161]

By astronomical convention, the four seasons can be determined by the solsticesâthe points in the orbit of maximum axial tilt toward or away from the Sunâand the equinoxes, when Earth's rotational axis is aligned with its orbital axis. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.[162]

The angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years.[163] The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.[164]

In modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing EarthâSun distance causes an increase of about 6.8% in solar energy reaching Earth at perihelion relative to aphelion.[165][n 19] Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.[166]

Earth-Moon system
Main articles: Orbit of the Moon and Satellite system (astronomy)
Moon
Main articles: Moon and Lunar theory
Characteristics
Full moon as seen from Earth's Northern Hemisphere
Diameter	3,474.8 km
Mass	7.349Ã1022 kg
Semi-major axis	384,400 km
Orbital period	27d 7h 43.7m
The Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto.[167][168] The natural satellites of other planets are also referred to as "moons", after Earth's.[169] The most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.[48]

The gravitational attraction between Earth and the Moon causes tides on Earth.[170] The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet.[171] As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases.[172] Due to their tidal interaction, the Moon recedes from Earth at the rate of approximately 38 mm/a (1.5 in/year). Over millions of years, these tiny modificationsâand the lengthening of Earth's day by about 23 Âµs/yrâadd up to significant changes.[173] During the Ediacaran period, for example, (approximately 620 Ma) there were 400Â±7 days in a year, with each day lasting 21.9Â±0.4 hours.[174]

The Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon.[175] Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting large changes over millions of years, as is the case for Mars, though this is disputed.[176][177]

Viewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant.[154] This allows total and annular solar eclipses to occur on Earth.[178]

Asteroids and artificial satellites
Main article: Near-Earth object

Tracy Caldwell Dyson viewing Earth from the ISS Cupola, 2010
Earth's co-orbital asteroids population consists of quasi-satellites, objects with a horseshoe orbit and trojans. There are at least five quasi-satellites, including 469219 KamoÊ»oalewa.[179][180] A trojan asteroid companion, 2010 TK7, is librating around the leading Lagrange triangular point, L4, in Earth's orbit around the Sun.[181][182] The tiny near-Earth asteroid 2006 RH120 makes close approaches to the EarthâMoon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.[183]

As of April 2020, there are 2,666 operational, human-made satellites orbiting Earth.[8] There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris.[n 3] Earth's largest artificial satellite is the International Space Station.[184]

Hydrosphere
Main article: Hydrosphere
Water typically evaporates over water surfaces like oceans and is transported to land via the atmosphere. Precipitation in the form of snow, rain and more then brings it back to the surface. A system of rivers brings the water back to oceans and seas.
Water is transported to various parts of the hydrosphere via the water cycle.
The abundance of water on Earth's surface is a unique feature that distinguishes the "Blue Planet" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of 2,000 m (6,600 ft). The mass of the oceans is approximately 1.35Ã1018 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of 361.8 million km2 (139.7 million sq mi) with a mean depth of 3,682 m (12,080 ft), resulting in an estimated volume of 1.332 billion km3 (320 million cu mi).[185] If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be 2.7 to 2.8 km (1.68 to 1.74 mi).[186] About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.[187]

In Earth's coldest regions, snow survives over the summer and eventually changes into ice. This accumulated snow and eyes eventually forms into glaciers, bodies of ice that flow under the influence of their own gravity. Alpine glaciers form in mountainous areas, whereas vast ice sheets form over land in polar regions. The flow of glaciers erodes the surface changing it dramatically, with the formation of U-shaped valleys and other landforms.[188] Sea ice in the Arctic covers an area about as big as the United States, although it is quickly retreating as a consequence of climate change.[189]

The average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt).[190] Most of this salt was released from volcanic activity or extracted from cool igneous rocks.[191] The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms.[192] Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir.[193] Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El NiÃ±oâSouthern Oscillation.[194]

Atmosphere
Main article: Atmosphere of Earth

Satellite image of Earth cloud cover using NASA's Moderate-Resolution Imaging Spectroradiometer

NASA photo showing the Earth's atmosphere, with the setting sun, with the Earth's landmass in shadow
The atmospheric pressure at Earth's sea level averages 101.325 kPa (14.696 psi),[195] with a scale height of about 8.5 km (5.3 mi).[5] A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules.[195] Water vapor content varies between 0.01% and 4%[195] but averages about 1%.[5] The height of the troposphere varies with latitude, ranging between 8 km (5 mi) at the poles to 17 km (11 mi) at the equator, with some variation resulting from weather and seasonal factors.[196]

Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved 2.7 Gya, forming the primarily nitrogenâoxygen atmosphere of today.[67] This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric O
2 into O
3. The ozone layer blocks ultraviolet solar radiation, permitting life on land.[197] Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature.[198] This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be â18 Â°C (0 Â°F), in contrast to the current +15 Â°C (59 Â°F),[199] and life on Earth probably would not exist in its current form.[200]

Weather and climate
Main articles: Weather and Climate
Earth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first 11 km (6.8 mi) of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.[201]


Hurricane Felix seen from low Earth orbit, September 2007

Massive clouds above the Mojave Desert, February 2016
The primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30Â° latitude and the westerlies in the mid-latitudes between 30Â° and 60Â°.[202] Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.[203]

The amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about 0.4 Â°C (0.7 Â°F) per degree of latitude from the equator.[204] Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.[205]

Further factors that affect a location's climates are its proximity to oceans, the oceanic and atmospheric circulation, and topology.[206] Places close to oceans typically have colder summers and warmer winters, due to the fact that oceans can the store large amounts of heat. The wind transports the cold or the heat of the ocean to the land.[207] Atmospheric circulation also plays an important role: San Francisco and Washington DC are both coastal cities at about the same latitude. San Francisco's climate is significantly more moderate as the prevailing wind direction is from sea to land.[208] Finally, temperatures decrease with height causing mountainous areas to be colder than low-lying areas.[209]

Water vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation.[201] Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.[210]

The commonly used KÃ¶ppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes.[202] The KÃ¶ppen system rates regions based on observed temperature and precipitation.[211] Surface air temperature can rise to around 55 Â°C (131 Â°F) in hot deserts, such as Death Valley, and can fall as low as â89 Â°C (â128 Â°F) in Antarctica.[212][213]

Upper atmosphere

This view from orbit shows the full moon partially obscured by Earth's atmosphere.
Above the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere.[198] Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind.[214] Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The KÃ¡rmÃ¡n line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.[215]

Thermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases.[216] The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere.[217] Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth.[218] In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.[219]

Life on Earth

A volcano injecting hot ash into the atmosphere
A planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid waterâan environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism.[220] The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.[221]

A planet's life forms inhabit ecosystems, whose total forms the biosphere.[222] Earth's biosphere is thought to have begun evolving about 3.5 Gya.[67] The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals.[223] On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.[224] Estimates of the number of species on Earth today vary; most species have not been described.[225]

Extreme weather, such as tropical cyclones (including hurricanes and typhoons), occurs over most of Earth's surface and has a large impact on life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year.[226] Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, blizzards, floods, droughts, wildfires, and other calamities and disasters.[227] Human impact is felt in many areas due to pollution of the air and water, acid rain, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.[228] There is a scientific consensus that humans are causing global warming by releasing greenhouse gases into the atmosphere.[229] This is driving changes such as the melting of glaciers and ice sheets, a global rise in average sea levels, and significant shifts in weather.[230]

Human geography
Main articles: Human geography and World

The seven continents of Earth:[231]
  North America
  South America
  Antarctica
  Europe
  Africa
  Asia
  Australia
vte
Earth's human population passed seven billion in the early 2010s,[232] and is projected to peak at around ten billion in the second half of the 21st century.[233] Most of the growth is expected to take place in sub-Saharan Africa.[233] Human population density varies widely around the world, but a majority live in Asia. By 2050, 68% of the world's population is expected to be living in urban, rather than rural, areas.[234] 68% of the land mass of the world is in the Northern Hemisphere.[235] Partly due to the predominance of land mass, 90% of humans live in the Northern Hemisphere.[236]

It is estimated that one-eighth of Earth's surface is suitable for humans to live on â three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%),[237] high mountains (27%),[238] or other unsuitable terrains. States claim the planet's entire land surface, except for parts of Antarctica and a few other unclaimed areas. Earth has never had a planetwide government, but the United Nations is the leading worldwide intergovernmental organization.[239][240] The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada (82Â°28â²N).[241] The southernmost is the AmundsenâScott South Pole Station, in Antarctica, almost exactly at the South Pole (90Â°S).[242]

The first human to orbit Earth was Yuri Gagarin on 12 April 1961.[243] In total, about 550 people have visited outer space and reached orbit as of November 2018, and, of these, twelve have walked on the Moon.[244][245] Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months.[246] The farthest that humans have traveled from Earth is 400,171 km (248,655 mi), achieved during the Apollo 13 mission in 1970.[247]

Natural resources and land use
Main articles: Natural resource and Land use
Land use in 2015 as a percentage of ice-free land surface[248]
Land use	Percentage
Cropland	12 â 14%
Pastures	30 â 47%
Human-used forests	16 â 27%
Infrastructure	1%
Unused land	24 â 31%
Earth has resources that have been exploited by humans.[249] Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.[250] Large deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas.[251] These deposits are used by humans both for energy production and as feedstock for chemical production.[252] Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics.[253] These metals and other elements are extracted by mining, a process which often brings environmental and health damage.[254]

Earth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of organic waste. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends on dissolved nutrients washed down from the land.[255] In 2019, 39 million km2 (15 million sq mi) of Earth's land surface consisted of forest and woodlands, 12 million km2 (4.6 million sq mi) was shrub and grassland, 40 million km2 (15 million sq mi) were used for animal feed production and grazing, and 11 million km2 (4.2 million sq mi) were cultivated as croplands.[256] Of the 12â14% of ice-free land that is used for croplands, 2 percent point was irrigated in 2015.[248] Humans use building materials to construct shelters.[257]

Cultural and historical viewpoint
Main article: Earth in culture

Earthrise, taken in 1968 by William Anders, an astronaut on board Apollo 8
Human cultures have developed many views of the planet.[258] The standard astronomical symbol of Earth consists of a cross circumscribed by a circle, Earth symbol.svg,[259] representing the four corners of the world. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity.[260] Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.[260] The Gaia Principle, developed mid-20th century, compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability.[261][262][263] Images of Earth taken from space, particularly during the Apollo program, have been credited with altering the way that people viewed the planet that they lived on, emphasising its beauty, uniqueness and apparent fragility.[264][265]

Scientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth,[266][267][268] which was attributed to both the philosophers Pythagoras and Parmenides.[267][268][needs update] By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals.[269][globalize] Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System.[270]

It was only during the 19th century that geologists realized Earth's age was at least many millions of years.[271] Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old.[272][273]

See also
Celestial sphere
Earth phase
Earth physical characteristics tables
Earth science
Earth system science
Earthling
List of gravitationally rounded objects of the Solar System
Outline of Earth
Timeline of natural history
Timeline of the far future
Notes
 All astronomical quantities vary, both secularly and periodically. The quantities given are the values at the instant J2000.0 of the secular variation, ignoring all periodic variations.
 aphelion = a Ã (1 + e); perihelion = a Ã (1 â e), where a is the semi-major axis and e is the eccentricity. The difference between Earth's perihelion and aphelion is 5 million kilometers.Wilkinson, John (8 January 2009). Probing the New Solar System. CSIRO Publishing. p. 144. ISBN 9780643099494.
 As of 4 January 2018, the United States Strategic Command tracked a total of 18,835 artificial objects, mostly debris. See: Anz-Meador, Phillip; Shoots, Debi, eds. (February 2018). "Satellite Box Score" (PDF). Orbital Debris Quarterly News. 22 (1): 12. Retrieved 18 April 2018.
 Earth's circumference is almost exactly 40,000 km because the metre was calibrated on this measurementâmore specifically, 1/10-millionth of the distance between the poles and the equator.
 Due to natural fluctuations, ambiguities surrounding ice shelves, and mapping conventions for vertical datums, exact values for land and ocean coverage are not meaningful. Based on data from the Vector Map and Global Landcover Archived 26 March 2015 at the Wayback Machine datasets, extreme values for coverage of lakes and streams are 0.6% and 1.0% of Earth's surface. The ice sheets of Antarctica and Greenland are counted as land, even though much of the rock that supports them lies below sea level.
 Middle English spellings include eorÃ¾e, erÃ¾e, erde, and erthe.[25]
 As in Beowulf (1531â33):
Wearp Ã°a wundelmÃ¦l   wrÃ¦ttum gebunden
yrre oretta,   Ã¾Ã¦t hit on eorÃ°an lÃ¦g,
stiÃ° ond stylecg.[25][26]
"He threw the artfully-wound sword so that it lay upon the earth, firm and sharp-edged."[26]
 As in the Old English glosses of the Lindisfarne Gospels (Luke 13:7):
Succidite ergo illam ut quid etiam terram occupat: hrendas uel scearfaÃ° forÃ°on Ã°ailca uel hia to huon uutedlice eorÃ°o gionetaÃ° uel gemerras.[25]
"Remove it. Why should it use up the soil?"[27]
 As in Ãlfric's Heptateuch (Gen. 1:10):
Ond God gecygde Ã°a drignysse eorÃ°an ond Ã°Ã¦re wÃ¦tera gegaderunge he het sÃ¦.[25][28]
"And God called the dry land Earth; and the gathering together of the waters called he Seas."[29]
 As in the Wessex Gospels (Matt. 28:18):
Me is geseald Ã¦lc anweald on heofonan & on eorÃ°an.[25]
"All authority in heaven and on earth has been given to me."[30]
 As in the Codex Junius's Genesis (112â16):
her Ã¦rest gesceop   ece drihten,
helm eallwihta,   heofon and eorÃ°an,
rodor arÃ¦rde   and Ã¾is rume land
gestaÃ¾elode   strangum mihtum,
frea Ã¦lmihtig.[25][31]
"Here first with mighty power the Everlasting Lord, the Helm of all created things, Almighty King, made earth and heaven, raised up the sky and founded the spacious land."[32]
 As in Ãlfric's On the Seasons of the Year (Ch. 6, Â§ 9):
Seo eorÃ°e stent on gelicnysse anre pinnhnyte, & seo sunne glit onbutan be Godes gesetnysse.[25]
"The earth can be compared to a pine cone, and the Sun glides around it by God's decree.[33]
 If Earth were shrunk to the size of a billiard ball, some areas of Earth such as large mountain ranges and oceanic trenches would feel like tiny imperfections, whereas much of the planet, including the Great Plains and the abyssal plains, would feel smoother.[97]
 Locally varies between 5 and 200 km.
 Locally varies between 5 and 70 km.
 Including the Somali Plate, which is being formed out of the African Plate. See: Chorowicz, Jean (October 2005). "The East African rift system". Journal of African Earth Sciences. 43 (1â3): 379â410. Bibcode:2005JAfES..43..379C. doi:10.1016/j.jafrearsci.2005.07.019.
 The ultimate source of these figures, uses the term "seconds of UT1" instead of "seconds of mean solar time".âAoki, S.; Kinoshita, H.; Guinot, B.; Kaplan, G. H.; McCarthy, D. D.; Seidelmann, P. K. (1982). "The new definition of universal time". Astronomy and Astrophysics. 105 (2): 359â61. Bibcode:1982A&A...105..359A.
 For Earth, the Hill radius is {\displaystyle R_{H}=a\left({\frac {m}{3M}}\right)^{\frac {1}{3}}} R_{H}=a\left({\frac {m}{3M}}\right)^{\frac {1}{3}}, where m is the mass of Earth, a is an astronomical unit, and M is the mass of the Sun. So the radius in AU is about {\displaystyle \left({\frac {1}{3\cdot 332,946}}\right)^{\frac {1}{3}}=0.01} \left({\frac {1}{3\cdot 332,946}}\right)^{\frac {1}{3}}=0.01.
 Aphelion is 103.4% of the distance to perihelion. Due to the inverse square law, the radiation at perihelion is about 106.9% the energy at aphelion.
References
 Petsko, Gregory A. (28 April 2011). "The blue marble". Genome Biology. 12 (4): 112. doi:10.1186/gb-2011-12-4-112. PMC 3218853. PMID 21554751.
 "Apollo Imagery â AS17-148-22727". NASA. 1 November 2012. Retrieved 22 October 2020.
 Simon, J.L.; Bretagnon, P.; Chapront, J.; Chapront-TouzÃ©, M.; Francou, G.; Laskar, J. (February 1994). "Numerical expressions for precession formulae and mean elements for the Moon and planets". Astronomy and Astrophysics. 282 (2): 663â83. Bibcode:1994A&A...282..663S.
 Staff (7 August 2007). "Useful Constants". International Earth Rotation and Reference Systems Service. Retrieved 23 September 2008.
 Williams, David R. (16 March 2017). "Earth Fact Sheet". NASA/Goddard Space Flight Center. Retrieved 26 July 2018.
 Allen, Clabon Walter; Cox, Arthur N. (2000). Allen's Astrophysical Quantities. Springer. p. 294. ISBN 978-0-387-98746-0. Retrieved 13 March 2011.
 Park, Ryan S.; Chamberlin, Alan B. "Solar System Dynamics". NASA.
 "UCS Satellite Database". Nuclear Weapons & Global Security. Union of Concerned Scientists. 1 April 2020. Retrieved 25 August 2020.
 Various (2000). David R. Lide (ed.). Handbook of Chemistry and Physics (81st ed.). CRC. ISBN 978-0-8493-0481-1.
 "Selected Astronomical Constants, 2011". The Astronomical Almanac. Archived from the original on 26 August 2013. Retrieved 25 February 2011.
 World Geodetic System (WGS-84). Available online from National Geospatial-Intelligence Agency.
 Cazenave, Anny (1995). "Geoid, Topography and Distribution of Landforms" (PDF). In Ahrens, Thomas J (ed.). Global Earth Physics: A Handbook of Physical Constants. Global Earth Physics: A Handbook of Physical Constants. Washington, DC: American Geophysical Union. Bibcode:1995geph.conf.....A. ISBN 978-0-87590-851-9. Archived from the original (PDF) on 16 October 2006. Retrieved 3 August 2008.
 International Earth Rotation and Reference Systems Service (IERS) Working Group (2004). "General Definitions and Numerical Standards" (PDF). In McCarthy, Dennis D.; Petit, GÃ©rard (eds.). IERS Conventions (2003) (PDF). IERS Technical Note No. 32. Frankfurt am Main: Verlag des Bundesamts fÃ¼r Kartographie und GeodÃ¤sie. p. 12. ISBN 978-3-89888-884-4. Retrieved 29 April 2016.
 Humerfelt, Sigurd (26 October 2010). "How WGS 84 defines Earth". Archived from the original on 24 April 2011. Retrieved 29 April 2011.
 Pidwirny, Michael (2 February 2006). "Surface area of our planet covered by oceans and continents.(Table 8o-1)". University of British Columbia, Okanagan. Retrieved 26 November 2007.
 Luzum, Brian; Capitaine, Nicole; Fienga, AgnÃ¨s; Folkner, William; Fukushima, Toshio; et al. (August 2011). "The IAU 2009 system of astronomical constants: The report of the IAU working group on numerical standards for Fundamental Astronomy". Celestial Mechanics and Dynamical Astronomy. 110 (4): 293â304. Bibcode:2011CeMDA.110..293L. doi:10.1007/s10569-011-9352-4.
 The international system of units (SI) (PDF) (2008 ed.). United States Department of Commerce, NIST Special Publication 330. p. 52.
 Williams, James G. (1994). "Contributions to the Earth's obliquity rate, precession, and nutation". The Astronomical Journal. 108: 711. Bibcode:1994AJ....108..711W. doi:10.1086/117108. ISSN 0004-6256.
 Allen, Clabon Walter; Cox, Arthur N. (2000). Allen's Astrophysical Quantities. Springer. p. 296. ISBN 978-0-387-98746-0. Retrieved 17 August 2010.
 Arthur N. Cox, ed. (2000). Allen's Astrophysical Quantities (4th ed.). New York: AIP Press. p. 244. ISBN 978-0-387-98746-0. Retrieved 17 August 2010.
 "World: Lowest Temperature". WMO Weather and Climate Extremes Archive. Arizona State University. Retrieved 6 September 2020.
 Kinver, Mark (10 December 2009). "Global average temperature may hit record level in 2010". BBC. Retrieved 22 April 2010.
 "World: Highest Temperature". WMO Weather and Climate Extremes Archive. Arizona State University. Retrieved 6 September 2020.
 "Trends in Atmospheric Carbon Dioxide: Recent Global CO
2 Trend". Earth System Research Laboratory. National Oceanic and Atmospheric Administration. 19 October 2020. Archived from the original on 4 October 2020.
 Oxford English Dictionary, 3rd ed. "earth, n.Â¹" Oxford University Press (Oxford), 2010.
 Beowulf. Trans. Chad Matlick in "Beowulf: Lines 1399 to 1799". West Virginia University. Retrieved 5 August 2014. (in Old English) &
 Mounce Reverse-Intralinear New Testament: "Luke 13:7". Hosted at Bible Gateway. 2014. Retrieved 5 August 2014. (in Ancient Greek) &
 Ãlfric of Eynsham. Heptateuch. Reprinted by S.J. Crawford as The Old English Version of the Heptateuch, Ãlfricâs Treatise on the Old and New Testament and his Preface to Genesis. Humphrey Milford (London), 1922. Archived 8 March 2015 at the Wayback Machine Hosted at Wordhord. Retrieved 5 August 2014. (in Old English)
 King James Version of the Bible: "Genesis 1:10". Hosted at Bible Gateway. 2014. Retrieved 5 August 2014.
 Mounce Reverse-Intralinear New Testament: "Matthew 28:18". Hosted at Bible Gateway. 2014. Retrieved 5 August 2014. (in Ancient Greek) &
 "Genesis A". Hosted at the Dept. of Linguistic Studies at the University of Padua. Retrieved 5 August 2014. (in Old English)
 Killings, Douglas. Codex Junius 11, I.ii. 1996. Hosted at Project Gutenberg. Retrieved 5 August 2014.
 Ãlfric, Abbot of Eynsham. "De temporibus annis" Trans. P. Baker as "On the Seasons of the Year Archived 30 January 2015 at the Wayback Machine". Hosted at Old English at the University of Virginia, 1998. Retrieved 6 August 2014.
 Simek, Rudolf. Trans. Angela Hall as Dictionary of Northern Mythology, p. 179. D.S. Brewer, 2007. ISBN 0-85991-513-1.
 The New Oxford Dictionary of English, 1st ed. "earth". Oxford University Press (Oxford), 1998. ISBN 0-19-861263-X.
 "Terra". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "Tellus". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "Gaia". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "Terran". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "terrestrial". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "terrene". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 "tellurian". Oxford English Dictionary (3rd ed.). Oxford University Press. September 2005. (Subscription or UK public library membership required.)
 Bouvier, Audrey; Wadhwa, Meenakshi (September 2010). "The age of the Solar System redefined by the oldest PbâPb age of a meteoritic inclusion". Nature Geoscience. 3 (9): 637â641. doi:10.1038/ngeo941.
 See:
Dalrymple, G.B. (1991). The Age of the Earth. California: Stanford University Press. ISBN 978-0-8047-1569-0.
Newman, William L. (9 July 2007). "Age of the Earth". Publications Services, USGS. Retrieved 20 September 2007.
Dalrymple, G. Brent (2001). "The age of the Earth in the twentieth century: a problem (mostly) solved". Geological Society, London, Special Publications. 190 (1): 205â21. Bibcode:2001GSLSP.190..205D. doi:10.1144/GSL.SP.2001.190.01.14. S2CID 130092094. Retrieved 20 September 2007.
 Righter, K.; Schonbachler, M. (7 May 2018). "Ag Isotopic Evolution of the Mantle During Accretion: New Constraints from Pd and Ag Metal-Silicate Partitioning". NASA: 1. Retrieved 25 October 2020.
 TartÃ¨se, Romain; Anand, Mahesh; Gattacceca, JÃ©rÃ´me; Joy, Katherine H.; Mortimer, James I.; Pernet-Fisher, John F.; Russell, Sara; Snape, Joshua F.; Weiss, Benjamin P. (2019). "Constraining the Evolutionary History of the Moon and the Inner Solar System: A Case for New Returned Lunar Samples". Space Science Reviews. 215 (8): 54. doi:10.1007/s11214-019-0622-x. ISSN 1572-9672.
 Reilly, Michael (22 October 2009). "Controversial Moon Origin Theory Rewrites History". Archived from the original on 9 January 2010. Retrieved 30 January 2010.
 Canup, R.; Asphaug, E. (2001). "Origin of the Moon in a giant impact near the end of the Earth's formation". Nature. 412 (6848): 708â12. Bibcode:2001Natur.412..708C. doi:10.1038/35089010. PMID 11507633. S2CID 4413525.
 Meier, M. M. M.; Reufer, A.; Wieler, R. (4 August 2014). "On the origin and composition of Theia: Constraints from new models of the Giant Impact" (PDF). Icarus. p. 5. Retrieved 25 October 2020.
 Claeys, Philippe; Morbidelli, Alessandro (1 January 2011). "Late Heavy Bombardment". In Gargaud, Muriel; Amils, Prof Ricardo; Quintanilla, JosÃ© Cernicharo; Cleaves II, Henderson James (Jim); Irvine, William M.; Pinti, Prof Daniele L.; Viso, Michel (eds.). Encyclopedia of Astrobiology. Springer Berlin Heidelberg. pp. 909â912. doi:10.1007/978-3-642-11274-4_869. ISBN 978-3-642-11271-3.
 "Earth's Early Atmosphere and Oceans". Lunar and Planetary Institute. Universities Space Research Association. Retrieved 27 June 2019.
 Morbidelli, A.; et al. (2000). "Source regions and time scales for the delivery of water to Earth". Meteoritics & Planetary Science. 35 (6): 1309â20. Bibcode:2000M&PS...35.1309M. doi:10.1111/j.1945-5100.2000.tb01518.x.
 Piani, Laurette; Marrocchi, Yves; Rigaudier, Thomas; Vacher, Lionel G.; Thomassin, Dorian; Marty, Bernard (2020). "Earth's water may have been inherited from material similar to enstatite chondrite meteorites". Science. 369 (6507): 1110â1113. doi:10.1126/science.aba1948. ISSN 0036-8075. PMID 32855337. S2CID 221342529.
 Guinan, E. F.; Ribas, I. (2002). Benjamin Montesinos, Alvaro Gimenez and Edward F. Guinan (ed.). Our Changing Sun: The Role of Solar Nuclear Evolution and Magnetic Activity on Earth's Atmosphere and Climate. ASP Conference Proceedings: The Evolving Sun and its Influence on Planetary Environments. San Francisco: Astronomical Society of the Pacific. Bibcode:2002ASPC..269...85G. ISBN 1-58381-109-5.
 Staff (4 March 2010). "Oldest measurement of Earth's magnetic field reveals battle between Sun and Earth for our atmosphere". Physorg.news. Retrieved 27 March 2010.
 Harrison, T.; et al. (December 2005). "Heterogeneous Hadean hafnium: evidence of continental crust at 4.4 to 4.5 ga". Science. 310 (5756): 1947â50. Bibcode:2005Sci...310.1947H. doi:10.1126/science.1117926. PMID 16293721. S2CID 11208727.
 Rogers, John James William; Santosh, M. (2004). Continents and Supercontinents. Oxford University Press US. p. 48. ISBN 978-0-19-516589-0.
 Hurley, P. M.; Rand, J. R. (June 1969). "Pre-drift continental nuclei". Science. 164 (3885): 1229â42. Bibcode:1969Sci...164.1229H. doi:10.1126/science.164.3885.1229. PMID 17772560.
 Armstrong, R. L. (1991). "The persistent myth of crustal growth" (PDF). Australian Journal of Earth Sciences. 38 (5): 613â30. Bibcode:1991AuJES..38..613A. CiteSeerX 10.1.1.527.9577. doi:10.1080/08120099108727995.
 De Smet, J.; Van Den Berg, A.P.; Vlaar, N.J. (2000). "Early formation and long-term stability of continents resulting from decompression melting in a convecting mantle" (PDF). Tectonophysics. 322 (1â2): 19â33. Bibcode:2000Tectp.322...19D. doi:10.1016/S0040-1951(00)00055-X. hdl:1874/1653.
 Dhuime, B.; Hawksworth, C.J.; Delavault, H.; Cawood, P.A. (2018). "Rates of generation and destruction of the continental crust: implications for continental growth". Philos Trans a Math Phys Eng Sci. 376 (2132). doi:10.1098/rsta.2017.0403. PMC 6189557. PMID 30275156.
 Bradley, D.C. (2011). "Secular Trends in the Geologic Record and the Supercontinent Cycle". Earth-Science Reviews. 108: 16â33. doi:10.1016/j.earscirev.2011.05.003.
 Kinzler, Ro. "When and how did the ice age end? Could another one start?". American Museum of Natural History. Retrieved 27 June 2019.
 Chalk, Thomas B.; Hain, Mathis P.; Foster, Gavin L.; Rohling, Eelco J.; Sexton, Philip F.; Badger, Marcus P. S.; Cherry, Soraya G.; Hasenfratz, Adam P.; Haug, Gerald H.; Jaccard, Samuel L.; MartÃ­nez-GarcÃ­a, Alfredo; PÃ¤like, Heiko; Pancost, Richard D.; Wilson, Paul A. (12 December 2007). "Causes of ice age intensification across the Mid-Pleistocene Transition" (PDF). Proc Natl Acad Sci U S A. 114 (50): 13114â13119. doi:10.1073/pnas.1702143114. PMC 5740680. PMID 29180424. Retrieved 28 June 2019.
 Staff. "Paleoclimatology â The Study of Ancient Climates". Page Paleontology Science Center. Archived from the original on 4 March 2007. Retrieved 2 March 2007.
 Doolittle, W. Ford; Worm, Boris (February 2000). "Uprooting the tree of life" (PDF). Scientific American. 282 (6): 90â95. Bibcode:2000SciAm.282b..90D. doi:10.1038/scientificamerican0200-90. PMID 10710791. Archived from the original (PDF) on 15 July 2011.
 Zimmer, Carl (3 October 2013). "Earth's Oxygen: A Mystery Easy to Take for Granted". The New York Times. Retrieved 3 October 2013.
 Berkner, L. V.; Marshall, L. C. (1965). "On the Origin and Rise of Oxygen Concentration in the Earth's Atmosphere". Journal of the Atmospheric Sciences. 22 (3): 225â61. Bibcode:1965JAtS...22..225B. doi:10.1175/1520-0469(1965)022<0225:OTOARO>2.0.CO;2.
 Burton, Kathleen (29 November 2002). "Astrobiologists Find Evidence of Early Life on Land". NASA. Retrieved 5 March 2007.
 Noffke, Nora; Christian, Daniel; Wacey, David; Hazen, Robert M. (8 November 2013). "Microbially Induced Sedimentary Structures Recording an Ancient Ecosystem in the ca. 3.48 Billion-Year-Old Dresser Formation, Pilbara, Western Australia". Astrobiology. 13 (12): 1103â24. Bibcode:2013AsBio..13.1103N. doi:10.1089/ast.2013.1030. PMC 3870916. PMID 24205812.
 Ohtomo, Yoko; Kakegawa, Takeshi; Ishida, Akizumi; et al. (January 2014). "Evidence for biogenic graphite in early Archaean Isua metasedimentary rocks". Nature Geoscience. 7 (1): 25â28. Bibcode:2014NatGe...7...25O. doi:10.1038/ngeo2025. ISSN 1752-0894. S2CID 54767854.
 Borenstein, Seth (19 October 2015). "Hints of life on what was thought to be desolate early Earth". Excite. Yonkers, NY: Mindspark Interactive Network. Associated Press. Retrieved 20 October 2015.
 Bell, Elizabeth A.; Boehnike, Patrick; Harrison, T. Mark; et al. (19 October 2015). "Potentially biogenic carbon preserved in a 4.1 billion-year-old zircon" (PDF). Proc. Natl. Acad. Sci. U.S.A. 112 (47): 14518â21. Bibcode:2015PNAS..11214518B. doi:10.1073/pnas.1517557112. ISSN 1091-6490. PMC 4664351. PMID 26483481. Retrieved 20 October 2015. Early edition, published online before print.
 Tyrell, Kelly April (18 December 2017). "Oldest fossils ever found show life on Earth began before 3.5 billion years ago". University of WisconsinâMadison. Retrieved 18 December 2017.
 Schopf, J. William; Kitajima, Kouki; Spicuzza, Michael J.; Kudryavtsev, Anatolly B.; Valley, John W. (2017). "SIMS analyses of the oldest known assemblage of microfossils document their taxon-correlated carbon isotope compositions". PNAS. 115 (1): 53â58. Bibcode:2018PNAS..115...53S. doi:10.1073/pnas.1718063115. PMC 5776830. PMID 29255053.
 Brooke, John L. (17 March 2014). Climate Change and the Course of Global History. Cambridge University Press. p. 42. ISBN 9780521871648.
 Cabej, Nelson R. (12 October 2019). Epigenetic Mechanisms of the Cambrian Explosion. Elsevier Science. p. 56. ISBN 9780128143124.
 Raup, D. M.; Sepkoski Jr, J. J. (1982). "Mass Extinctions in the Marine Fossil Record". Science. 215 (4539): 1501â03. Bibcode:1982Sci...215.1501R. doi:10.1126/science.215.4539.1501. PMID 17788674. S2CID 43002817.
 Stanley, S. M. (2016). "Estimates of the magnitudes of major marine mass extinctions in earth history". Proceedings of the National Academy of Sciences of the United States of America. 113 (42): E6325âE6334. doi:10.1073/pnas.1613094113. PMC 5081622. PMID 27698119. S2CID 23599425.
 Gould, Stephan J. (October 1994). "The Evolution of Life on Earth". Scientific American. 271 (4): 84â91. Bibcode:1994SciAm.271d..84G. doi:10.1038/scientificamerican1094-84. PMID 7939569. Retrieved 5 March 2007.
 Wilkinson, B. H.; McElroy, B. J. (2007). "The impact of humans on continental erosion and sedimentation". Bulletin of the Geological Society of America. 119 (1â2): 140â56. Bibcode:2007GSAB..119..140W. doi:10.1130/B25899.1. S2CID 128776283.
 Ganopolski, A.; Winkelmann, R.; Schellnhuber, H. J. (2016). "Critical insolationâCO2 relation for diagnosing past and future glacial inception". Nature. 529 (7585): 200â203. doi:10.1038/nature16494. ISSN 1476-4687. PMID 26762457. S2CID 4466220.
 Sackmann, I.-J.; Boothroyd, A. I.; Kraemer, K. E. (1993). "Our Sun. III. Present and Future". Astrophysical Journal. 418: 457â68. Bibcode:1993ApJ...418..457S. doi:10.1086/173407.
 Britt, Robert (25 February 2000). "Freeze, Fry or Dry: How Long Has the Earth Got?". Archived from the original on 5 June 2009.
 Li, King-Fai; Pahlevan, Kaveh; Kirschvink, Joseph L.; Yung, Yuk L. (2009). "Atmospheric pressure as a natural climate regulator for a terrestrial planet with a biosphere" (PDF). Proceedings of the National Academy of Sciences. 106 (24): 9576â79. Bibcode:2009PNAS..106.9576L. doi:10.1073/pnas.0809436106. PMC 2701016. PMID 19487662. Retrieved 19 July 2009.
 Ward, Peter D.; Brownlee, Donald (2002). The Life and Death of Planet Earth: How the New Science of Astrobiology Charts the Ultimate Fate of Our World. New York: Times Books, Henry Holt and Company. ISBN 978-0-8050-6781-1.
 Mello, Fernando de Sousa; FriaÃ§a, AmÃ¢ncio CÃ©sar Santos (2020). "The end of life on Earth is not the end of the world: converging to an estimate of life span of the biosphere?". International Journal of Astrobiology. 19 (1): 25â42. doi:10.1017/S1473550419000120. ISSN 1473-5504.
 Bounama, Christine; Franck, S.; Von Bloh, W. (2001). "The fate of Earth's ocean". Hydrology and Earth System Sciences. 5 (4): 569â75. Bibcode:2001HESS....5..569B. doi:10.5194/hess-5-569-2001. S2CID 14024675.
 SchrÃ¶der, K.-P.; Connon Smith, Robert (2008). "Distant future of the Sun and Earth revisited". Monthly Notices of the Royal Astronomical Society. 386 (1): 155â63. arXiv:0801.4031. Bibcode:2008MNRAS.386..155S. doi:10.1111/j.1365-2966.2008.13022.x. S2CID 10073988.
See also Palmer, Jason (22 February 2008). "Hope dims that Earth will survive Sun's death". NewScientist.com news service. Archived from the original on 15 April 2012. Retrieved 24 March 2008.
 "Tall Tales about Highest Peaks". ABC Science. 16 April 2004. Retrieved 29 May 2019.
 Milbert, D. G.; Smith, D. A. "Converting GPS Height into NAVD88 Elevation with the GEOID96 Geoid Height Model". National Geodetic Survey, NOAA. Retrieved 7 March 2007.
 Sandwell, D. T.; Smith, W. H. F. (7 July 2006). "Exploring the Ocean Basins with Satellite Altimeter Data". NOAA/NGDC. Archived from the original on 11 August 2014. Retrieved 21 April 2007.
 Senne, Joseph H. (2000). "Did Edmund Hillary Climb the Wrong Mountain". Professional Surveyor. 20 (5): 16â21.
 Sharp, David (5 March 2005). "Chimborazo and the old kilogram". The Lancet. 365 (9462): 831â32. doi:10.1016/S0140-6736(05)71021-7. PMID 15752514. S2CID 41080944.
 "The 'Highest' Spot on Earth". NPR. 7 April 2007. Retrieved 31 July 2012.
 Stewart, Heather A.; Jamieson, Alan J. (2019). "The five deeps: The location and depth of the deepest place in each of the world's oceans". Earth-Science Reviews. 197: 102896. doi:10.1016/j.earscirev.2019.102896. ISSN 0012-8252.
 "Is a Pool Ball Smoother than the Earth?" (PDF). Billiards Digest. 1 June 2013. Retrieved 26 November 2014.
 Tewksbury, Barbara. "Back-of-the-Envelope Calculations: Scale of the Himalayas". Carleton University. Retrieved 19 October 2020.
 "What is the geoid?". National Ocean Service. Retrieved 10 October 2020.
 Rudnick, R. L.; Gao, S. (2003). "Composition of the Continental Crust". In Holland, H. D.; Turekian, K. K. (eds.). Treatise on Geochemistry. Treatise on Geochemistry. 3. New York: Elsevier Science. pp. 1â64. Bibcode:2003TrGeo...3....1R. doi:10.1016/B0-08-043751-6/03016-4. ISBN 978-0-08-043751-4.
 White, W. M.; Klein, E. M. (2014). "Composition of the Oceanic Crust". In Holland, H. D.; Turekian, K. K. (eds.). Treatise on Geochemistry. 4. New York: Elsevier Science. pp. 457â496. doi:10.1016/B978-0-08-095975-7.00315-6. hdl:10161/8301. ISBN 978-0-08-098300-4.
 Morgan, J. W.; Anders, E. (1980). "Chemical composition of Earth, Venus, and Mercury". Proceedings of the National Academy of Sciences. 77 (12): 6973â77. Bibcode:1980PNAS...77.6973M. doi:10.1073/pnas.77.12.6973. PMC 350422. PMID 16592930.
 Brown, Geoff C.; Mussett, Alan E. (1981). The Inaccessible Earth (2nd ed.). Taylor & Francis. p. 166. ISBN 978-0-04-550028-4. Note: After Ronov and Yaroshevsky (1969).
 Jordan, T. H. (1979). "Structural geology of the Earth's interior". Proceedings of the National Academy of Sciences of the United States of America. 76 (9): 4192â4200. Bibcode:1979PNAS...76.4192J. doi:10.1073/pnas.76.9.4192. PMC 411539. PMID 16592703.
 Robertson, Eugene C. (26 July 2001). "The Interior of the Earth". USGS. Retrieved 24 March 2007.
 Geological Society, London. "The Crust and Lithosphere". Retrieved 25 October 2020.
 Micalizio, Caryl-Sue; Evers, Jeannie (20 May 2015). "Lithosphere". National Geographic. Retrieved 13 October 2020.
 Tanimoto, Toshiro (1995). "Crustal Structure of the Earth" (PDF). In Thomas J. Ahrens (ed.). Global Earth Physics: A Handbook of Physical Constants. Global Earth Physics: A Handbook of Physical Constants. Washington, DC: American Geophysical Union. Bibcode:1995geph.conf.....A. ISBN 978-0-87590-851-9. Archived from the original (PDF) on 16 October 2006. Retrieved 3 February 2007.
 Deuss, A. (2014). "Heterogeneity and Anisotropy of Earth's Inner Core" (PDF). Annu. Rev. Earth Planet. Sci. 42: 103â126. doi:10.1146/annurev-earth-060313-054658.
 Sanders, Robert (10 December 2003). "Radioactive potassium may be major heat source in Earth's core". UC Berkeley News. Retrieved 28 February 2007.
 "The Earth's Centre is 1000 Degrees Hotter than Previously Thought". The European Synchrotron (ESRF). 25 April 2013. Archived from the original on 28 June 2013. Retrieved 12 April 2015.
 AlfÃ¨, D.; Gillan, M. J.; Vocadlo, L.; Brodholt, J.; Price, G. D. (2002). "The ab initio simulation of the Earth's core" (PDF). Philosophical Transactions of the Royal Society. 360 (1795): 1227â44. Bibcode:2002RSPTA.360.1227A. doi:10.1098/rsta.2002.0992. PMID 12804276. S2CID 21132433. Retrieved 28 February 2007.
 Turcotte, D. L.; Schubert, G. (2002). "4". Geodynamics (2 ed.). Cambridge, England, UK: Cambridge University Press. p. 137. ISBN 978-0-521-66624-4.
 Vlaar, N; Vankeken, P.; Vandenberg, A. (1994). "Cooling of the Earth in the Archaean: Consequences of pressure-release melting in a hotter mantle" (PDF). Earth and Planetary Science Letters. 121 (1â2): 1â18. Bibcode:1994E&PSL.121....1V. doi:10.1016/0012-821X(94)90028-0. Archived from the original (PDF) on 19 March 2012.
 Pollack, Henry N.; Hurter, Suzanne J.; Johnson, Jeffrey R. (August 1993). "Heat flow from the Earth's interior: Analysis of the global data set". Reviews of Geophysics. 31 (3): 267â80. Bibcode:1993RvGeo..31..267P. doi:10.1029/93RG01249.
 Richards, M. A.; Duncan, R. A.; Courtillot, V. E. (1989). "Flood Basalts and Hot-Spot Tracks: Plume Heads and Tails". Science. 246 (4926): 103â07. Bibcode:1989Sci...246..103R. doi:10.1126/science.246.4926.103. PMID 17837768. S2CID 9147772.
 Sclater, John G; Parsons, Barry; Jaupart, Claude (1981). "Oceans and Continents: Similarities and Differences in the Mechanisms of Heat Loss". Journal of Geophysical Research. 86 (B12): 11535. Bibcode:1981JGR....8611535S. doi:10.1029/JB086iB12p11535.
 Brown, W. K.; Wohletz, K. H. (2005). "SFT and the Earth's Tectonic Plates". Los Alamos National Laboratory. Retrieved 2 March 2007.
 Kious, W. J.; Tilling, R. I. (5 May 1999). "Understanding plate motions". USGS. Retrieved 2 March 2007.
 Seligman, Courtney (2008). "The Structure of the Terrestrial Planets". Online Astronomy eText Table of Contents. cseligman.com. Retrieved 28 February 2008.
 Duennebier, Fred (12 August 1999). "Pacific Plate Motion". University of Hawaii. Retrieved 14 March 2007.
 Mueller, R. D.; et al. (7 March 2007). "Age of the Ocean Floor Poster". NOAA. Retrieved 14 March 2007.
 Bowring, Samuel A.; Williams, Ian S. (1999). "Priscoan (4.00â4.03 Ga) orthogneisses from northwestern Canada". Contributions to Mineralogy and Petrology. 134 (1): 3â16. Bibcode:1999CoMP..134....3B. doi:10.1007/s004100050465. S2CID 128376754.
 Meschede, Martin; Barckhausen, Udo (20 November 2000). "Plate Tectonic Evolution of the Cocos-Nazca Spreading Center". Proceedings of the Ocean Drilling Program. Texas A&M University. Retrieved 2 April 2007.
 Argus, D.F.; Gordon, R.G.; DeMets, C. (2011). "Geologically current motion of 56 plates relative to the noânetârotation reference frame". Geochemistry, Geophysics, Geosystems. 12 (11): n/a. doi:10.1029/2011GC003751.
 "World Factbook". Cia.gov. Retrieved 2 November 2012.
 Center, National Geophysical Data. "Hypsographic Curve of Earth's Surface from ETOPO1". ngdc.noaa.gov.
 Staff. "Layers of the Earth". Volcano World. Archived from the original on 11 February 2013. Retrieved 11 March 2007.
 Jessey, David. "Weathering and Sedimentary Rocks". Cal Poly Pomona. Archived from the original on 3 July 2007. Retrieved 20 March 2007.
 de Pater, Imke; Lissauer, Jack J. (2010). Planetary Sciences (2nd ed.). Cambridge University Press. p. 154. ISBN 978-0-521-85371-2.
 Wenk, Hans-Rudolf; Bulakh, AndreÄ­ Glebovich (2004). Minerals: their constitution and origin. Cambridge University Press. p. 359. ISBN 978-0-521-52958-7.
 Kring, David A. "Terrestrial Impact Cratering and Its Environmental Effects". Lunar and Planetary Laboratory. Retrieved 22 March 2007.
 Martin, Ronald (2011). Earth's Evolving Systems: The History of Planet Earth. Jones & Bartlett Learning. ISBN 978-0-7637-8001-2.
 "World Bank arable land". World Bank. Retrieved 19 October 2015.
 "World Bank permanent cropland". World Bank. Retrieved 19 October 2015.
 Hooke, Roger LeB.; MartÃ­n-Duque, JosÃ© F.; Pedraza, Javier (December 2012). "Land transformation by humans: A review" (PDF). GSA Today. 22 (12): 4â10. doi:10.1130/GSAT151A.1.
 Watts, A. B.; Daly, S. F. (May 1981). "Long wavelength gravity and topography anomalies". Annual Review of Earth and Planetary Sciences. 9: 415â18. Bibcode:1981AREPS...9..415W. doi:10.1146/annurev.ea.09.050181.002215.
 Olson, Peter; Amit, Hagay (2006), "Changes in earth's dipole" (PDF), Naturwissenschaften, 93 (11): 519â542, Bibcode:2006NW.....93..519O, doi:10.1007/s00114-006-0138-6, PMID 16915369, S2CID 22283432
 Fitzpatrick, Richard (16 February 2006). "MHD dynamo theory". NASA WMAP. Retrieved 27 February 2007.
 Campbell, Wallace Hall (2003). Introduction to Geomagnetic Fields. New York: Cambridge University Press. p. 57. ISBN 978-0-521-82206-0.
 Ganushkina, N. Yu; Liemohn, M. W.; Dubyagin, S. (2018). "Current Systems in the Earth's Magnetosphere". Reviews of Geophysics. 56 (2): 309â332. doi:10.1002/2017RG000590. hdl:2027.42/145256. ISSN 1944-9208.
 Masson, Arnaud (11 May 2007). "Cluster reveals the reformation of the Earth's bow shock". European Space Agency. Retrieved 16 August 2016.
 Gallagher, Dennis L. (14 August 2015). "The Earth's Plasmasphere". NASA/Marshall Space Flight Center. Retrieved 16 August 2016.
 Gallagher, Dennis L. (27 May 2015). "How the Plasmasphere is Formed". NASA/Marshall Space Flight Center. Retrieved 16 August 2016.
 Baumjohann, Wolfgang; Treumann, Rudolf A. (1997). Basic Space Plasma Physics. World Scientific. pp. 8, 31. ISBN 978-1-86094-079-8.
 McElroy, Michael B. (2012). "Ionosphere and magnetosphere". EncyclopÃ¦dia Britannica. EncyclopÃ¦dia Britannica, Inc.
 Van Allen, James Alfred (2004). Origins of Magnetospheric Physics. University of Iowa Press. ISBN 978-0-87745-921-7. OCLC 646887856.
 Stern, David P. (8 July 2005). "Exploration of the Earth's Magnetosphere". NASA. Retrieved 21 March 2007.
 McCarthy, Dennis D.; Hackman, Christine; Nelson, Robert A. (November 2008). "The Physical Basis of the Leap Second" (PDF). The Astronomical Journal. 136 (5): 1906â08. Bibcode:2008AJ....136.1906M. doi:10.1088/0004-6256/136/5/1906.
 "Leap seconds". Time Service Department, USNO. Archived from the original on 12 March 2015. Retrieved 23 September 2008.
 "Rapid Service/Prediction of Earth Orientation". IERS Bulletin-A. 28 (15). 9 April 2015. Archived from the original (.DAT file (displays as plaintext in browser)) on 14 March 2015. Retrieved 12 April 2015.
 Seidelmann, P. Kenneth (1992). Explanatory Supplement to the Astronomical Almanac. Mill Valley, CA: University Science Books. p. 48. ISBN 978-0-935702-68-2.
 Zeilik, M.; Gregory, S. A. (1998). Introductory Astronomy & Astrophysics (4th ed.). Saunders College Publishing. p. 56. ISBN 978-0-03-006228-5.
 Williams, David R. (10 February 2006). "Planetary Fact Sheets". NASA. Retrieved 28 September 2008.âSee the apparent diameters on the Sun and Moon pages.
 Staff (12 February 2020). "Pale Blue Dot Revisited". NASA. Retrieved 12 February 2020.
 Williams, David R. (1 September 2004). "Moon Fact Sheet". NASA. Retrieved 21 March 2007.
 VÃ¡zquez, M.; RodrÃ­guez, P. MontaÃ±Ã©s; Palle, E. (2006). "The Earth as an Object of Astrophysical Interest in the Search for Extrasolar Planets" (PDF). Lecture Notes and Essays in Astrophysics. 2: 49. Bibcode:2006LNEA....2...49V. Archived from the original (PDF) on 22 August 2011. Retrieved 21 March 2007.
 Astrophysicist team (1 December 2005). "Earth's location in the Milky Way". NASA. Archived from the original on 1 July 2008. Retrieved 11 June 2008.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. pp. 291â292. ISBN 9781284126563.
 Burn, Chris (March 1996). The Polar Night (PDF). The Aurora Research Institute. Retrieved 28 September 2015.
 "Sunlight Hours". Australian Antarctic Programme. 24 June 2020. Retrieved 13 October 2020.
 Bromberg, Irv (1 May 2008). "The Lengths of the Seasons (on Earth)". University of Toronto. Archived from the original on 18 December 2008. Retrieved 8 November 2008.
 Lin, Haosheng (2006). "Animation of precession of moon orbit". Survey of Astronomy AST110-6. University of Hawaii at Manoa. Retrieved 10 September 2010.
 Fisher, Rick (5 February 1996). "Earth Rotation and Equatorial Coordinates". National Radio Astronomy Observatory. Retrieved 21 March 2007.
 Buis, Alan (27 February 2020). "Milankovitch (Orbital) Cycles and Their Role in Earth's Climate". NASA. Retrieved 27 October 2020.
 Kang, Sarah M.; Seager, Richard. "Croll Revisited: Why is the Northern Hemisphere Warmer than the Southern Hemisphere?" (PDF). Columbia University. New York City. Retrieved 27 October 2020.
 Klemetti, Erik (17 June 2019). "What's so special about our Moon, anyway?". Astronomy. Retrieved 13 October 2020.
 "Charon". NASA. 19 December 2019. Retrieved 13 October 2020.
 Brown, Toby (2 December 2019). "Curious Kids: Why is the moon called the moon?". The Conversation. Retrieved 13 October 2020.
 Coughenour, Christopher L.; Archer, Allen W.; Lacovara, Kenneth J. (2009). "Tides, tidalites, and secular changes in the EarthâMoon system". Earth-Science Reviews. 97 (1): 59â79. doi:10.1016/j.earscirev.2009.09.002. ISSN 0012-8252.
 Kelley, Peter (17 August 2017). "Tidally locked exoplanets may be more common than previously thought". University of Washington News. Retrieved 8 October 2020.
 "Lunar Phases and Eclipses | Earth's Moon". NASA Solar System Exploration. Retrieved 8 October 2020.
 Espenak, F.; Meeus, J. (7 February 2007). "Secular acceleration of the Moon". NASA. Archived from the original on 2 March 2008. Retrieved 20 April 2007.
 Williams, G.E. (2000). "Geological constraints on the Precambrian history of Earth's rotation and the Moon's orbit". Reviews of Geophysics. 38 (1): 37â59. doi:10.1029/1999RG900016.
 Laskar, J.; et al. (2004). "A long-term numerical solution for the insolation quantities of the Earth". Astronomy and Astrophysics. 428 (1): 261â85. Bibcode:2004A&A...428..261L. doi:10.1051/0004-6361:20041335.
 Cooper, Keith (27 January 2015). "Earth's moon may not be critical to life". Phys.org. Retrieved 26 October 2020.
 Dadarich, Amy; Mitrovica, Jerry X.; Matsuyama, Isamu; Perron, J. Taylor; Manga, Michael; Richards, Mark A. (22 November 2007). "Equilibrium rotational stability and figure of Mars" (PDF). Icarus. doi:10.1016/j.icarus.2007.10.017. Retrieved 26 October 2020.
 Sharf, Caleb A. (18 May 2012). "The Solar Eclipse Coincidence". Scientific American. Retrieved 13 October 2020.
 Christou, Apostolos A.; Asher, David J. (31 March 2011). "A long-lived horseshoe companion to the Earth". Monthly Notices of the Royal Astronomical Society. 414 (4): 2965â2969. arXiv:1104.0036. Bibcode:2011MNRAS.414.2965C. doi:10.1111/j.1365-2966.2011.18595.x. S2CID 13832179. See table 2, p. 5.
 Marcos, C. de la Fuente; Marcos, R. de la Fuente (8 August 2016). "Asteroid (469219) 2016 HO3, the smallest and closest Earth quasi-satellite". Monthly Notices of the Royal Astronomical Society. 462 (4): 3441-3456. doi:10.1093/mnras/stw1972. Retrieved 28 October 2020.
 Connors, Martin; Wiegert, Paul; Veillet, Christian (27 July 2011). "Earth's Trojan asteroid". Nature. 475 (7357): 481â83. Bibcode:2011Natur.475..481C. doi:10.1038/nature10233. PMID 21796207. S2CID 205225571.
 Choi, Charles Q. (27 July 2011). "First Asteroid Companion of Earth Discovered at Last". Space.com. Retrieved 27 July 2011.
 "2006 RH120 ( = 6R10DB9) (A second moon for the Earth?)". Great Shefford Observatory. Great Shefford Observatory. Archived from the original on 6 February 2015. Retrieved 17 July 2015.
 Welch, Rosanne; Lamphier, Peg A. (22 February 2019). Technical Innovation in American History: An Encyclopedia of Science and Technology [3 volumes]. ABC-CLIO. p. 126. ISBN 978-1-61069-094-2.
 Charette, Matthew A.; Smith, Walter H. F. (June 2010). "The Volume of Earth's Ocean" (PDF). Oceanography. 23 (2): 112â14. doi:10.5670/oceanog.2010.51. Archived from the original (PDF) on 2 November 2013. Retrieved 6 June 2013.
 "Third rock from the Sun â restless Earth". NASA's Cosmos. Retrieved 12 April 2015.
 Perlman, Howard (17 March 2014). "The World's Water". USGS Water-Science School. Retrieved 12 April 2015.
 Hendrix, Mark (2019). Earth Science: An Introduction. Bosten: Cengage. p. 330. ISBN 978-0-357-11656-2.
 Hendrix, Mark (2019). Earth Science: An Introduction. Bosten: Cengage. p. 329. ISBN 978-0-357-11656-2.
 Kennish, Michael J. (2001). Practical handbook of marine science. Marine science series (3rd ed.). CRC Press. p. 35. ISBN 978-0-8493-2391-1.
 Mullen, Leslie (11 June 2002). "Salt of the Early Earth". NASA Astrobiology Magazine. Archived from the original on 30 June 2007. Retrieved 14 March 2007.
 Morris, Ron M. "Oceanic Processes". NASA Astrobiology Magazine. Archived from the original on 15 April 2009. Retrieved 14 March 2007.
 Scott, Michon (24 April 2006). "Earth's Big heat Bucket". NASA Earth Observatory. Retrieved 14 March 2007.
 Sample, Sharron (21 June 2005). "Sea Surface Temperature". NASA. Archived from the original on 27 April 2013. Retrieved 21 April 2007.
 Exline, Joseph D.; Levine, Arlene S.; Levine, Joel S. (2006). Meteorology: An Educator's Resource for Inquiry-Based Learning for Grades 5-9 (PDF). NASA/Langley Research Center. p. 6. NP-2006-08-97-LaRC.
 Geerts, B.; Linacre, E. (November 1997). "The height of the tropopause". Resources in Atmospheric Sciences. University of Wyoming. Retrieved 10 August 2006.
 Harrison, Roy M.; Hester, Ronald E. (2002). Causes and Environmental Implications of Increased UV-B Radiation. Royal Society of Chemistry. ISBN 978-0-85404-265-4.
 Staff (8 October 2003). "Earth's Atmosphere". NASA. Retrieved 21 March 2007.
 Pidwirny, Michael (2006). "Fundamentals of Physical Geography (2nd Edition)". University of British Columbia, Okanagan. Retrieved 19 March 2007.
 Gaan, Narottam (2008). Climate Change and International Politics. Kalpaz Publications. p. 40. ISBN 978-81-7835-641-9.
 Moran, Joseph M. (2005). "Weather". World Book Online Reference Center. NASA/World Book, Inc. Archived from the original on 13 December 2010. Retrieved 17 March 2007.
 Berger, Wolfgang H. (2002). "The Earth's Climate System". University of California, San Diego. Retrieved 24 March 2007.
 Rahmstorf, Stefan (2003). "The Thermohaline Ocean Circulation". Potsdam Institute for Climate Impact Research. Retrieved 21 April 2007.
 Sadava, David E.; Heller, H. Craig; Orians, Gordon H. (2006). Life, the Science of Biology (8th ed.). MacMillan. p. 1114. ISBN 978-0-7167-7671-0.
 Staff. "Climate Zones". UK Department for Environment, Food and Rural Affairs. Archived from the original on 8 August 2010. Retrieved 24 March 2007.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. p. 49. ISBN 9781284126563.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. p. 32. ISBN 9781284126563.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. p. 34. ISBN 9781284126563.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. p. 46. ISBN 9781284126563.
 Various (21 July 1997). "The Hydrologic Cycle". University of Illinois. Retrieved 24 March 2007.
 Rohli, Robert. V.; Vega, Anthony J. (2018). Climatology (fourth ed.). Jones & Bartlett Learning. p. 159. ISBN 9781284126563.
 El Fadli, Khalid I.; Cerveny, Randall S.; Burt, Christopher C.; Eden, Philip; Parker, David; Brunet, Manola; Peterson, Thomas C.; Mordacchini, Gianpaolo; Pelino, Vinicio; Bessemoulin, Pierre; Stella, JosÃ© Luis (2013). "World Meteorological Organization Assessment of the Purported World Record 58Â°C Temperature Extreme at El Azizia, Libya (13 September 1922)". Bulletin of the American Meteorological Society. 94 (2): 199â204. doi:10.1175/BAMS-D-12-00093.1. ISSN 0003-0007.
 Turner, John; Anderson, Phil; LachlanâCope, Tom; Colwell, Steve; Phillips, Tony; Kirchgaessner, AmÃ©lie; Marshall, Gareth J.; King, John C.; Bracegirdle, Tom; Vaughan, David G.; Lagun, Victor (2009). "Record low surface air temperature at Vostok station, Antarctica". Journal of Geophysical Research: Atmospheres. 114 (D24). doi:10.1029/2009JD012104. ISSN 2156-2202.
 Staff (2004). "Stratosphere and Weather; Discovery of the Stratosphere". Science Week. Archived from the original on 13 July 2007. Retrieved 14 March 2007.
 de CÃ³rdoba, S. Sanz FernÃ¡ndez (21 June 2004). "Presentation of the Karman separation line, used as the boundary separating Aeronautics and Astronautics". FÃ©dÃ©ration AÃ©ronautique Internationale. Archived from the original on 15 January 2010. Retrieved 21 April 2007.
 Liu, S. C.; Donahue, T. M. (1974). "The Aeronomy of Hydrogen in the Atmosphere of the Earth". Journal of the Atmospheric Sciences. 31 (4): 1118â36. Bibcode:1974JAtS...31.1118L. doi:10.1175/1520-0469(1974)031<1118:TAOHIT>2.0.CO;2.
 Catling, David C.; Zahnle, Kevin J.; McKay, Christopher P. (2001). "Biogenic Methane, Hydrogen Escape, and the Irreversible Oxidation of Early Earth". Science. 293 (5531): 839â43. Bibcode:2001Sci...293..839C. CiteSeerX 10.1.1.562.2763. doi:10.1126/science.1061976. PMID 11486082. S2CID 37386726.
 Abedon, Stephen T. (31 March 1997). "History of Earth". Ohio State University. Archived from the original on 29 November 2012. Retrieved 19 March 2007.
 Hunten, D. M.; Donahue, T. M (1976). "Hydrogen loss from the terrestrial planets". Annual Review of Earth and Planetary Sciences. 4 (1): 265â92. Bibcode:1976AREPS...4..265H. doi:10.1146/annurev.ea.04.050176.001405.
 Staff (September 2003). "Astrobiology Roadmap". NASA, Lockheed Martin. Archived from the original on 12 March 2012. Retrieved 10 March 2007.
 Dole, Stephen H. (1970). Habitable Planets for Man (2nd ed.). American Elsevier Publishing Co. ISBN 978-0-444-00092-7. Retrieved 11 March 2007.
 "What is the biosphere?". Biodiversidad Mexicana. Gobierno de MÃ©xico. Retrieved 28 June 2019.
 "Interdependency between animal and plant species". BBC Bitesize. BBC. p. 3. Retrieved 28 June 2019.
 Hillebrand, Helmut (2004). "On the Generality of the Latitudinal Gradient" (PDF). American Naturalist. 163 (2): 192â211. doi:10.1086/381004. PMID 14970922. S2CID 9886026.
 Sweetlove, L. (24 August 2011). "Number of species on Earth tagged at 8.7 million". Nature. Retrieved 28 October 2020.
 Smith, Sharon; Fleming, Lora; Solo-Gabriele, Helena; Gerwick, William H. (2 September 2011). Oceans and Human Health. Elsevier Science. p. 212. ISBN 9780080877822.
 Alexander, David (30 September 1993). Natural Disasters. Springer Science & Business Media. p. 3. ISBN 9781317938811.
 Goudie, Andrew (2000). The Human Impact on the Natural Environment. MIT Press. pp. 52, 66, 69, 137, 142, 185, 202, 355, 366. ISBN 9780262571388.
 Cook, John; Oreskes, Naomi; Doran, Peter T.; Anderegg, William R. L.; Verheggen, Bart; Maibach, Ed W.; Carlton, J. Stuart; Lewandowsky, Stephan; Skuce, Andrew G.; Green, Sarah A.; Nuccitelli, Dana (2016). "Consensus on consensus: a synthesis of consensus estimates on human-caused global warming". Environmental Research Letters. 11 (4): 048002. doi:10.1088/1748-9326/11/4/048002. ISSN 1748-9326.
 "Global Warming Effects". National Geographic. 14 January 2019. Retrieved 16 September 2020.
 World at the Xpeditions Atlas, National Geographic Society, Washington D.C., 2006.
 Gomez, Jim; Sullivan, Tim. "Various '7 billionth' babies celebrated worldwide". Yahoo News. Archived from the original on 31 October 2011. Retrieved 31 October 2011.
 Harvey, Fiona (15 July 2020). "World population in 2100 could be 2 billion below UN forecasts, study suggests". The Guardian. ISSN 0261-3077. Retrieved 18 September 2020.
 Ritchie, H.; Roser, M. (2019). "What share of people will live in urban areas in the future?". Our World in Data. Retrieved 26 October 2020.
 Abel Mendez (6 July 2011). "Distribution of landmasses of the Paleo-Earth". University of Puerto Rico at Arecibo. Retrieved 5 January 2019.
 Lutz, Ashley (4 May 2012). "MAP OF THE DAY: Pretty Much Everyone Lives In The Northern Hemisphere". Business Insider. Retrieved 5 January 2019.
 Peel, M. C.; Finlayson, B. L.; McMahon, T. A. (2007). "Updated world map of the KÃ¶ppen-Geiger climate classification" (PDF). Hydrology and Earth System Sciences Discussions. 4 (2): 439â73. doi:10.5194/hessd-4-439-2007.
 Staff. "Themes & Issues". Secretariat of the Convention on Biological Diversity. Archived from the original on 7 April 2007. Retrieved 29 March 2007.
 Smith, Courtney B. (2006). Politics and Process at the United Nations: The Global Dance (PDF). Lynne Reiner. p. 1-4. ISBN 1-58826-323-1.
 Lloyd, John; Mitchinson, John (2010). The Discretely Plumper Second QI Book of General Ignorance. Faber & Faber. p. 116-117. ISBN 9780571290727.
 Staff (15 August 2006). "Canadian Forces Station (CFS) Alert". Information Management Group. Retrieved 31 March 2007.
 "Amundsen-Scott South Pole Station". National Science Foundation. Retrieved 23 October 2020.
 Kuhn, Betsy (2006). The race for space: the United States and the Soviet Union compete for the new frontier. Twenty-First Century Books. p. 34. ISBN 978-0-8225-5984-9.
 Shayler, David; Vis, Bert (2005). Russia's Cosmonauts: Inside the Yuri Gagarin Training Center. BirkhÃ¤user. ISBN 978-0-387-21894-6.
 Holmes, Oliver (19 November 2018). "Space: how far have we gone â and where are we going?". The Guardian. ISSN 0261-3077. Retrieved 10 October 2020.
 "Reference Guide to the International Space Station". NASA. 16 January 2007. Retrieved 23 December 2008.
 "Apollo 13 The Seventh Mission: The Third Lunar Landing Attempt 11 Aprilâ17 April 1970". NASA. Retrieved 7 November 2015.
 IPCC (2019). "Summary for Policymakers" (PDF). IPCC Special Report on Climate Change and Land. p. 8.
 "What are the consequences of the overexploitation of natural resources?". Iberdrola. Retrieved 28 June 2019.
 "13. Exploitation of Natural Resources". European Environment Agency. European Union. 20 April 2016. Retrieved 28 June 2019.
 Huebsch, Russell (29 September 2017). "How Are Fossil Fuels Extracted From the Ground?". Sciencing. Leaf Group Media. Retrieved 28 June 2019.
 "Electricity generation â what are the options?". World Nuclear Association. Retrieved 28 June 2019.
 Brimhall, George (May 1991). "The Genesis of Ores". Scientific American. Nature America. 264 (5): 84â91. doi:10.1038/scientificamerican0591-84. JSTOR 24936905. Retrieved 13 October 2020.
 Lunine, Jonathan I. (2013). Earth: Evolution of a Habitable World (second ed.). Cambridge University Press. pp. 292â294. ISBN 978-0-521-61519-8.
 Rona, Peter A. (2003). "Resources of the Sea Floor". Science. 299 (5607): 673â74. doi:10.1126/science.1080679. PMID 12560541. S2CID 129262186.
 Ritchie, H.; Roser, M. (2019). "Land Use". Our World in Data. Retrieved 26 October 2020.
 Tate, Nikki; Tate-Stratton, Dani (1 October 2014). Take Shelter: At Home Around the World. Orca Book Publishers. p. 6. ISBN 978-1459807426.
 Widmer, Ted (24 December 2018). "What Did Plato Think the Earth Looked Like? - For millenniums, humans have tried to imagine the world in space. Fifty years ago, we finally saw it". The New York Times. Retrieved 25 December 2018.
 Liungman, Carl G. (2004). "Group 29: Multi-axes symmetric, both soft and straight-lined, closed signs with crossing lines". Symbols â Encyclopedia of Western Signs and Ideograms. New York: Ionfox AB. pp. 281â82. ISBN 978-91-972705-0-2.
 Stookey, Lorena Laura (2004). Thematic Guide to World Mythology. Westport, Conn.: Greenwood Press. pp. 114â15. ISBN 978-0-313-31505-3.
 Lovelock, James. The Vanishing Face of Gaia. Basic Books, 2009, p. 255. ISBN 978-0-465-01549-8
 Lovelock, J.E. (1972). "Gaia as seen through the atmosphere". Atmospheric Environment. 6 (8): 579â80. Bibcode:1972AtmEn...6..579L. doi:10.1016/0004-6981(72)90076-5. ISSN 1352-2310.
 Lovelock, J.E.; Margulis, L. (1974). "Atmospheric homeostasis by and for the biosphere: the Gaia hypothesis". Tellus. Series A. 26 (1â2): 2â10. Bibcode:1974Tell...26....2L. doi:10.1111/j.2153-3490.1974.tb01946.x. ISSN 1600-0870.
 Overbye, Dennis (21 December 2018). "Apollo 8's Earthrise: The Shot Seen Round the World â Half a century ago today, a photograph from the moon helped humans rediscover Earth". The New York Times. Retrieved 24 December 2018.
 Boulton, Matthew Myer; Heithaus, Joseph (24 December 2018). "We Are All Riders on the Same Planet â Seen from space 50 years ago, Earth appeared as a gift to preserve and cherish. What happened?". The New York Times. Retrieved 25 December 2018.
 Russell, Jeffrey B. "The Myth of the Flat Earth". American Scientific Affiliation. Retrieved 14 March 2007..
 Burkert, Walter (1 June 1972). Lore and Science in Ancient Pythagoreanism. Cambridge, Massachusetts: Harvard University Press. pp. 306â308. ISBN 978-0-674-53918-1.
 Kahn, Charles H. (2001). Pythagoras and the Pythagoreans: A Brief History. Indianapolis, Indiana and Cambridge, England: Hackett Publishing Company. p. 53. ISBN 978-0-87220-575-8.
 Dicks, D. R. (1970). Early Greek Astronomy to Aristotle. Ithaca, New York: Cornell University Press. p. 68. ISBN 978-0-8014-0561-7.
 Arnett, Bill (16 July 2006). "Earth". The Nine Planets, A Multimedia Tour of the Solar System: one star, eight planets, and more. Retrieved 9 March 2010.
 Monroe, James; Wicander, Reed; Hazlett, Richard (2007). Physical Geology: Exploring the Earth. Thomson Brooks/Cole. pp. 263â65. ISBN 978-0-495-01148-4.
 Henshaw, John M. (2014). An Equation for Every Occasion: Fifty-Two Formulas and Why They Matter. Johns Hopkins University Press. pp. 117â18. ISBN 978-1-4214-1491-1.
 Burchfield, Joe D. (1990). Lord Kelvin and the Age of the Earth. University of Chicago Press. pp. 13â18. ISBN 978-0-226-08043-7.
Further reading
Ashley Strickland (22 November 2019). "This is one place on Earth where no life can exist". CNN.
External links
Listen to this article (4 parts) Â· (info)
MENU0:00
MENU0:00
Spoken Wikipedia icon
This audio file was created from a revision of this article dated 2012-06-13, and does not reflect subsequent edits.
(Audio helpMore spoken articles)
Earth â Profile â Solar System Exploration â NASA
Earth Observatory â NASA
Earth â Videos â International Space Station:
Video (01:02) â Earth (time-lapse)
Video (00:27) â Earth and auroras (time-lapse)
Google Earth 3D, interactive map
Interactive 3D visualisation of the Sun, Earth and Moon system
GPlates Portal (University of Sydney)
Find out more on
Wikipedia's
Sister projects
Media
from CommonsNews stories
from WikinewsDefinitions
from WiktionaryTextbooks
from WikibooksQuotations
from WikiquoteSource texts
from WikisourceLearning resources
from Wikiversity
vte
Earth
Other articles related to Earth
Issoria lathonia.jpgBiology portalThe Earth seen from Apollo 17 with transparent background.pngEarth sciences portalEarth Day Flag.pngEcology portalTerra.pngGeography portalSpaccato vulcano.svgVolcanoes portalSolar system.jpgSolar System portalCumulus clouds in fair weather.jpegWeather portal
Authority control Edit this at Wikidata
BNF: cb11975911n (data)GND: 1135962553LCCN: sh85040427NARA: 10637787NDL: 00573040NKC: ph117167NLI: 003982849TDVÄ°A: yerVIAF: 6270149919445006650001WorldCat Identities: viaf-6270149919445006650001
Categories: EarthAstronomical objects known since antiquityGlobal natural environmentPlanets in the circumstellar habitable zoneNaturePlanets of the Solar SystemTerrestrial planets
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadView sourceView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons
Wikiquote
Wikiversity

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tagalog
Tiáº¿ng Viá»t
ä¸­æ
253 more
Edit links
This page was last edited on 30 October 2020, at 18:14 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Page semi-protected
Age of Earth
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
See also: History of Earth

The Blue Marble, Earth as seen in 1972 from Apollo 17
The age of Earth is estimated to be 4.54 Â± 0.05 billion years (4.54 Ã 109 years Â± 1%).[1][2][3][4] This age may represent the age of the Earth's accretion, or core formation, or of the material from which the Earth formed.[2] This dating is based on evidence from radiometric age-dating of meteorite[5] material and is consistent with the radiometric ages of the oldest-known terrestrial and lunar samples.

Following the development of radiometric age-dating in the early 20th century, measurements of lead in uranium-rich minerals showed that some were in excess of a billion years old.[6] The oldest such minerals analyzed to dateâsmall crystals of zircon from the Jack Hills of Western Australiaâare at least 4.404 billion years old.[7][8][9] Calciumâaluminium-rich inclusionsâthe oldest known solid constituents within meteorites that are formed within the Solar Systemâare 4.567 billion years old,[10][11] giving a lower limit for the age of the Solar System.

It is hypothesised that the accretion of Earth began soon after the formation of the calcium-aluminium-rich inclusions and the meteorites. Because the time this accretion process took is not yet known, and predictions from different accretion models range from a few million up to about 100 million years, the difference between the age of Earth and of the oldest rocks is difficult to determine. It is also difficult to determine the exact age of the oldest rocks on Earth, exposed at the surface, as they are aggregates of minerals of possibly different ages.


Contents
1	Development of modern geologic concepts
2	Early calculations
3	Radiometric dating
3.1	Overview
3.2	Convective mantle and radioactivity
3.3	Invention of radiometric dating
3.4	Arthur Holmes establishes radiometric dating
3.5	Modern radiometric dating
3.5.1	Why meteorites were used
3.5.2	Canyon Diablo meteorite
4	See also
5	References
6	Bibliography
7	Further reading
8	External links
Development of modern geologic concepts
Life timeline
This box: viewtalkedit
-4500 ââ-4000 ââ-3500 ââ-3000 ââ-2500 ââ-2000 ââ-1500 ââ-1000 ââ-500 ââ0 â
Water
Single-celled
life
Photosynthesis
Eukaryotes
Multicellular
life
Arthropods Molluscs
Plants
Dinosaurs
Mammals
Flowers
Birds
Primates











â
Earth (â4540)
â
Earliest water
â
Earliest life
â
LHB meteorites
â
Earliest oxygen
â
Atmospheric oxygen
â
Oxygen crisis
â
Earliest fungi
â
Sexual reproduction
â
Earliest plants
â
Earliest animals
â
Ediacaran
â
Cambrian
â
Tetrapoda
â
Earliest apes
P
h
a
n
e
r
o
z
o
i
c







P
r
o
t
e
r
o
z
o
i
c



A
r
c
h
e
a
n
H
a
d
e
a
n
Pongola
Huronian
Cryogenian
Andean
Karoo
Quaternary
Ice Ages
(million years ago)
Main article: History of geology
Further information: Relative dating
Studies of strataâthe layering of rocks and earthâgave naturalists an appreciation that Earth may have been through many changes during its existence. These layers often contained fossilized remains of unknown creatures, leading some to interpret a progression of organisms from layer to layer.[12][13]

Nicolas Steno in the 17th century was one of the first naturalists to appreciate the connection between fossil remains and strata.[13] His observations led him to formulate important stratigraphic concepts (i.e., the "law of superposition" and the "principle of original horizontality").[14] In the 1790s, William Smith hypothesized that if two layers of rock at widely differing locations contained similar fossils, then it was very plausible that the layers were the same age.[15] Smith's nephew and student, John Phillips, later calculated by such means that Earth was about 96 million years old.[16]

In the mid-18th century, the naturalist Mikhail Lomonosov suggested that Earth had been created separately from, and several hundred thousand years before, the rest of the universe. Lomonosov's ideas were mostly speculative. In 1779 the Comte du Buffon tried to obtain a value for the age of Earth using an experiment: He created a small globe that resembled Earth in composition and then measured its rate of cooling. This led him to estimate that Earth was about 75,000 years old.

Other naturalists used these hypotheses to construct a history of Earth, though their timelines were inexact as they did not know how long it took to lay down stratigraphic layers.[14] In 1830, geologist Charles Lyell, developing ideas found in James Hutton's works, popularized the concept that the features of Earth were in perpetual change, eroding and reforming continuously, and the rate of this change was roughly constant. This was a challenge to the traditional view, which saw the history of Earth as dominated by intermittent catastrophes. Many naturalists were influenced by Lyell to become "uniformitarians" who believed that changes were constant and uniform.[citation needed]

Early calculations
Further information: William Thomson, 1st Baron Kelvin Â§ Age of the Earth: geology
In 1862, the physicist William Thomson, 1st Baron Kelvin published calculations that fixed the age of Earth at between 20 million and 400 million years.[17][18] He assumed that Earth had formed as a completely molten object, and determined the amount of time it would take for the near-surface temperature gradient to decrease to its present value. His calculations did not account for heat produced via radioactive decay (a then unknown process) or, more significantly, convection inside the Earth, which allows the temperature in the upper mantle to remain high much longer, maintaining a high thermal gradient in the crust much longer.[17] Even more constraining were Kelvin's estimates of the age of the Sun, which were based on estimates of its thermal output and a theory that the Sun obtains its energy from gravitational collapse; Kelvin estimated that the Sun is about 20 million years old.[19][20]


William Thomson (Lord Kelvin)
Geologists such as Charles Lyell had trouble accepting such a short age for Earth. For biologists, even 100 million years seemed much too short to be plausible. In Charles Darwin's theory of evolution, the process of random heritable variation with cumulative selection requires great durations of time, and Darwin himself stated that Lord Kelvin's estimates did not appear to provide enough.[21] According to modern biology, the total evolutionary history from the beginning of life to today has taken place since 3.5 to 3.8 billion years ago, the amount of time which passed since the last universal ancestor of all living organisms as shown by geological dating.[22]

In a lecture in 1869, Darwin's great advocate, Thomas H. Huxley, attacked Thomson's calculations, suggesting they appeared precise in themselves but were based on faulty assumptions. The physicist Hermann von Helmholtz (in 1856) and astronomer Simon Newcomb (in 1892) contributed their own calculations of 22 and 18 million years, respectively, to the debate: they independently calculated the amount of time it would take for the Sun to condense down to its current diameter and brightness from the nebula of gas and dust from which it was born.[23] Their values were consistent with Thomson's calculations. However, they assumed that the Sun was only glowing from the heat of its gravitational contraction. The process of solar nuclear fusion was not yet known to science.

In 1895 John Perry challenged Kelvin's figure on the basis of his assumptions on conductivity, and Oliver Heaviside entered the dialogue, considering it "a vehicle to display the ability of his operator method to solve problems of astonishing complexity."[24]

Other scientists backed up Thomson's figures. Charles Darwin's son, the astronomer George H. Darwin, proposed that Earth and Moon had broken apart in their early days when they were both molten. He calculated the amount of time it would have taken for tidal friction to give Earth its current 24-hour day. His value of 56 million years added additional evidence that Thomson was on the right track.[23]

The last estimate Thomson gave, in 1897, was: "that it was more than 20 and less than 40 million year old, and probably much nearer 20 than 40".[25] In 1899 and 1900, John Joly calculated the rate at which the oceans should have accumulated salt from erosion processes, and determined that the oceans were about 80 to 100 million years old.[23]

Radiometric dating
Main article: Radiometric dating
Overview
By their chemical nature, rock minerals contain certain elements and not others; but in rocks containing radioactive isotopes, the process of radioactive decay generates exotic elements over time. By measuring the concentration of the stable end product of the decay, coupled with knowledge of the half life and initial concentration of the decaying element, the age of the rock can be calculated.[26] Typical radioactive end products are argon from decay of potassium-40, and lead from decay of uranium and thorium.[26] If the rock becomes molten, as happens in Earth's mantle, such nonradioactive end products typically escape or are redistributed.[26] Thus the age of the oldest terrestrial rock gives a minimum for the age of Earth, assuming that no rock has been intact for longer than the Earth itself.

Convective mantle and radioactivity
In 1892, Thomson had been made Lord Kelvin in appreciation of his many scientific accomplishments. Kelvin calculated the age of the Earth by using thermal gradients, and he arrived at an estimate of about 100 million years.[27] He did not realize that the Earth mantle was convecting, and this invalidated his estimate. In 1895, John Perry produced an age-of-Earth estimate of 2 to 3 billion years using a model of a convective mantle and thin crust,[27] however his work was largely ignored.[17] Kelvin stuck by his estimate of 100 million years, and later reduced it to about 20 million years.

The discovery of radioactivity introduced another factor in the calculation. After Henri Becquerel's initial discovery in 1896, Marie and Pierre Curie discovered the radioactive elements polonium and radium in 1898; and in 1903, Pierre Curie and Albert Laborde announced that radium produces enough heat to melt its own weight in ice in less than an hour. Geologists quickly realized that this upset the assumptions underlying most calculations of the age of Earth. These had assumed that the original heat of the Earth and Sun had dissipated steadily into space, but radioactive decay meant that this heat had been continually replenished. George Darwin and John Joly were the first to point this out, in 1903.[28]

Invention of radiometric dating
Radioactivity, which had overthrown the old calculations, yielded a bonus by providing a basis for new calculations, in the form of radiometric dating.


Ernest Rutherford in 1908
Ernest Rutherford and Frederick Soddy jointly had continued their work on radioactive materials and concluded that radioactivity was due to a spontaneous transmutation of atomic elements. In radioactive decay, an element breaks down into another, lighter element, releasing alpha, beta, or gamma radiation in the process. They also determined that a particular isotope of a radioactive element decays into another element at a distinctive rate. This rate is given in terms of a "half-life", or the amount of time it takes half of a mass of that radioactive material to break down into its "decay product".

Some radioactive materials have short half-lives; some have long half-lives. Uranium and thorium have long half-lives, and so persist in Earth's crust, but radioactive elements with short half-lives have generally disappeared. This suggested that it might be possible to measure the age of Earth by determining the relative proportions of radioactive materials in geological samples. In reality, radioactive elements do not always decay into nonradioactive ("stable") elements directly, instead, decaying into other radioactive elements that have their own half-lives and so on, until they reach a stable element. These "decay chains", such as the uranium-radium and thorium series, were known within a few years of the discovery of radioactivity and provided a basis for constructing techniques of radiometric dating.

The pioneers of radioactivity were chemist Bertram B. Boltwood and the energetic Rutherford. Boltwood had conducted studies of radioactive materials as a consultant, and when Rutherford lectured at Yale in 1904,[29] Boltwood was inspired to describe the relationships between elements in various decay series. Late in 1904, Rutherford took the first step toward radiometric dating by suggesting that the alpha particles released by radioactive decay could be trapped in a rocky material as helium atoms. At the time, Rutherford was only guessing at the relationship between alpha particles and helium atoms, but he would prove the connection four years later.

Soddy and Sir William Ramsay had just determined the rate at which radium produces alpha particles, and Rutherford proposed that he could determine the age of a rock sample by measuring its concentration of helium. He dated a rock in his possession to an age of 40 million years by this technique. Rutherford wrote,

I came into the room, which was half dark, and presently spotted Lord Kelvin in the audience and realized that I was in trouble at the last part of my speech dealing with the age of the Earth, where my views conflicted with his. To my relief, Kelvin fell fast asleep, but as I came to the important point, I saw the old bird sit up, open an eye, and cock a baleful glance at me! Then a sudden inspiration came, and I said, "Lord Kelvin had limited the age of the Earth, provided no new source was discovered. That prophetic utterance refers to what we are now considering tonight, radium!" Behold! the old boy beamed upon me.[30]

Rutherford assumed that the rate of decay of radium as determined by Ramsay and Soddy was accurate, and that helium did not escape from the sample over time. Rutherford's scheme was inaccurate, but it was a useful first step.

Boltwood focused on the end products of decay series. In 1905, he suggested that lead was the final stable product of the decay of radium. It was already known that radium was an intermediate product of the decay of uranium. Rutherford joined in, outlining a decay process in which radium emitted five alpha particles through various intermediate products to end up with lead, and speculated that the radium-lead decay chain could be used to date rock samples. Boltwood did the legwork, and by the end of 1905 had provided dates for 26 separate rock samples, ranging from 92 to 570 million years. He did not publish these results, which was fortunate because they were flawed by measurement errors and poor estimates of the half-life of radium. Boltwood refined his work and finally published the results in 1907.[6]

Boltwood's paper pointed out that samples taken from comparable layers of strata had similar lead-to-uranium ratios, and that samples from older layers had a higher proportion of lead, except where there was evidence that lead had leached out of the sample. His studies were flawed by the fact that the decay series of thorium was not understood, which led to incorrect results for samples that contained both uranium and thorium. However, his calculations were far more accurate than any that had been performed to that time. Refinements in the technique would later give ages for Boltwood's 26 samples of 410 million to 2.2 billion years.[6]

Arthur Holmes establishes radiometric dating
Although Boltwood published his paper in a prominent geological journal, the geological community had little interest in radioactivity.[citation needed] Boltwood gave up work on radiometric dating and went on to investigate other decay series. Rutherford remained mildly curious about the issue of the age of Earth but did little work on it.

Robert Strutt tinkered with Rutherford's helium method until 1910 and then ceased. However, Strutt's student Arthur Holmes became interested in radiometric dating and continued to work on it after everyone else had given up. Holmes focused on lead dating, because he regarded the helium method as unpromising. He performed measurements on rock samples and concluded in 1911 that the oldest (a sample from Ceylon) was about 1.6 billion years old.[31] These calculations were not particularly trustworthy. For example, he assumed that the samples had contained only uranium and no lead when they were formed.

More important research was published in 1913. It showed that elements generally exist in multiple variants with different masses, or "isotopes". In the 1930s, isotopes would be shown to have nuclei with differing numbers of the neutral particles known as "neutrons". In that same year, other research was published establishing the rules for radioactive decay, allowing more precise identification of decay series.

Many geologists felt these new discoveries made radiometric dating so complicated as to be worthless.[citation needed] Holmes felt that they gave him tools to improve his techniques, and he plodded ahead with his research, publishing before and after the First World War. His work was generally ignored until the 1920s, though in 1917 Joseph Barrell, a professor of geology at Yale, redrew geological history as it was understood at the time to conform to Holmes's findings in radiometric dating. Barrell's research determined that the layers of strata had not all been laid down at the same rate, and so current rates of geological change could not be used to provide accurate timelines of the history of Earth.[citation needed]

Holmes' persistence finally began to pay off in 1921, when the speakers at the yearly meeting of the British Association for the Advancement of Science came to a rough consensus that Earth was a few billion years old, and that radiometric dating was credible. Holmes published The Age of the Earth, an Introduction to Geological Ideas in 1927 in which he presented a range of 1.6 to 3.0 billion years. No great push to embrace radiometric dating followed, however, and the die-hards in the geological community stubbornly resisted. They had never cared for attempts by physicists to intrude in their domain, and had successfully ignored them so far.[32] The growing weight of evidence finally tilted the balance in 1931, when the National Research Council of the US National Academy of Sciences decided to resolve the question of the age of Earth by appointing a committee to investigate. Holmes, being one of the few people on Earth who was trained in radiometric dating techniques, was a committee member, and in fact wrote most of the final report.[33]

Thus, Arthur Holmes' report concluded that radioactive dating was the only reliable means of pinning down geological time scales. Questions of bias were deflected by the great and exacting detail of the report. It described the methods used, the care with which measurements were made, and their error bars and limitations.[citation needed]

Modern radiometric dating
Radiometric dating continues to be the predominant way scientists date geologic timescales. Techniques for radioactive dating have been tested and fine-tuned on an ongoing basis since the 1960s. Forty or so different dating techniques have been utilized to date, working on a wide variety of materials. Dates for the same sample using these different techniques are in very close agreement on the age of the material.[citation needed]

Possible contamination problems do exist, but they have been studied and dealt with by careful investigation, leading to sample preparation procedures being minimized to limit the chance of contamination.[citation needed]

Why meteorites were used
An age of 4.55 Â± 0.07 billion years, very close to today's accepted age, was determined by Clair Cameron Patterson using uranium-lead isotope dating (specifically lead-lead dating) on several meteorites including the Canyon Diablo meteorite and published in 1956.[34]


Lead isotope isochron diagram showing data used by Patterson to determine the age of the Earth in 1956.
The quoted age of Earth is derived, in part, from the Canyon Diablo meteorite for several important reasons and is built upon a modern understanding of cosmochemistry built up over decades of research.

Most geological samples from Earth are unable to give a direct date of the formation of Earth from the solar nebula because Earth has undergone differentiation into the core, mantle, and crust, and this has then undergone a long history of mixing and unmixing of these sample reservoirs by plate tectonics, weathering and hydrothermal circulation.

All of these processes may adversely affect isotopic dating mechanisms because the sample cannot always be assumed to have remained as a closed system, by which it is meant that either the parent or daughter nuclide (a species of atom characterised by the number of neutrons and protons an atom contains) or an intermediate daughter nuclide may have been partially removed from the sample, which will skew the resulting isotopic date. To mitigate this effect it is usual to date several minerals in the same sample, to provide an isochron. Alternatively, more than one dating system may be used on a sample to check the date.

Some meteorites are furthermore considered to represent the primitive material from which the accreting solar disk was formed.[35] Some have behaved as closed systems (for some isotopic systems) soon after the solar disk and the planets formed.[citation needed] To date, these assumptions are supported by much scientific observation and repeated isotopic dates, and it is certainly a more robust hypothesis than that which assumes a terrestrial rock has retained its original composition.

Nevertheless, ancient Archaean lead ores of galena have been used to date the formation of Earth as these represent the earliest formed lead-only minerals on the planet and record the earliest homogeneous lead-lead isotope systems on the planet. These have returned age dates of 4.54 billion years with a precision of as little as 1% margin for error.[36]

Statistics for several meteorites that have undergone isochron dating are as follows:[37]

1. St. Severin (ordinary chondrite)
1.	Pb-Pb isochron	4.543 Â± 0.019 billion years
2.	Sm-Nd isochron	4.55 Â± 0.33 billion years
3.	Rb-Sr isochron	4.51 Â± 0.15 billion years
4.	Re-Os isochron	4.68 Â± 0.15 billion years
2. Juvinas (basaltic achondrite)
1.	Pb-Pb isochron	4.556 Â± 0.012 billion years
2.	Pb-Pb isochron	4.540 Â± 0.001 billion years
3.	Sm-Nd isochron	4.56 Â± 0.08 billion years
4.	Rb-Sr isochron	4.50 Â± 0.07 billion years
3. Allende (carbonaceous chondrite)
1.	Pb-Pb isochron	4.553 Â± 0.004 billion years
2.	Ar-Ar age spectrum	4.52 Â± 0.02 billion years
3.	Ar-Ar age spectrum	4.55 Â± 0.03 billion years
4.	Ar-Ar age spectrum 	4.56 Â± 0.05 billion years
Canyon Diablo meteorite
Further information: Age of the Solar System and Canyon Diablo (meteorite)

Barringer Crater, Arizona where the Canyon Diablo meteorite was found.
The Canyon Diablo meteorite was used because it is both large and representative of a particularly rare type of meteorite that contains sulfide minerals (particularly troilite, FeS), metallic nickel-iron alloys, plus silicate minerals. This is important because the presence of the three mineral phases allows investigation of isotopic dates using samples that provide a great separation in concentrations between parent and daughter nuclides. This is particularly true of uranium and lead. Lead is strongly chalcophilic and is found in the sulfide at a much greater concentration than in the silicate, versus uranium. Because of this segregation in the parent and daughter nuclides during the formation of the meteorite, this allowed a much more precise date of the formation of the solar disk and hence the planets than ever before.


Fragment of the Canyon Diablo iron meteorite.
The age determined from the Canyon Diablo meteorite has been confirmed by hundreds of other age determinations, from both terrestrial samples and other meteorites.[38] The meteorite samples, however, show a spread from 4.53 to 4.58 billion years ago. This is interpreted as the duration of formation of the solar nebula and its collapse into the solar disk to form the Sun and the planets. This 50 million year time span allows for accretion of the planets from the original solar dust and meteorites.

The Moon, as another extraterrestrial body that has not undergone plate tectonics and that has no atmosphere, provides quite precise age dates from the samples returned from the Apollo missions. Rocks returned from the Moon have been dated at a maximum of 4.51 billion years old. Martian meteorites that have landed upon Earth have also been dated to around 4.5 billion years old by lead-lead dating. Lunar samples, since they have not been disturbed by weathering, plate tectonics or material moved by organisms, can also provide dating by direct electron microscope examination of cosmic ray tracks. The accumulation of dislocations generated by high energy cosmic ray particle impacts provides another confirmation of the isotopic dates. Cosmic ray dating is only useful on material that has not been melted, since melting erases the crystalline structure of the material, and wipes away the tracks left by the particles.

Altogether, the concordance of age dates of both the earliest terrestrial lead reservoirs and all other reservoirs within the Solar System found to date are used to support the fact that Earth and the rest of the Solar System formed at around 4.53 to 4.58 billion years ago.[citation needed]

See also
	Astronomy portal
	Earth sciences portal
Age of the universe
Creation myth
Geochronology
History of Earth
Natural history
Oldest dated rocks
Timeline of natural history
References
 "Age of the Earth". U.S. Geological Survey. 1997. Archived from the original on 23 December 2005. Retrieved 2006-01-10.
 Dalrymple, G. Brent (2001). "The age of the Earth in the twentieth century: a problem (mostly) solved". Special Publications, Geological Society of London. 190 (1): 205â221. Bibcode:2001GSLSP.190..205D. doi:10.1144/GSL.SP.2001.190.01.14. S2CID 130092094.
 Manhesa, GÃ©rard; AllÃ¨gre, Claude J.; DuprÃ©a, Bernard & Hamelin, Bruno (1980). "Lead isotope study of basic-ultrabasic layered complexes: Speculations about the age of the earth and primitive mantle characteristics". Earth and Planetary Science Letters. 47 (3): 370â382. Bibcode:1980E&PSL..47..370M. doi:10.1016/0012-821X(80)90024-2.
 Braterman, Paul S. (2013). "How Science Figured Out the Age of Earth". Scientific American. Archived from the original on 2016-04-12.
 Hedman, Matthew (2007). "9: Meteorites and the Age of the Solar System". The Age of Everything. University of Chicago Press. pp. 142â162. ISBN 9780226322940. Archived from the original on 2018-02-14.
 Boltwood, B. B. (1907). "On the ultimate disintegration products of the radio-active elements. Part II. The disintegration products of uranium". American Journal of Science. 23 (134): 77â88. doi:10.2475/ajs.s4-23.134.78. S2CID 131688682.
For the abstract, see: Chemical Abstracts Service, American Chemical Society (1907). Chemical Abstracts. New York, London: American Chemical Society. p. 817. Retrieved 2008-12-19.
 Wilde, S. A.; Valley, J. W.; Peck, W. H.; Graham C. M. (2001-01-11). "Evidence from detrital zircons for the existence of continental crust and oceans on the Earth 4.4 Gyr ago". Nature. 409 (6817): 175â178. Bibcode:2001Natur.409..175W. doi:10.1038/35051550. PMID 11196637. S2CID 4319774.
 Valley, John W.; Peck, William H.; Kin, Elizabeth M. (1999). "Zircons Are Forever" (PDF). The Outcrop, Geology Alumni Newsletter. University of Wisconsin-Madison. pp. 34â35. Archived (PDF) from the original on 2009-02-26. Retrieved 2008-12-22.
 Wyche, S.; Nelson, D. R.; Riganti, A. (2004). "4350â3130 Ma detrital zircons in the Southern Cross GraniteâGreenstone Terrane, Western Australia: implications for the early evolution of the Yilgarn Craton". Australian Journal of Earth Sciences. 51 (1): 31â45. Bibcode:2004AuJES..51...31W. doi:10.1046/j.1400-0952.2003.01042.x.
 Amelin, Y; Krot, An; Hutcheon, Id; Ulyanov, Aa (Sep 2002). "Lead isotopic ages of chondrules and calcium-aluminum-rich inclusions". Science. 297 (5587): 1678â83. Bibcode:2002Sci...297.1678A. doi:10.1126/science.1073950. ISSN 0036-8075. PMID 12215641. S2CID 24923770.
 Baker, J.; Bizzarro, M.; Wittig, N.; Connelly, J.; et al. (2005-08-25). "Early planetesimal melting from an age of 4.5662 Gyr for differentiated meteorites". Nature. 436 (7054): 1127â1131. Bibcode:2005Natur.436.1127B. doi:10.1038/nature03882. PMID 16121173. S2CID 4304613.
 Lyell, Charles, Sir (1866). Elements of Geology; or, The Ancient Changes of the Earth and its Inhabitants as Illustrated by Geological Monuments (Sixth ed.). New York: D. Appleton and company. Retrieved 2008-12-19.
 Stiebing, William H. (1994). Uncovering the Past. Oxford University Press US. ISBN 978-0-19-508921-9.
 Brookfield, Michael E. (2004). Principles of Stratigraphy. Blackwell Publishing. p. 116. ISBN 978-1-4051-1164-5.
 Fuller, J. G. C. M. (2007-07-17). "Smith's other debt, John Strachey, William Smith and the strata of England 1719â1801". Geoscientist. The Geological Society. Archived from the original on 24 November 2008. Retrieved 2008-12-19.
 Burchfield, Joe D. (1998). "The age of the Earth and the invention of geological time". Geological Society, London, Special Publications. 143 (1): 137â143. Bibcode:1998GSLSP.143..137B. CiteSeerX 10.1.1.557.2702. doi:10.1144/GSL.SP.1998.143.01.12. S2CID 129443412.
 England, P.; Molnar, P.; Righter, F. (January 2007). "John Perry's neglected critique of Kelvin's age for the Earth: A missed opportunity in geodynamics". GSA Today. 17 (1): 4â9. doi:10.1130/GSAT01701A.1.
 Dalrymple (1994) pp. 14â17, 38
 Burchfield, Joe D. (1990-05-15). Lord Kelvin and the Age of the Earth. University of Chicago Press. pp. 69 ff. ISBN 9780226080437. Archived from the original on 2018-02-14.
 Stacey, Frank D. (2000). "Kelvin's age of the Earth paradox revisited". Journal of Geophysical Research. 105 (B6): 13155â13158. Bibcode:2000JGR...10513155S. doi:10.1029/2000JB900028.
 Origin of Species, Charles Darwin, 1872 edition, page 286
 Borenstein, Seth (November 13, 2013). "Oldest fossil found: Meet your microbial mom". Excite. Yonkers, NY: Mindspark Interactive Network. Associated Press. Archived from the original on June 29, 2015. Retrieved 2015-03-02.)
 Dalrymple (1994) pp. 14â17
 Paul J. Nahin (1985) Oliver Heaviside, Fractional Operators, and the Age of the Earth, IEEE Transactions on Education E-28(2): 94â104, link from IEEE Explore
 Dalrymple (1994) pp. 14, 43
 Nichols, Gary (2009). "21.2 Radiometric Dating". Sedimentology and Stratigraphy. John Wiley & Sons. pp. 325â327. ISBN 978-1405193795.
 England, Philip C.; Molnar, Peter; Richter, Frank M. (2007). "Kelvin, Perry and the Age of the Earth" (PDF). American Scientist. 95 (4): 342â349. CiteSeerX 10.1.1.579.1433. doi:10.1511/2007.66.3755. Archived (PDF) from the original on 2010-07-02.
 Joly, John (1909). Radioactivity and Geology: An Account of the Influence of Radioactive Energy on Terrestrial History (1st ed.). London, UK: Archibald Constable & Co., ltd. p. 36. Reprinted by BookSurge Publishing (2004) ISBN 1-4021-3577-7.
 Rutherford, E. (1906). Radioactive Transformations. London: Charles Scribner's Sons. Reprinted by Juniper Grove (2007) ISBN 978-1-60355-054-3.
 Eve, Arthur Stewart (1939). Rutherford: Being the life and letters of the Rt. Hon. Lord Rutherford, O. M.. Cambridge: Cambridge University Press.
 Dalrymple (1994) p. 74
 The Age of the Earth Debate Badash, L Scientific American 1989 esp p95 Archived 2016-11-05 at the Wayback Machine
 Dalrymple (1994) pp. 77â78
 Patterson, Claire (1956). "Age of meteorites and the earth" (PDF). Geochimica et Cosmochimica Acta. 10 (4): 230â237. Bibcode:1956GeCoA..10..230P. doi:10.1016/0016-7037(56)90036-9. Archived (PDF) from the original on 2010-06-21. Retrieved 2009-07-07.
 Carlson, R. W.; Tera, F. (December 1â3, 1998). "Lead-Lead Constraints on the Timescale of Early Planetary Differentiation" (PDF). Conference Proceedings, Origin of the Earth and Moon. Houston, Texas: Lunar and Planetary Institute. p. 6. Archived (PDF) from the original on 16 December 2008. Retrieved 2008-12-22.
 Dalrymple (1994) pp. 310â341
 Dalrymple, Brent G. (2004). Ancient Earth, Ancient Skies: The Age of the Earth and Its Cosmic Surroundings. Stanford University Press. pp. 147, 169. ISBN 978-0-8047-4933-6.
 Terada, K.; Sano, Y. (May 20â24, 2001). "In-situ ion microprobe U-Pb dating of phosphates in H-chondrites" (PDF). Proceedings, Eleventh Annual V. M. Goldschmidt Conference. Hot Springs, Virginia: Lunar and Planetary Institute. Bibcode:2001eag..conf.3306T. Archived (PDF) from the original on 16 December 2008. Retrieved 2008-12-22.
Bibliography
Dalrymple, G. Brent (1994-02-01). The Age of the Earth. Stanford University Press. ISBN 978-0-8047-2331-2.
Further reading
Baadsgaard, H.; Lerbekmo, J.F.; Wijbrans, J.R., 1993. Multimethod radiometric age for a bentonite near the top of the Baculites reesidei Zone of southwestern Saskatchewan (Campanian-Maastrichtian stage boundary?). Canadian Journal of Earth Sciences, v.30, p. 769â775.
Baadsgaard, H. and Lerbekmo, J.F., 1988. A radiometric age for the Cretaceous-Tertiary boundary based on K-Ar, Rb-Sr, and U-Pb ages of bentonites from Alberta, Saskatchewan, and Montana. Canadian Journal of Earth Sciences, v.25, p. 1088â1097.
Eberth, D.A. and Braman, D., 1990. Stratigraphy, sedimentology, and vertebrate paleontology of the Judith River Formation (Campanian) near Muddy Lake, west-central Saskatchewan. Bulletin of Canadian Petroleum Geology, v.38, no.4, p. 387â406.
Goodwin, M.B. and Deino, A.L., 1989. The first radiometric ages from the Judith River Formation (Upper Cretaceous), Hill County, Montana. Canadian Journal of Earth Sciences, v.26, p. 1384â1391.
Gradstein, F. M.; Agterberg, F.P.; Ogg, J.G.; Hardenbol, J.; van Veen, P.; Thierry, J. and Zehui Huang., 1995. A Triassic, Jurassic and Cretaceous time scale. IN: Bergren, W. A. ; Kent, D.V.; Aubry, M-P. and Hardenbol, J. (eds.), Geochronology, Time Scales, and Global Stratigraphic Correlation. Society of Economic Paleontologists and Mineralogists, Special Publication No. 54, p. 95â126.
Harland, W.B., Cox, A.V.; Llewellyn, P.G.; Pickton, C.A.G.; Smith, A.G.; and Walters, R., 1982. A Geologic Time Scale: 1982 edition. Cambridge University Press: Cambridge, 131p.
Harland, W.B.; Armstrong, R.L.; Cox, A.V.; Craig, L.E.; Smith, A.G.; Smith, D.G., 1990. A Geologic Time Scale, 1989 edition. Cambridge University Press: Cambridge, p. 1â263. ISBN 0-521-38765-5
Harper, C.W. Jr (1980). "Relative age inference in paleontology". Lethaia. 13 (3): 239â248. doi:10.1111/j.1502-3931.1980.tb00638.x.
Obradovich, J.D., 1993. A Cretaceous time scale. IN: Caldwell, W.G.E. and Kauffman, E.G. (eds.). Evolution of the Western Interior Basin. Geological Association of Canada, Special Paper 39, p. 379â396.
Palmer, Allison R (1983). "The Decade of North American Geology 1983 Geologic Time Scale". Geology. 11 (9): 503â504. Bibcode:1983Geo....11..503P. doi:10.1130/0091-7613(1983)11<503:tdonag>2.0.co;2.
Powell, James Lawrence, 2001, Mysteries of Terra Firma: the Age and Evolution of the Earth, Simon & Schuster, ISBN 0-684-87282-X
External links
TalkOrigins.org
USGS preface on the Age of the Earth
NASA exposition on the age of Martian meteorites
Ageing the Earth on In Our Time at the BBC
Pre-1900 Non-Religious Estimates of the Age of the Earth
vte
Earth
Continents
AfricaAntarcticaAsiaAustraliaEuropeNorth AmericaSouth America
The Earth seen from Apollo 17.jpg
Oceans
Arctic OceanAtlantic OceanIndian OceanPacific OceanSouthern Ocean
Geology
Age of EarthGeologyEarth scienceExtremes on EarthFutureGeological historyGeologic recordGeophysicsGravityHistory of EarthMagnetic fieldPlate tectonicsStructure
Atmosphere
Atmosphere of EarthClimateGlobal warmingWeather
Environment
BiomeBiosphereEcologyEcosystemHuman impact on the environmentEvolutionary history of lifeNature
Culture and society
CartographyList of countriesDigital mappingIn cultureEarth DayWorld economyEtymologyWorld historyTime zonesWorld
Planetary science
Earth's orbitEvolution of Solar SystemGeology of solar terrestrial planetsLocation in the UniverseThe MoonSolar System
Category CategoryOutline Outline of EarthPortal Earth sciences portalPortal Solar System portal
Authority control Edit this at Wikidata
LCCN: sh85040428
Categories: GeochronologyHistory of Earth scienceGeology theories
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadView sourceView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Nederlands
æ¥æ¬èª
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
32 more
Edit links
This page was last edited on 15 October 2020, at 17:27 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Radiometric dating
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Radiometric dating, radioactive dating or radioisotope dating is a technique which is used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay.[1] The use of radiometric dating was first published in 1907 by Bertram Boltwood[2] and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of the Earth itself, and can also be used to date a wide range of natural and man-made materials.

Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale.[3] Among the best-known techniques are radiocarbon dating, potassiumâargon dating and uraniumâlead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.

Different methods of radiometric dating vary in the timescale over which they are accurate and the materials to which they can be applied.


Contents
1	Fundamentals
1.1	Radioactive decay
1.2	Decay constant determination
1.3	Accuracy of radiometric dating
1.4	Closure temperature
1.5	The age equation
2	Modern dating methods
2.1	Uraniumâlead dating method
2.2	Samariumâneodymium dating method
2.3	Potassiumâargon dating method
2.4	Rubidiumâstrontium dating method
2.5	Uraniumâthorium dating method
2.6	Radiocarbon dating method
2.7	Fission track dating method
2.8	Chlorine-36 dating method
2.9	Luminescence dating methods
2.10	Other methods
3	Dating with decay products of short-lived extinct radionuclides
3.1	The 129I â 129Xe chronometer
3.2	The 26Al â 26Mg chronometer
4	See also
5	References
6	Further reading
Fundamentals
Radioactive decay

Example of a radioactive decay chain from lead-212 (212Pb) to lead-208 (208Pb) . Each parent nuclide spontaneously decays into a daughter nuclide (the decay product) via an Î± decay or a Î²â decay. The final decay product, lead-208 (208Pb), is stable and can no longer undergo spontaneous radioactive decay.
All ordinary matter is made up of combinations of chemical elements, each with its own atomic number, indicating the number of protons in the atomic nucleus. Additionally, elements may exist in different isotopes, with each isotope of an element differing in the number of neutrons in the nucleus. A particular isotope of a particular element is called a nuclide. Some nuclides are inherently unstable. That is, at some point in time, an atom of such a nuclide will undergo radioactive decay and spontaneously transform into a different nuclide. This transformation may be accomplished in a number of different ways, including alpha decay (emission of alpha particles) and beta decay (electron emission, positron emission, or electron capture). Another possibility is spontaneous fission into two or more nuclides.

While the moment in time at which a particular nucleus decays is unpredictable, a collection of atoms of a radioactive nuclide decays exponentially at a rate described by a parameter known as the half-life, usually given in units of years when discussing dating techniques. After one half-life has elapsed, one half of the atoms of the nuclide in question will have decayed into a "daughter" nuclide or decay product. In many cases, the daughter nuclide itself is radioactive, resulting in a decay chain, eventually ending with the formation of a stable (nonradioactive) daughter nuclide; each step in such a chain is characterized by a distinct half-life. In these cases, usually the half-life of interest in radiometric dating is the longest one in the chain, which is the rate-limiting factor in the ultimate transformation of the radioactive nuclide into its stable daughter. Isotopic systems that have been exploited for radiometric dating have half-lives ranging from only about 10 years (e.g., tritium) to over 100 billion years (e.g., samarium-147).[4]

For most radioactive nuclides, the half-life depends solely on nuclear properties and is essentially constant.[5] This is known because decay constants measured by different techniques give consistent values within analytical errors and the ages of the same materials are consistent from one method to another. It is not affected by external factors such as temperature, pressure, chemical environment, or presence of a magnetic or electric field.[6][7][8] The only exceptions are nuclides that decay by the process of electron capture, such as beryllium-7, strontium-85, and zirconium-89, whose decay rate may be affected by local electron density. For all other nuclides, the proportion of the original nuclide to its decay products changes in a predictable way as the original nuclide decays over time.

This predictability allows the relative abundances of related nuclides to be used as a clock to measure the time from the incorporation of the original nuclides into a material to the present. Nature has conveniently provided us with radioactive nuclides that have half-lives which range from considerably longer than the age of the universe, to less than a zeptosecond. This allows one to measure a very wide range of ages. Isotopes with very long half-lives are called "stable isotopes," and isotopes with very short half-lives are known as "extinct isotopes."

Decay constant determination
See also: Radioactive decay law
The radioactive decay constant, the probability that an atom will decay per year, is the solid foundation of the common measurement of radioactivity. The accuracy and precision of the determination of an age (and a nuclide's half-life) depends on the accuracy and precision of the decay constant measurement.[9] The in-growth method is one way of measuring the decay constant of a system, which involves accumulating daughter nuclides. Unfortunately for nuclides with high decay constants (which are useful for dating very old samples), long periods of time (decades) are required to accumulate enough decay products in a single sample to accurately measure them. A faster method involves using particle counters to determine alpha, beta or gamma activity, and then dividing that by the number of radioactive nuclides. However, it is challenging and expensive to accurately determine the number of radioactive nuclides. Alternatively, decay constants can be determined by comparing isotope data for rocks of known age. This method requires at least one of the isotope systems to be very precisely calibrated, such as the Pb-Pb system.

Accuracy of radiometric dating

Thermal ionization mass spectrometer used in radiometric dating.
The basic equation of radiometric dating requires that neither the parent nuclide nor the daughter product can enter or leave the material after its formation. The possible confounding effects of contamination of parent and daughter isotopes have to be considered, as do the effects of any loss or gain of such isotopes since the sample was created. It is therefore essential to have as much information as possible about the material being dated and to check for possible signs of alteration.[10] Precision is enhanced if measurements are taken on multiple samples from different locations of the rock body. Alternatively, if several different minerals can be dated from the same sample and are assumed to be formed by the same event and were in equilibrium with the reservoir when they formed, they should form an isochron. This can reduce the problem of contamination. In uraniumâlead dating, the concordia diagram is used which also decreases the problem of nuclide loss. Finally, correlation between different isotopic dating methods may be required to confirm the age of a sample. For example, the age of the Amitsoq gneisses from western Greenland was determined to be 3.60 Â± 0.05 Ga (billion years ago) using uraniumâlead dating and 3.56 Â± 0.10 Ga (billion years ago) using leadâlead dating, results that are consistent with each other.[11]:142â143

Accurate radiometric dating generally requires that the parent has a long enough half-life that it will be present in significant amounts at the time of measurement (except as described below under "Dating with short-lived extinct radionuclides"), the half-life of the parent is accurately known, and enough of the daughter product is produced to be accurately measured and distinguished from the initial amount of the daughter present in the material. The procedures used to isolate and analyze the parent and daughter nuclides must be precise and accurate. This normally involves isotope-ratio mass spectrometry.[12]

The precision of a dating method depends in part on the half-life of the radioactive isotope involved. For instance, carbon-14 has a half-life of 5,730 years. After an organism has been dead for 60,000 years, so little carbon-14 is left that accurate dating cannot be established. On the other hand, the concentration of carbon-14 falls off so steeply that the age of relatively young remains can be determined precisely to within a few decades.[13]

Closure temperature
Main article: Closure temperature
The closure temperature or blocking temperature represents the temperature below which the mineral is a closed system for the studied isotopes. If a material that selectively rejects the daughter nuclide is heated above this temperature, any daughter nuclides that have been accumulated over time will be lost through diffusion, resetting the isotopic "clock" to zero. As the mineral cools, the crystal structure begins to form and diffusion of isotopes is less easy. At a certain temperature, the crystal structure has formed sufficiently to prevent diffusion of isotopes. Thus an igneous or metamorphic rock or melt, which is slowly cooling, does not begin to exhibit measurable radioactive decay until it cools below the closure temperature. The age that can be calculated by radiometric dating is thus the time at which the rock or mineral cooled to closure temperature.[14][15] This temperature varies for every mineral and isotopic system, so a system can be closed for one mineral but open for another. Dating of different minerals and/or isotope systems (with differing closure temperatures) within the same rock can therefore enable the tracking of the thermal history of the rock in question with time, and thus the history of metamorphic events may become known in detail. These temperatures are experimentally determined in the lab by artificially resetting sample minerals using a high-temperature furnace. This field is known as thermochronology or thermochronometry.

The age equation

Lu-Hf isochrons plotted of meteorite samples. The age is calculated from the slope of the isochron (line) and the original composition from the intercept of the isochron with the y-axis.
The mathematical expression that relates radioactive decay to geologic time is[14][16]

D* = D0 + N(t) (eÎ»t â 1)
where

t is age of the sample,
D* is number of atoms of the radiogenic daughter isotope in the sample,
D0 is number of atoms of the daughter isotope in the original or initial composition,
N(t) is number of atoms of the parent isotope in the sample at time t (the present), given by N(t) = Noe-Î»t, and
Î» is the decay constant of the parent isotope, equal to the inverse of the radioactive half-life of the parent isotope[17] times the natural logarithm of 2.
The equation is most conveniently expressed in terms of the measured quantity N(t) rather than the constant initial value No.

To calculate the age, it is assumed that the system is closed (neither parent nor daughter isotopes have been lost from system), D0 must be either negligible or can be accurately estimated, Î» is known to a high precision, and one has accurate and precise measurements of D* and N(t).

The above equation makes use of information on the composition of parent and daughter isotopes at the time the material being tested cooled below its closure temperature. This is well-established for most isotopic systems.[15][18] However, construction of an isochron does not require information on the original compositions, using merely the present ratios of the parent and daughter isotopes to a standard isotope. An isochron plot is used to solve the age equation graphically and calculate the age of the sample and the original composition.

Modern dating methods
Radiometric dating has been carried out since 1905 when it was invented by Ernest Rutherford as a method by which one might determine the age of the Earth. In the century since then the techniques have been greatly improved and expanded.[17] Dating can now be performed on samples as small as a nanogram using a mass spectrometer. The mass spectrometer was invented in the 1940s and began to be used in radiometric dating in the 1950s. It operates by generating a beam of ionized atoms from the sample under test. The ions then travel through a magnetic field, which diverts them into different sampling sensors, known as "Faraday cups", depending on their mass and level of ionization. On impact in the cups, the ions set up a very weak current that can be measured to determine the rate of impacts and the relative concentrations of different atoms in the beams.

Uraniumâlead dating method
Main article: Uraniumâlead dating

A concordia diagram as used in uraniumâlead dating, with data from the Pfunze Belt, Zimbabwe.[19] All the samples show loss of lead isotopes, but the intercept of the errorchron (straight line through the sample points) and the concordia (curve) shows the correct age of the rock.[15]
Uraniumâlead radiometric dating involves using uranium-235 or uranium-238 to date a substance's absolute age. This scheme has been refined to the point that the error margin in dates of rocks can be as low as less than two million years in two-and-a-half billion years.[20][21] An error margin of 2â5% has been achieved on younger Mesozoic rocks.[22]

Uraniumâlead dating is often performed on the mineral zircon (ZrSiO4), though it can be used on other materials, such as baddeleyite, as well as monazite (see: monazite geochronology).[23] Zircon and baddeleyite incorporate uranium atoms into their crystalline structure as substitutes for zirconium, but strongly reject lead. Zircon has a very high closure temperature, is resistant to mechanical weathering and is very chemically inert. Zircon also forms multiple crystal layers during metamorphic events, which each may record an isotopic age of the event. In situ micro-beam analysis can be achieved via laser ICP-MS or SIMS techniques.[24]

One of its great advantages is that any sample provides two clocks, one based on uranium-235's decay to lead-207 with a half-life of about 700 million years, and one based on uranium-238's decay to lead-206 with a half-life of about 4.5 billion years, providing a built-in crosscheck that allows accurate determination of the age of the sample even if some of the lead has been lost. This can be seen in the concordia diagram, where the samples plot along an errorchron (straight line) which intersects the concordia curve at the age of the sample.

Samariumâneodymium dating method
Main article: Samariumâneodymium dating
This involves the alpha decay of 147Sm to 143Nd with a half-life of 1.06 x 1011 years. Accuracy levels of within twenty million years in ages of two-and-a-half billion years are achievable.[25]

Potassiumâargon dating method
Main article: Potassiumâargon dating
This involves electron capture or positron decay of potassium-40 to argon-40. Potassium-40 has a half-life of 1.3 billion years, so this method is applicable to the oldest rocks. Radioactive potassium-40 is common in micas, feldspars, and hornblendes, though the closure temperature is fairly low in these materials, about 350 Â°C (mica) to 500 Â°C (hornblende).

Rubidiumâstrontium dating method
Main article: Rubidiumâstrontium dating
This is based on the beta decay of rubidium-87 to strontium-87, with a half-life of 50 billion years. This scheme is used to date old igneous and metamorphic rocks, and has also been used to date lunar samples. Closure temperatures are so high that they are not a concern. Rubidium-strontium dating is not as precise as the uranium-lead method, with errors of 30 to 50 million years for a 3-billion-year-old sample. Application of in situ analysis (Laser-Ablation ICP-MS) within single mineral grains in faults have shown that the Rb-Sr method can be used to decipher episodes of fault movement.[26]

Uraniumâthorium dating method
Main article: Uraniumâthorium dating
A relatively short-range dating technique is based on the decay of uranium-234 into thorium-230, a substance with a half-life of about 80,000 years. It is accompanied by a sister process, in which uranium-235 decays into protactinium-231, which has a half-life of 32,760 years.

While uranium is water-soluble, thorium and protactinium are not, and so they are selectively precipitated into ocean-floor sediments, from which their ratios are measured. The scheme has a range of several hundred thousand years. A related method is ioniumâthorium dating, which measures the ratio of ionium (thorium-230) to thorium-232 in ocean sediment.

Radiocarbon dating method
Main article: Radiocarbon dating

Ale's Stones at KÃ¥seberga, around ten kilometres south east of Ystad, Sweden were dated at 56 CE using the carbon-14 method on organic material found at the site.[27]
Radiocarbon dating is also simply called carbon-14 dating. Carbon-14 is a radioactive isotope of carbon, with a half-life of 5,730 years[28][29] (which is very short compared with the above isotopes), and decays into nitrogen.[30] In other radiometric dating methods, the heavy parent isotopes were produced by nucleosynthesis in supernovas, meaning that any parent isotope with a short half-life should be extinct by now. Carbon-14, though, is continuously created through collisions of neutrons generated by cosmic rays with nitrogen in the upper atmosphere and thus remains at a near-constant level on Earth. The carbon-14 ends up as a trace component in atmospheric carbon dioxide (CO2).

A carbon-based life form acquires carbon during its lifetime. Plants acquire it through photosynthesis, and animals acquire it from consumption of plants and other animals. When an organism dies, it ceases to take in new carbon-14, and the existing isotope decays with a characteristic half-life (5730 years). The proportion of carbon-14 left when the remains of the organism are examined provides an indication of the time elapsed since its death. This makes carbon-14 an ideal dating method to date the age of bones or the remains of an organism. The carbon-14 dating limit lies around 58,000 to 62,000 years.[31]

The rate of creation of carbon-14 appears to be roughly constant, as cross-checks of carbon-14 dating with other dating methods show it gives consistent results. However, local eruptions of volcanoes or other events that give off large amounts of carbon dioxide can reduce local concentrations of carbon-14 and give inaccurate dates. The releases of carbon dioxide into the biosphere as a consequence of industrialization have also depressed the proportion of carbon-14 by a few percent; conversely, the amount of carbon-14 was increased by above-ground nuclear bomb tests that were conducted into the early 1960s. Also, an increase in the solar wind or the Earth's magnetic field above the current value would depress the amount of carbon-14 created in the atmosphere.

Fission track dating method
Main article: fission track dating

Apatite crystals are widely used in fission track dating.
This involves inspection of a polished slice of a material to determine the density of "track" markings left in it by the spontaneous fission of uranium-238 impurities. The uranium content of the sample has to be known, but that can be determined by placing a plastic film over the polished slice of the material, and bombarding it with slow neutrons. This causes induced fission of 235U, as opposed to the spontaneous fission of 238U. The fission tracks produced by this process are recorded in the plastic film. The uranium content of the material can then be calculated from the number of tracks and the neutron flux.

This scheme has application over a wide range of geologic dates. For dates up to a few million years micas, tektites (glass fragments from volcanic eruptions), and meteorites are best used. Older materials can be dated using zircon, apatite, titanite, epidote and garnet which have a variable amount of uranium content.[32] Because the fission tracks are healed by temperatures over about 200 Â°C the technique has limitations as well as benefits. The technique has potential applications for detailing the thermal history of a deposit.

Chlorine-36 dating method
Large amounts of otherwise rare 36Cl (half-life ~300ky) were produced by irradiation of seawater during atmospheric detonations of nuclear weapons between 1952 and 1958. The residence time of 36Cl in the atmosphere is about 1 week. Thus, as an event marker of 1950s water in soil and ground water, 36Cl is also useful for dating waters less than 50 years before the present. 36Cl has seen use in other areas of the geological sciences, including dating ice and sediments.

Luminescence dating methods
Main article: Luminescence dating
Luminescence dating methods are not radiometric dating methods in that they do not rely on abundances of isotopes to calculate age. Instead, they are a consequence of background radiation on certain minerals. Over time, ionizing radiation is absorbed by mineral grains in sediments and archaeological materials such as quartz and potassium feldspar. The radiation causes charge to remain within the grains in structurally unstable "electron traps". Exposure to sunlight or heat releases these charges, effectively "bleaching" the sample and resetting the clock to zero. The trapped charge accumulates over time at a rate determined by the amount of background radiation at the location where the sample was buried. Stimulating these mineral grains using either light (optically stimulated luminescence or infrared stimulated luminescence dating) or heat (thermoluminescence dating) causes a luminescence signal to be emitted as the stored unstable electron energy is released, the intensity of which varies depending on the amount of radiation absorbed during burial and specific properties of the mineral.

These methods can be used to date the age of a sediment layer, as layers deposited on top would prevent the grains from being "bleached" and reset by sunlight. Pottery shards can be dated to the last time they experienced significant heat, generally when they were fired in a kiln.

Other methods
Other methods include:

Argonâargon (ArâAr)
Iodineâxenon (IâXe)
Lanthanumâbarium (LaâBa)
Leadâlead (PbâPb)
Lutetiumâhafnium (LuâHf)
Potassiumâcalcium (KâCa)
Rheniumâosmium (ReâOs)
Uraniumâuranium (UâU)
Kryptonâkrypton (KrâKr)
Beryllium (10Beâ9Be)[33]
Dating with decay products of short-lived extinct radionuclides
Absolute radiometric dating requires a measurable fraction of parent nucleus to remain in the sample rock. For rocks dating back to the beginning of the solar system, this requires extremely long-lived parent isotopes, making measurement of such rocks' exact ages imprecise. To be able to distinguish the relative ages of rocks from such old material, and to get a better time resolution than that available from long-lived isotopes, short-lived isotopes that are no longer present in the rock can be used.[34]

At the beginning of the solar system, there were several relatively short-lived radionuclides like 26Al, 60Fe, 53Mn, and 129I present within the solar nebula. These radionuclidesâpossibly produced by the explosion of a supernovaâare extinct today, but their decay products can be detected in very old material, such as that which constitutes meteorites. By measuring the decay products of extinct radionuclides with a mass spectrometer and using isochronplots, it is possible to determine relative ages of different events in the early history of the solar system. Dating methods based on extinct radionuclides can also be calibrated with the U-Pb method to give absolute ages. Thus both the approximate age and a high time resolution can be obtained. Generally a shorter half-life leads to a higher time resolution at the expense of timescale.

The 129I â 129Xe chronometer
See also: Iodine-129 Â§ Meteorite age dating
129
I
 beta-decays to 129
Xe
 with a half-life of 16 million years. The iodine-xenon chronometer[35] is an isochron technique. Samples are exposed to neutrons in a nuclear reactor. This converts the only stable isotope of iodine (127
I
) into 128
Xe
 via neutron capture followed by beta decay (of 128
I
). After irradiation, samples are heated in a series of steps and the xenon isotopic signature of the gas evolved in each step is analysed. When a consistent 129
Xe
/128
Xe
 ratio is observed across several consecutive temperature steps, it can be interpreted as corresponding to a time at which the sample stopped losing xenon.

Samples of a meteorite called Shallowater are usually included in the irradiation to monitor the conversion efficiency from 127
I
 to 128
Xe
. The difference between the measured 129
Xe
/128
Xe
 ratios of the sample and Shallowater then corresponds to the different ratios of 129
I
/127
I
 when they each stopped losing xenon. This in turn corresponds to a difference in age of closure in the early solar system.

The 26Al â 26Mg chronometer
Another example of short-lived extinct radionuclide dating is the 26
Al
 â 26
Mg
 chronometer, which can be used to estimate the relative ages of chondrules. 26
Al
 decays to 26
Mg
 with a half-life of 720 000 years. The dating is simply a question of finding the deviation from the natural abundance of 26
Mg
 (the product of 26
Al
 decay) in comparison with the ratio of the stable isotopes 27
Al
/24
Mg
.

The excess of 26
Mg
 (often designated 26
Mg
*) is found by comparing the 26
Mg
/27
Mg
 ratio to that of other Solar System materials.[36]

The 26
Al
 â 26
Mg
 chronometer gives an estimate of the time period for formation of primitive meteorites of only a few million years (1.4 million years for Chondrule formation).[37]

See also
	Earth sciences portal
icon	Geophysics portal
icon	Physics portal
Hadean zircon
Isotope geochemistry
Paleopedological record
Radioactivity
Radiohalo
Sensitive high-resolution ion microprobe (SHRIMP)
References
 IUPAC, Compendium of Chemical Terminology, 2nd ed. (the "Gold Book") (1997). Online corrected version:  (2006â) "radioactive dating". doi:10.1351/goldbook.R05082
 Boltwood, Bertram (1907). "The Ultimate Disintegration Products of the Radio-active Elements. Part II. The disintegration products of uranium". American Journal of Science. 4. 23 (134): 77â88. doi:10.2475/ajs.s4-23.134.78. S2CID 131688682.
 McRae, A. 1998. Radiometric Dating and the Geological Time Scale: Circular Reasoning or Reliable Tools? Radiometric Dating and the Geological Time Scale TalkOrigins Archive
 Bernard-Griffiths, J.; Groan, G. (1989). "The samariumâneodymium method". In Roth, Etienne; Poty, Bernard (eds.). Nuclear Methods of Dating. Springer Netherlands. pp. 53â72. ISBN 978-0-7923-0188-2.
 PommÃ©, S.; Stroh, H.; Altzitzoglou, T.; Paepen, J.; Van Ammel, R.; Kossert, K.; NÃ¤hle, O.; Keightley, J. D.; Ferreira, K. M.; Verheyen, L.; Bruggeman, M. (1 April 2018). "Is decay constant?". Applied Radiation and Isotopes. ICRM 2017 Proceedings of the 21st International Conference on Radionuclide Metrology and its Applications. 134: 6â12. doi:10.1016/j.apradiso.2017.09.002. ISSN 0969-8043. PMID 28947247.
 Emery, G T (1972). "Perturbation of Nuclear Decay Rates". Annual Review of Nuclear Science. 22 (1): 165â202. Bibcode:1972ARNPS..22..165E. doi:10.1146/annurev.ns.22.120172.001121.
 Shlyakhter, A. I. (1976). "Direct test of the constancy of fundamental nuclear constants". Nature. 264 (5584): 340. Bibcode:1976Natur.264..340S. doi:10.1038/264340a0. S2CID 4252035.
 Johnson, B. 1993. How to Change Nuclear Decay Rates Usenet Physics FAQ
 Begemann, F.; Ludwig, K.R.; Lugmair, G.W.; Min, K.; Nyquist, L.E.; Patchett, P.J.; Renne, P.R.; Shih, C.-Y.; Villa, I.M.; Walker, R.J. (January 2001). "Call for an improved set of decay constants for geochronological use". Geochimica et Cosmochimica Acta. 65 (1): 111â121. doi:10.1016/s0016-7037(00)00512-3. ISSN 0016-7037.
 Stewart, K,, Turner, S, Kelley, S, Hawkesworh, C Kristein, L and Manotvani, M (1996). "3-D, 40Ar---39Ar geochronology in the ParanÃ¡ continental flood basalt province". Earth and Planetary Science Letters. 143 (1â4): 95â109. Bibcode:1996E&PSL.143...95S. doi:10.1016/0012-821X(96)00132-X.
 Dalrymple, G. Brent (1994). The age of the earth. Stanford, Calif.: Stanford Univ. Press. ISBN 9780804723312.
 Dickin, Alan P. (2008). Radiogenic isotope geology (2nd ed.). Cambridge: Cambridge Univ. Press. pp. 15â49. ISBN 9780521530170.
 Reimer Paula J, et al. (2004). "INTCAL04 Terrestrial Radiocarbon Age Calibration, 0â26 Cal Kyr BP". Radiocarbon. 46 (3): 1029â1058. doi:10.1017/S0033822200032999.
 Faure, Gunter (1998). Principles and applications of geochemistry: a comprehensive textbook for geology students (2nd ed.). Englewood Cliffs, New Jersey: Prentice Hall. ISBN 978-0-02-336450-1. OCLC 37783103.[page needed]
 Rollinson, Hugh R. (1993). Using geochemical data: evaluation, presentation, interpretation. Harlow: Longman. ISBN 978-0-582-06701-1. OCLC 27937350.[page needed]
 White, W. M. (2003). "Basics of Radioactive Isotope Geochemistry" (PDF). Cornell University.
 "Geologic Time: Radiometric Time Scale". United States Geological Survey. 16 June 2001.
 Stacey, J. S.; J. D. Kramers (June 1975). "Approximation of terrestrial lead isotope evolution by a two-stage model". Earth and Planetary Science Letters. 26 (2): 207â221. Bibcode:1975E&PSL..26..207S. doi:10.1016/0012-821X(75)90088-6.
 Vinyu, M. L.; R. E. Hanson; M. W. Martin; S. A. Bowring; H. A. Jelsma; P. H. G. M. Dirks (2001). "U-Pb zircon ages from a craton-margin archaean orogenic belt in northern Zimbabwe". Journal of African Earth Sciences. 32 (1): 103â114. Bibcode:2001JAfES..32..103V. doi:10.1016/S0899-5362(01)90021-1.
 OberthÃ¼r, T, Davis, DW, Blenkinsop, TG, Hoehndorf, A (2002). "Precise UâPb mineral ages, RbâSr and SmâNd systematics for the Great Dyke, Zimbabweâconstraints on late Archean events in the Zimbabwe craton and Limpopo belt". Precambrian Research. 113 (3â4): 293â306. Bibcode:2002PreR..113..293O. doi:10.1016/S0301-9268(01)00215-7.
 Manyeruke, Tawanda D.; Thomas G. Blenkinsop; Peter Buchholz; David Love; Thomas OberthÃ¼r; Ulrich K. Vetter; Donald W. Davis (2004). "The age and petrology of the Chimbadzi Hill Intrusion, NW Zimbabwe: first evidence for early Paleoproterozoic magmatism in Zimbabwe". Journal of African Earth Sciences. 40 (5): 281â292. Bibcode:2004JAfES..40..281M. doi:10.1016/j.jafrearsci.2004.12.003.
 Li, Xian-hua; Liang, Xi-rong; Sun, Min; Guan, Hong; Malpas, J. G. (2001). "Precise 206Pb/238U age determination on zircons by laser ablation microprobe-inductively coupled plasma-mass spectrometry using continuous linear ablation". Chemical Geology. 175 (3â4): 209â219. Bibcode:2001ChGeo.175..209L. doi:10.1016/S0009-2541(00)00394-6.
 Wingate, M.T.D. (2001). "SHRIMP baddeleyite and zircon ages for an Umkondo dolerite sill, Nyanga Mountains, Eastern Zimbabwe". South African Journal of Geology. 104 (1): 13â22. doi:10.2113/104.1.13.
 Ireland, Trevor (December 1999). "Isotope Geochemistry: New Tools for Isotopic Analysis". Science. 286 (5448): 2289â2290. doi:10.1126/science.286.5448.2289. S2CID 129408440.
 Mukasa, S. B.; A. H. Wilson; R. W. Carlson (December 1998). "A multielement geochronologic study of the Great Dyke, Zimbabwe: significance of the robust and reset ages". Earth and Planetary Science Letters. 164 (1â2): 353â369. Bibcode:1998E&PSL.164..353M. doi:10.1016/S0012-821X(98)00228-3.
 Tillberg, M., Drake, H., Zack, T. et al. In situ Rb-Sr dating of slickenfibres in deep crystalline basement faults. Sci Rep 10, 562 (2020). https://doi.org/10.1038/s41598-019-57262-5
 "Ales stenar". The Swedish National Heritage Board. 11 October 2006. Archived from the original on 31 March 2009. Retrieved 9 March 2009.
 Clark, R. M. (1975). "A calibration curve for radiocarbon dates". Antiquity. 49 (196): 251â266. doi:10.1017/S0003598X00070277.
 Vasiliev, S. S.; V. A. Dergachev (2002). "The ~2400-year cycle in atmospheric radiocarbon concentration: Bispectrum of 14C data over the last 8000 years" (PDF). Annales Geophysicae. 20 (1): 115â120. Bibcode:2002AnGeo..20..115V. doi:10.5194/angeo-20-115-2002.
 "Carbon-14 Dating". www.chem.uwec.edu. Retrieved 6 April 2016.
 Plastino, Wolfango; Lauri Kaihola; Paolo Bartolomei; Francesco Bella (2001). "Cosmic background reduction in the radiocarbon measurement by scintillation spectrometry at the underground laboratory of Gran Sasso" (PDF). Radiocarbon. 43 (2A): 157â161. doi:10.1017/S0033822200037954.
 Jacobs, J.; R. J. Thomas (August 2001). "A titanite fission track profile across the southeastern ArchÃ¦an Kaapvaal Craton and the Mesoproterozoic Natal Metamorphic Province, South Africa: evidence for differential cryptic Meso- to Neoproterozoic tectonism". Journal of African Earth Sciences. 33 (2): 323â333. Bibcode:2001JAfES..33..323J. doi:10.1016/S0899-5362(01)80066-X.
 Application of the authigenic 10 Be/ 9 Be dating method to Late MioceneâPliocene sequences in the northern Danube Basin;Michal Å ujan â Global and Planetary Change 137 (2016) 35â53; pdf
 Imke de Pater and Jack J. Lissauer: Planetary Sciences, page 321. Cambridge University Press, 2001. ISBN 0-521-48219-4
 Gilmour, J. D.; O. V Pravdivtseva; A. Busfield; C. M. Hohenberg (2006). "The I-Xe Chronometer and the Early Solar System". Meteoritics and Planetary Science. 41 (1): 19â31. Bibcode:2006M&PS...41...19G. doi:10.1111/j.1945-5100.2006.tb00190.x.
 Alexander N. Krot(2002) Dating the Earliest Solids in our Solar System, Hawai'i Institute of Geophysics and Planetology http://www.psrd.hawaii.edu/Sept02/isotopicAges.html.
 Imke de Pater and Jack J. Lissauer: Planetary Sciences, page 322. Cambridge University Press, 2001. ISBN 0-521-48219-4
Further reading
Gunten, Hans R. von (1995). "Radioactivity: A Tool to Explore the Past" (PDF). Radiochimica Acta. 70â71 (s1). doi:10.1524/ract.1995.7071.special-issue.305. ISSN 2193-3405. S2CID 100441969.
Magill, Joseph; Galy, Jean (2005). "Archaeology and Dating". Radioactivity Radionuclides Radiation. Springer Berlin Heidelberg. pp. 105â115. Bibcode:2005rrr..book.....M. doi:10.1007/3-540-26881-2_6. ISBN 978-3-540-26881-9.
AllÃ¨gre, Claude J (4 December 2008). Isotope Geology. ISBN 978-0521862288.
McSween, Harry Y; Richardson, Steven Mcafee; Uhle, Maria E; Uhle, Professor Maria (2003). Geochemistry: Pathways and Processes (2 ed.). ISBN 978-0-231-12440-9.
Harry y. Mcsween, Jr; Huss, Gary R (29 April 2010). Cosmochemistry. ISBN 978-0-521-87862-3.
vte
Chronology
Authority control Edit this at Wikidata
GND: 4277253-9LCCN: sh85110596
Categories: Radiometric datingConservation and restoration of cultural heritage
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Ø§Ø±Ø¯Ù
Tiáº¿ng Viá»t
ä¸­æ
28 more
Edit links
This page was last edited on 10 October 2020, at 03:15 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

This is a good article. Click here for more information.
Paleontology
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"Palaeontology" redirects here. For the scientific journal, see Palaeontology (journal).
Part of a series on
Paleontology
Palais de la Decouverte Tyrannosaurus rex p1050042.jpg
Fossils[show]
Natural history[show]
Organs and processes[show]
Evolution of various taxa[show]
Evolution[show]
History of paleontology[show]
Branches of paleontology[show]
Paleontology Portal
Category
vte

A paleontologist at work at John Day Fossil Beds National Monument
Paleontology, also spelled palaeontology or palÃ¦ontology (/ËpeÉªliÉnËtÉlÉdÊi, ËpÃ¦li-, -Én-/), is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene Epoch (roughly 11,700 years before present). It includes the study of fossils to classify organisms and study interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BCE. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek ÏÎ±Î»Î±Î¹ÏÏ, palaios, "old, ancient", á½Î½, on (gen. ontos), "being, creature", and Î»ÏÎ³Î¿Ï, logos, "speech, thought, study".[1]

Paleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, about 3.8 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.

Body fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the "jigsaw puzzles" of biostratigraphy (arrangement of rock layers from youngest to oldest). Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary "family trees". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring the similarity of the DNA in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend.


Contents
1	Overview
1.1	Historical science
1.2	Related sciences
1.3	Subdivisions
2	Sources of evidence
2.1	Body fossils
2.2	Trace fossils
2.3	Geochemical observations
3	Classifying ancient organisms
4	Estimating the dates of organisms
5	History of life
5.1	Mass extinctions
6	History
7	See also
8	References
9	External links
Overview
The simplest definition of "paleontology" is "the study of ancient life".[2] The field seeks information about several aspects of past organisms: "their identity and origin, their environment and evolution, and what they can tell us about the Earth's organic and inorganic past".[3]

Historical science

The preparation of the fossilised bones of Europasaurus holgeri
William Whewell (1794â1866) classified paleontology as one of the historical sciences, along with archaeology, geology, astronomy, cosmology, philology and history itself:[4] paleontology aims to describe phenomena of the past and to reconstruct their causes.[5] Hence it has three main elements: description of past phenomena; developing a general theory about the causes of various types of change; and applying those theories to specific facts.[6] When trying to explain the past, paleontologists and other historical scientists often construct a set of one or more hypotheses about the causes and then look for a "smoking gun", a piece of evidence that strongly accords with one hypothesis over any others.[7] Sometimes researchers discover a "smoking gun" by a fortunate accident during other research. For example, the 1980 discovery by Luis and Walter Alvarez of iridium, a mainly extraterrestrial metal, in the CretaceousâTertiary boundary layer made asteroid impact the most favored explanation for the CretaceousâPaleogene extinction event â although debate continues about the contribution of volcanism.[5]

A complementary approach to developing scientific knowledge, experimental science,[8] is often said[by whom?] to work by conducting experiments to disprove hypotheses about the workings and causes of natural phenomena. This approach cannot prove a hypothesis, since some later experiment may disprove it, but the accumulation of failures to disprove is often compelling evidence in favor. However, when confronted with totally unexpected phenomena, such as the first evidence for invisible radiation, experimental scientists often use the same approach as historical scientists: construct a set of hypotheses about the causes and then look for a "smoking gun".[5]

Related sciences
Hominin timeline
This box: viewtalkedit
-10 ââ-9 ââ-8 ââ-7 ââ-6 ââ-5 ââ-4 ââ-3 ââ-2 ââ-1 ââ0 â
Hominini
Nakalipithecus
Ouranopithecus
Oreopithecus
Sahelanthropus
Orrorin
Ardipithecus
Australopithecus
Homo habilis
Homo erectus
H. heidelbergensis
Homo sapiens
Neanderthals
â
Earlier apes
â
Gorilla split
â
Possibly bipedal
â
Chimpanzee split
â
Earliest bipedal
â
Stone tools
â
Expansion beyond Africa
â
Earliest fire use
â
Earliest cooking
â
Earliest clothes
â
Modern humans

P
l
e
i
s
t
o
c
e
n
e

P
l
i
o
c
e
n
e

M
i
o
c
e
n
e

H

o

m

i

n

i

d

s
(million years ago)
Life timeline
This box: viewtalkedit
-4500 ââ-4000 ââ-3500 ââ-3000 ââ-2500 ââ-2000 ââ-1500 ââ-1000 ââ-500 ââ0 â
Water
Single-celled
life
Photosynthesis
Eukaryotes
Multicellular
life
Arthropods Molluscs
Plants
Dinosaurs
Mammals
Flowers
Birds
Primates











â
Earth (â4540)
â
Earliest water
â
Earliest life
â
LHB meteorites
â
Earliest oxygen
â
Atmospheric oxygen
â
Oxygen crisis
â
Earliest fungi
â
Sexual reproduction
â
Earliest plants
â
Earliest animals
â
Ediacaran
â
Cambrian
â
Tetrapoda
â
Earliest apes
P
h
a
n
e
r
o
z
o
i
c







P
r
o
t
e
r
o
z
o
i
c



A
r
c
h
e
a
n
H
a
d
e
a
n
Pongola
Huronian
Cryogenian
Andean
Karoo
Quaternary
Ice Ages
(million years ago)
Paleontology lies between biology and geology since it focuses on the record of past life, but its main source of evidence is fossils in rocks.[9][10] For historical reasons, paleontology is part of the geology department at many universities: in the 19th and early 20th centuries, geology departments found fossil evidence important for dating rocks, while biology departments showed little interest.[11]

Paleontology also has some overlap with archaeology, which primarily works with objects made by humans and with human remains, while paleontologists are interested in the characteristics and evolution of humans as a species. When dealing with evidence about humans, archaeologists and paleontologists may work together â for example paleontologists might identify animal or plant fossils around an archaeological site, to discover what the people who lived there ate; or they might analyze the climate at the time of habitation.[12]

In addition, paleontology often borrows techniques from other sciences, including biology, osteology, ecology, chemistry, physics and mathematics.[2] For example, geochemical signatures from rocks may help to discover when life first arose on Earth,[13] and analyses of carbon isotope ratios may help to identify climate changes and even to explain major transitions such as the PermianâTriassic extinction event.[14] A relatively recent discipline, molecular phylogenetics, compares the DNA and RNA of modern organisms to re-construct the "family trees" of their evolutionary ancestors. It has also been used to estimate the dates of important evolutionary developments, although this approach is controversial because of doubts about the reliability of the "molecular clock".[15] Techniques from engineering have been used to analyse how the bodies of ancient organisms might have worked, for example the running speed and bite strength of Tyrannosaurus,[16][17] or the flight mechanics of Microraptor.[18] It is relatively commonplace to study the internal details of fossils using X-ray microtomography.[19][20] Paleontology, biology, archaeology, and paleoneurobiology combine to study endocranial casts (endocasts) of species related to humans to clarify the evolution of the human brain.[21]

Paleontology even contributes to astrobiology, the investigation of possible life on other planets, by developing models of how life may have arisen and by providing techniques for detecting evidence of life.[22]

Subdivisions
As knowledge has increased, paleontology has developed specialised subdivisions.[23] Vertebrate paleontology concentrates on fossils from the earliest fish to the immediate ancestors of modern mammals. Invertebrate paleontology deals with fossils such as molluscs, arthropods, annelid worms and echinoderms. Paleobotany studies fossil plants, algae, and fungi. Palynology, the study of pollen and spores produced by land plants and protists, straddles paleontology and botany, as it deals with both living and fossil organisms. Micropaleontology deals with microscopic fossil organisms of all kinds.[24]


Analyses using engineering techniques show that Tyrannosaurus had a devastating bite, but raise doubts about its running ability.
Instead of focusing on individual organisms, paleoecology examines the interactions between different ancient organisms, such as their food chains, and the two-way interactions with their environments.[25]  For example, the development of oxygenic photosynthesis by bacteria caused the oxygenation of the atmosphere and hugely increased the productivity and diversity of ecosystems.[26] Together, these led to the evolution of complex eukaryotic cells, from which all multicellular organisms are built.[27]

Paleoclimatology, although sometimes treated as part of paleoecology,[24] focuses more on the history of Earth's climate and the mechanisms that have changed it[28] â which have sometimes included evolutionary developments, for example the rapid expansion of land plants in the Devonian period removed more carbon dioxide from the atmosphere, reducing the greenhouse effect and thus helping to cause an ice age in the Carboniferous period.[29]

Biostratigraphy, the use of fossils to work out the chronological order in which rocks were formed, is useful to both paleontologists and geologists.[30] Biogeography studies the spatial distribution of organisms, and is also linked to geology, which explains how Earth's geography has changed over time.[31]

Sources of evidence
Body fossils
Main article: Fossil

This Marrella specimen illustrates how clear and detailed the fossils from the Burgess Shale lagerstÃ¤tte are.
Fossils of organisms' bodies are usually the most informative type of evidence. The most common types are wood, bones, and shells.[32] Fossilisation is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence the fossil record is very incomplete, increasingly so further back in time. Despite this, it is often adequate to illustrate the broader patterns of life's history.[33] There are also biases in the fossil record: different environments are more favorable to the preservation of different types of organism or parts of organisms.[34] Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although there are 30-plus phyla of living animals, two-thirds have never been found as fossils.[2]

Occasionally, unusual environments may preserve soft tissues. These lagerstÃ¤tten allow paleontologists to examine the internal anatomy of animals that in other sediments are represented only by shells, spines, claws, etc. â if they are preserved at all. However, even lagerstÃ¤tten present an incomplete picture of life at the time. The majority of organisms living at the time are probably not represented because lagerstÃ¤tten are restricted to a narrow range of environments, e.g. where soft-bodied organisms can be preserved very quickly by events such as mudslides; and the exceptional events that cause quick burial make it difficult to study the normal environments of the animals.[35] The sparseness of the fossil record means that organisms are expected to exist long before and after they are found in the fossil record â this is known as the SignorâLipps effect.[36]

Trace fossils

Cambrian trace fossils including Rusophycus, made by a trilobite.

Climactichnites---Cambrian trackways (10-12 cm wide) from large, slug-like animals on a Cambrian tidal flat in what is now Wisconsin.
Main article: Trace fossil
Trace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding.[32][37] Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilised hard parts, and they reflect organisms' behaviours. Also many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them.[38] Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).[37]

Geochemical observations
Main article: Geochemistry
Geochemical observations may help to deduce the global level of biological activity at a certain period, or the affinity of certain fossils. For example, geochemical features of rocks may reveal when life first arose on Earth,[13] and may provide evidence of the presence of eukaryotic cells, the type from which all multicellular organisms are built.[39] Analyses of carbon isotope ratios may help to explain major transitions such as the PermianâTriassic extinction event.[14]

Classifying ancient organisms
Main articles: Biological classification, Cladistics, Phylogenetic nomenclature, and Evolutionary taxonomy

Levels in the Linnaean taxonomy.
Naming groups of organisms in a way that is clear and widely agreed is important, as some disputes in paleontology have been based just on misunderstandings over names.[40] Linnaean taxonomy is commonly used for classifying living organisms, but runs into difficulties when dealing with newly discovered organisms that are significantly different from known ones. For example: it is hard to decide at what level to place a new higher-level grouping, e.g. genus or family or order; this is important since the Linnaean rules for naming groups are tied to their levels, and hence if a group is moved to a different level it must be renamed.[41]

Tetrapods
â¯
Amphibians

â¯
Amniotes
Synapsids
â¯
Extinct Synapsids

â¯

Mammals

â¯
â¯
Reptiles
â¯
Extinct reptiles

â¯
â¯
Lizards and snakes

â¯
Archosaurs
â¯
Extinct
Archosaurs

â¯
â¯
Crocodilians

â¯
Dinosaurs
 ?
â¯
Extinct
Dinosaurs

â¯

 ?
Birds

â¯
â¯
â¯
â¯
â¯
â¯
Simple example cladogram
    Warm-bloodedness evolved somewhere in the
synapsidâmammal transition.
 ?  Warm-bloodedness must also have evolved at one of
these points â an example of convergent evolution.[2]
Paleontologists generally use approaches based on cladistics, a technique for working out the evolutionary "family tree" of a set of organisms.[40] It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characters that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or proteins. The result of a successful analysis is a hierarchy of clades â groups that share a common ancestor. Ideally the "family tree" has only two branches leading from each node ("junction"), but sometimes there is too little information to achieve this and paleontologists have to make do with junctions that have several branches. The cladistic technique is sometimes fallible, as some features, such as wings or camera eyes, evolved more than once, convergently â this must be taken into account in analyses.[2]

Evolutionary developmental biology, commonly abbreviated to "Evo Devo", also helps paleontologists to produce "family trees", and understand fossils.[42] For example, the embryological development of some modern brachiopods suggests that brachiopods may be descendants of the halkieriids, which became extinct in the Cambrian period.[43]

Estimating the dates of organisms
Main article: Geochronology

CenozoicMesozoicPaleozoicProterozoicQuater-
naryTertiaryCreta-
ceousJurassicTriassicPermianMissis-
sippianPennsyl-
vanianDevo-
nianSilurianOrdo-
vicianCamb-
rianPecten gibbusCalyptraphorus
velatusScaphites
hippocrepisPerisphinctes
tizianiTropites
subbullatusLeptodus
americanusCactocrinus
multibrachiatusDictyoclostus
americanusMucrospirifer
mucronatusCystiphyllum
niagarenseBathyurus extansParadoxides pinusNeptunea tabulataVenericardia
planicostaInoceramus
labiatusNerinea trinodosaMonotis
subcircularisParafusulina
boseiLophophyllidium
proliferumProlecanites gurleyiPalmatolepus
unicornisHexamocaras hertzeriTetragraptus fructicosusBillingsella corrugata
Common index fossils used to date rocks in the northeast United States.
Paleontology seeks to map out how living things have changed through time. A substantial hurdle to this aim is the difficulty of working out how old fossils are. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better.[44] Although radiometric dating requires very careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to the element into which it decays shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are a few volcanic ash layers.[44]

Consequently, paleontologists must usually rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the "layer-cake" that is the sedimentary record, and has been compared to a jigsaw puzzle.[45] Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age must lie between the two known ages.[46] Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly next to one another. However, fossils of species that survived for a relatively short time can be used to link up isolated rocks: this technique is called biostratigraphy. For instance, the conodont Eoplacognathus pseudoplanus has a short range in the Middle Ordovician period.[47] If rocks of unknown age are found to have traces of E. pseudoplanus, they must have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and have a short time range to be useful. However, misleading results are produced if the index fossils turn out to have longer fossil ranges than first thought.[48] Stratigraphy and biostratigraphy can in general provide only relative dating (A was before B), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching up rocks of the same age across different continents.[48]

Family-tree relationships may also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated "family tree" says A was an ancestor of B and C, then A must have evolved more than X million years ago.

It is also possible to estimate how long ago two living clades diverged â i.e. approximately how long ago their last common ancestor must have lived â by assuming that DNA mutations accumulate at a constant rate. These "molecular clocks", however, are fallible, and provide only a very approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved,[49] and estimates produced by different techniques may vary by a factor of two.[15]

History of life

This wrinkled "elephant skin" texture is a trace fossil of a non-stromatolite microbial mat. The image shows the location, in the Burgsvik beds of Sweden, where the texture was first identified as evidence of a microbial mat.[50]
Main article: Evolutionary history of life
Further information: Timeline of evolutionary history of life
Earth formed about 4,570 million years ago and, after a collision that formed the Moon about 40 million years later, may have cooled quickly enough to have oceans and an atmosphere about 4,440 million years ago.[51] There is evidence on the Moon of a Late Heavy Bombardment by asteroids from 4,000 to 3,800 million years ago. If, as seems likely, such a bombardment struck Earth at the same time, the first atmosphere and oceans may have been stripped away.[52]

Paleontology traces the evolutionary history of life back to over 3,000 million years ago, possibly as far as 3,800 million years ago.[53] The oldest clear evidence of life on Earth dates to 3,000 million years ago, although there have been reports, often disputed, of fossil bacteria from 3,400 million years ago and of geochemical evidence for the presence of life 3,800 million years ago.[13][54] Some scientists have proposed that life on Earth was "seeded" from elsewhere,[55] but most research concentrates on various explanations of how life could have arisen independently on Earth.[56]

For about 2,000 million years microbial mats, multi-layered colonies of different bacteria, were the dominant life on Earth.[57] The evolution of oxygenic photosynthesis enabled them to play the major role in the oxygenation of the atmosphere[26] from about 2,400 million years ago. This change in the atmosphere increased their effectiveness as nurseries of evolution.[58] While eukaryotes, cells with complex internal structures, may have been present earlier, their evolution speeded up when they acquired the ability to transform oxygen from a poison to a powerful source of metabolic energy. This innovation may have come from primitive eukaryotes capturing oxygen-powered bacteria as endosymbionts and transforming them into organelles called mitochondria.[53][59] The earliest evidence of complex eukaryotes with organelles (such as mitochondria) dates from 1,850 million years ago.[27]


Opabinia sparked modern interest in the Cambrian explosion.
Multicellular life is composed only of eukaryotic cells, and the earliest evidence for it is the Francevillian Group Fossils from 2,100 million years ago,[60] although specialisation of cells for different functions first appears between 1,430 million years ago (a possible fungus) and 1,200 million years ago (a probable red alga). Sexual reproduction may be a prerequisite for specialisation of cells, as an asexual multicellular organism might be at risk of being taken over by rogue cells that retain the ability to reproduce.[61][62]

The earliest known animals are cnidarians from about 580 million years ago, but these are so modern-looking that must be descendants of earlier animals.[63] Early fossils of animals are rare because they had not developed mineralised, easily fossilized hard parts until about 548 million years ago.[64] The earliest modern-looking bilaterian animals appear in the Early Cambrian, along with several "weird wonders" that bear little obvious resemblance to any modern animals. There is a long-running debate about whether this Cambrian explosion was truly a very rapid period of evolutionary experimentation; alternative views are that modern-looking animals began evolving earlier but fossils of their precursors have not yet been found, or that the "weird wonders" are evolutionary "aunts" and "cousins" of modern groups.[65] Vertebrates remained a minor group until the first jawed fish appeared in the Late Ordovician.[66][67]


At about 13 centimetres (5.1 in) the Early Cretaceous Yanoconodon was longer than the average mammal of the time.[68]
The spread of animals and plants from water to land required organisms to solve several problems, including protection against drying out and supporting themselves against gravity.[69][70][71][72] The earliest evidence of land plants and land invertebrates date back to about 476 million years ago and 490 million years ago respectively.[71][73] Those invertebrates, as indicated by their trace and body fossils, were shown to be arthropods known as euthycarcinoids.[74] The lineage that produced land vertebrates evolved later but very rapidly between 370 million years ago and 360 million years ago;[75] recent discoveries have overturned earlier ideas about the history and driving forces behind their evolution.[76] Land plants were so successful that their detritus caused an ecological crisis in the Late Devonian, until the evolution of fungi that could digest dead wood.[29]


Birds are the only surviving dinosaurs.[77]
During the Permian period, synapsids, including the ancestors of mammals, may have dominated land environments,[78] but this ended with the PermianâTriassic extinction event 251 million years ago, which came very close to wiping out all complex life.[79] The extinctions were apparently fairly sudden, at least among vertebrates.[80] During the slow recovery from this catastrophe a previously obscure group, archosaurs, became the most abundant and diverse terrestrial vertebrates. One archosaur group, the dinosaurs, were the dominant land vertebrates for the rest of the Mesozoic,[81] and birds evolved from one group of dinosaurs.[77] During this time mammals' ancestors survived only as small, mainly nocturnal insectivores, which may have accelerated the development of mammalian traits such as endothermy and hair.[82] After the CretaceousâPaleogene extinction event 66 million years ago[83] killed off all the dinosaurs except the birds, mammals increased rapidly in size and diversity, and some took to the air and the sea.[84][85][86]

Fossil evidence indicates that flowering plants appeared and rapidly diversified in the Early Cretaceous between 130 million years ago and 90 million years ago.[87] Their rapid rise to dominance of terrestrial ecosystems is thought to have been propelled by coevolution with pollinating insects.[88] Social insects appeared around the same time and, although they account for only small parts of the insect "family tree", now form over 50% of the total mass of all insects.[89]

Humans evolved from a lineage of upright-walking apes whose earliest fossils date from over 6 million years ago.[90] Although early members of this lineage had chimp-sized brains, about 25% as big as modern humans', there are signs of a steady increase in brain size after about 3 million years ago.[91] There is a long-running debate about whether modern humans are descendants of a single small population in Africa, which then migrated all over the world less than 200,000 years ago and replaced previous hominine species, or arose worldwide at the same time as a result of interbreeding.[92]

Mass extinctions
Extinction intensity.svg
Marine extinction intensity during the Phanerozoic%Millions of years ago(H)KâPgTrâJPâTrCapLate DOâS
Apparent extinction intensity, i.e. the fraction of genera going extinct at any given time, as reconstructed from the fossil record (graph not meant to include recent epoch of Holocene extinction event)
Main article: Mass extinction
Life on earth has suffered occasional mass extinctions at least since 542 million years ago. Despite their disastrous effects, mass extinctions have sometimes accelerated the evolution of life on earth. When dominance of an ecological niche passes from one group of organisms to another, this is rarely because the new dominant group outcompetes the old, but usually because an extinction event allows new group to outlive the old and move into its niche.[93][94]

The fossil record appears to show that the rate of extinction is slowing down, with both the gaps between mass extinctions becoming longer and the average and background rates of extinction decreasing. However, it is not certain whether the actual rate of extinction has altered, since both of these observations could be explained in several ways:[95]

The oceans may have become more hospitable to life over the last 500 million years and less vulnerable to mass extinctions: dissolved oxygen became more widespread and penetrated to greater depths; the development of life on land reduced the run-off of nutrients and hence the risk of eutrophication and anoxic events; marine ecosystems became more diversified so that food chains were less likely to be disrupted.[96][97]
Reasonably complete fossils are very rare: most extinct organisms are represented only by partial fossils, and complete fossils are rarest in the oldest rocks. So paleontologists have mistakenly assigned parts of the same organism to different genera, which were often defined solely to accommodate these finds â the story of Anomalocaris is an example of this.[98] The risk of this mistake is higher for older fossils because these are often unlike parts of any living organism. Many "superfluous" genera are represented by fragments that are not found again, and these "superfluous" genera are interpreted as becoming extinct very quickly.[95]

All genera"Well-defined" generaTrend line"Big Five" mass extinctionsOther mass extinctionsMillion years agoThousands of genera
Phanerozoic biodiversity as shown by the fossil record
Biodiversity in the fossil record, which is

"the number of distinct genera alive at any given time; that is, those whose first occurrence predates and whose last occurrence postdates that time"[99]
shows a different trend: a fairly swift rise from 542 to 400 million years ago, a slight decline from 400 to 200 million years ago, in which the devastating PermianâTriassic extinction event is an important factor, and a swift rise from 200 million years ago to the present.[99]

History
Main article: History of paleontology
Further information: Timeline of paleontology

This illustration of an Indian elephant jaw and a mammoth jaw (top) is from Cuvier's 1796 paper on living and fossil elephants.
Although paleontology became established around 1800, earlier thinkers had noticed aspects of the fossil record. The ancient Greek philosopher Xenophanes (570â480 BCE) concluded from fossil sea shells that some areas of land were once under water.[100] During the Middle Ages the Persian naturalist Ibn Sina, known as Avicenna in Europe, discussed fossils and proposed a theory of petrifying fluids on which Albert of Saxony elaborated in the 14th century.[100] The Chinese naturalist Shen Kuo (1031â1095) proposed a theory of climate change based on the presence of petrified bamboo in regions that in his time were too dry for bamboo.[101]

In early modern Europe, the systematic study of fossils emerged as an integral part of the changes in natural philosophy that occurred during the Age of Reason. In the Italian Renaissance, Leonardo Da Vinci made various significant contributions to the field as well depicted numerous fossils. Leonardo's contributions are central to the history of paleontology because he established a line of continuity between the two main branches of paleontology â ichnology and body fossil paleontology.[102][103][104] He identified the following:[102]

The biogenic nature of ichnofossils, i.e. ichnofossils were structures left by living organisms;
The utility of ichnofossils as paleoenvironmental tools â certain ichnofossils show the marine origin of rock strata;
The importance of the neoichnological approach â recent traces are a key to understanding ichnofossils;
The independence and complementary evidence of ichnofossils and body fossils â ichnofossils are distinct from body fossils, but can be integrated with body fossils to provide paleontological information
At the end of the 18th century Georges Cuvier's work established comparative anatomy as a scientific discipline and, by proving that some fossil animals resembled no living ones, demonstrated that animals could become extinct, leading to the emergence of paleontology.[105] The expanding knowledge of the fossil record also played an increasing role in the development of geology, particularly stratigraphy.[106]

The first half of the 19th century saw geological and paleontological activity become increasingly well organised with the growth of geologic societies and museums[107][108] and an increasing number of professional geologists and fossil specialists. Interest increased for reasons that were not purely scientific, as geology and paleontology helped industrialists to find and exploit natural resources such as coal.[100] This contributed to a rapid increase in knowledge about the history of life on Earth and to progress in the definition of the geologic time scale, largely based on fossil evidence. In 1822 Henri Marie Ducrotay de Blanville, editor of Journal de Physique, coined the word "palaeontology" to refer to the study of ancient living organisms through fossils.[109] As knowledge of life's history continued to improve, it became increasingly obvious that there had been some kind of successive order to the development of life. This encouraged early evolutionary theories on the transmutation of species.[110] After Charles Darwin published Origin of Species in 1859, much of the focus of paleontology shifted to understanding evolutionary paths, including human evolution, and evolutionary theory.[110]


Haikouichthys, from about 518 million years ago in China, may be the earliest known fish.[111]
The last half of the 19th century saw a tremendous expansion in paleontological activity, especially in North America.[112] The trend continued in the 20th century with additional regions of the Earth being opened to systematic fossil collection. Fossils found in China near the end of the 20th century have been particularly important as they have provided new information about the earliest evolution of animals, early fish, dinosaurs and the evolution of birds.[113] The last few decades of the 20th century saw a renewed interest in mass extinctions and their role in the evolution of life on Earth.[114] There was also a renewed interest in the Cambrian explosion that apparently saw the development of the body plans of most animal phyla. The discovery of fossils of the Ediacaran biota and developments in paleobiology extended knowledge about the history of life back far before the Cambrian.[65]

Increasing awareness of Gregor Mendel's pioneering work in genetics led first to the development of population genetics and then in the mid-20th century to the modern evolutionary synthesis, which explains evolution as the outcome of events such as mutations and horizontal gene transfer, which provide genetic variation, with genetic drift and natural selection driving changes in this variation over time.[114] Within the next few years the role and operation of DNA in genetic inheritance were discovered, leading to what is now known as the "Central Dogma" of molecular biology.[115] In the 1960s molecular phylogenetics, the investigation of evolutionary "family trees" by techniques derived from biochemistry, began to make an impact, particularly when it was proposed that the human lineage had diverged from apes much more recently than was generally thought at the time.[116] Although this early study compared proteins from apes and humans, most molecular phylogenetics research is now based on comparisons of RNA and DNA.[117]

See also
Biostratigraphy
European land mammal age
Fossil collecting â collecting fossils to study, collect or sell
List of fossil sites â A table of worldwide localities notable for the presence of fossils (with link directory)
List of notable fossils
List of paleontologists
List of transitional fossils
Paleoanthropology â Study of ancient humans
Paleobotany
Paleogenetics
Paleontographer
Paleophycology â The study and identification of fossil algae
Radiometric dating â Technique used to date materials such as rocks or carbon
Taxonomy of commonly fossilised invertebrates
Treatise on Invertebrate Paleontology â 1953 geology book
References
 "paleontology". Online Etymology Dictionary. Archived from the original on 2013-03-07.
 Cowen, R. (2000). History of Life (3rd ed.). Blackwell Science. pp. xi, 47â50, 61. ISBN 0-632-04444-6.
 Laporte, L.F. (October 1988). "What, after All, Is Paleontology?". PALAIOS. 3 (5): 453. Bibcode:1988Palai...3..453L. doi:10.2307/3514718. JSTOR 3514718.
 Laudan, R. (1992). "What's so Special about the Past?". In Nitecki, M.H.; Nitecki, D.V. (eds.). History and Evolution. SUNY Press. p. 58. ISBN 0-7914-1211-3. To structure my discussion of the historical sciences, I shall borrow a way of analyzing them from the great Victorian philosopher of science, William Whewell [...]. [...] while his analysis of the historical sciences (or as Whewell termed them, the palaetiological sciences) will doubtless need to be modified, it provides a good starting point. Among them he numbered geology, paleontology, cosmogony, philology, and what we would term archaeology and history.
 Cleland, C.E. (September 2002). "Methodological and Epistemic Differences between Historical Science and Experimental Science". Philosophy of Science. 69 (3): 474â96. doi:10.1086/342453. Archived from the original on October 3, 2008. Retrieved September 17, 2008.
 Laudan, R. (1992). "What's so Special about the Past?". In Nitecki, M.H.; Nitecki, D.V. (eds.). History and Evolution. SUNY Press. p. 58. ISBN 0-7914-1211-3. [Whewell] distinguished three tasks for such a historical science (1837 [...]): ' the Description of the facts and phenomena; â the general Theory of the causes of change appropriate to the case; â and the Application of the theory to the facts.'
 Perreault, Charles (2019). "The Search for Smoking Guns". The Quality of the Archaeological Record. Chicago: University of Chicago Press. p. 5. ISBN 978-0226631011. Retrieved 9 January 2020. Historical scientists successfully learn about the past by employing a 'smoking-gun' approach. They start by formulating multiple, mutually exclusive hypotheses and then search for a âsmoking gunâ that discriminates between these hypotheses [...].
 "'Historical science' vs. 'experimental science'". National Center for Science Education. 25 October 2019. Retrieved 9 January 2020. Philosophers of science draw a distinction between research directed towards identifying laws and research which seeks to determine how particular historical events occurred. They do not claim, however, that the line between these sorts of science can be drawn neatly, and certainly do not agree that historical claims are any less empirically verifiable than other sorts of claims. [...] 'we can separate their two enterprises by distinguishing means from ends. The astronomer's problem is a historical one because the goal is to infer the properties of a particular object; the astronomer uses laws only as a means. Particle physics, on the other hand, is a nomothetic discipline because the goal is to infer general laws; descriptions of particular objects are only relevant as a means.'
 "paleontology | science". EncyclopÃ¦dia Britannica. Archived from the original on 2017-08-24. Retrieved 2017-08-24.
 McGraw-Hill Encyclopedia of Science & Technology. McGraw-Hill. 2002. p. 58. ISBN 0-07-913665-6.
 Laudan, R. (1992). "What's so Special about the Past?". In Nitecki, M.H.; Nitecki, D.V. (eds.). History and Evolution. SUNY Press. p. 57. ISBN 0-7914-1211-3.
 "How does paleontology differ from anthropology and archaeology?". University of California Museum of Paleontology. Archived from the original on September 16, 2008. Retrieved September 17, 2008.
 Brasier, M.; McLoughlin, N.; Green, O. & Wacey, D. (June 2006). "A fresh look at the fossil evidence for early Archaean cellular life" (PDF). Philosophical Transactions of the Royal Society B. 361 (1470): 887â902. doi:10.1098/rstb.2006.1835. PMC 1578727. PMID 16754605. Archived (PDF) from the original on September 11, 2008. Retrieved August 30, 2008.
 Twitchett R.J.; Looy C.V.; Morante R.; Visscher H.; Wignall P.B. (2001). "Rapid and synchronous collapse of marine and terrestrial ecosystems during the end-Permian biotic crisis". Geology. 29 (4): 351â54. Bibcode:2001Geo....29..351T. doi:10.1130/0091-7613(2001)029<0351:RASCOM>2.0.CO;2. S2CID 129908787.
 Peterson, Kevin J. & Butterfield, N.J. (2005). "Origin of the Eumetazoa: Testing ecological predictions of molecular clocks against the Proterozoic fossil record". Proceedings of the National Academy of Sciences. 102 (27): 9547â52. Bibcode:2005PNAS..102.9547P. doi:10.1073/pnas.0503660102. PMC 1172262. PMID 15983372.
 Hutchinson, J.R. & Garcia, M. (28 February 2002). "Tyrannosaurus was not a fast runner". Nature. 415 (6875): 1018â21. Bibcode:2002Natur.415.1018H. doi:10.1038/4151018a. PMID 11875567. S2CID 4389633. Summary in press release No Olympian: Analysis hints T. rex ran slowly, if at all Archived 2008-04-15 at the Wayback Machine
 Meers, M.B. (August 2003). "Maximum bite force and prey size of Tyrannosaurus rex and their relationships to the inference of feeding behavior". Historical Biology. 16 (1): 1â12. doi:10.1080/0891296021000050755. S2CID 86782853.
 "The Four Winged Dinosaur: Wind Tunnel Test". Nova. Retrieved June 5, 2010.
 Garwood, Russell J.; Rahman, Imran A.; Sutton, Mark D. A. (2010). "From clergymen to computers: the advent of virtual palaeontology". Geology Today. 26 (3): 96â100. doi:10.1111/j.1365-2451.2010.00753.x. Retrieved June 16, 2015.
 Mark Sutton; Imran Rahman; Russell Garwood (2013). Techniques for Virtual Palaeontology. Wiley. ISBN 978-1-118-59125-3.
 Bruner, Emiliano (November 2004). "Geometric morphometrics and palaeoneurology: brain shape evolution in the genus Homo". Journal of Human Evolution. 47 (5): 279â303. doi:10.1016/j.jhevol.2004.03.009. PMID 15530349.
 Cady, S.L. (April 1998). "Astrobiology: A New Frontier for 21st Century Paleontologists". PALAIOS. 13 (2): 95â97. Bibcode:1998Palai..13...95C. doi:10.2307/3515482. JSTOR 3515482. PMID 11542813.
 Plotnick, R.E. "A Somewhat Fuzzy Snapshot of Employment in Paleontology in the United States". Palaeontologia Electronica. Coquina Press. 11 (1). ISSN 1094-8074. Archived from the original on May 18, 2008. Retrieved September 17, 2008.
 "What is Paleontology?". University of California Museum of Paleontology. Archived from the original on August 3, 2008. Retrieved September 17, 2008.
 Kitchell, J.A. (1985). "Evolutionary Paleocology: Recent Contributions to Evolutionary Theory". Paleobiology. 11 (1): 91â104. doi:10.1017/S0094837300011428. Archived from the original on August 3, 2008. Retrieved September 17, 2008.
 Hoehler, T.M.; Bebout, B.M. & Des Marais, D.J. (19 July 2001). "The role of microbial mats in the production of reduced gases on the early Earth". Nature. 412 (6844): 324â27. Bibcode:2001Natur.412..324H. doi:10.1038/35085554. PMID 11460161. S2CID 4365775.
 Hedges, S.B.; Blair, J.E; Venturi, M.L. & Shoe, J.L. (January 2004). "A molecular timescale of eukaryote evolution and the rise of complex multicellular life". BMC Evolutionary Biology. 4: 2. doi:10.1186/1471-2148-4-2. PMC 341452. PMID 15005799.
 "Paleoclimatology". Ohio State University. Archived from the original on November 9, 2007. Retrieved September 17, 2008.
 Algeo, T.J. & Scheckler, S.E. (1998). "Terrestrial-marine teleconnections in the Devonian: links between the evolution of land plants, weathering processes, and marine anoxic events". Philosophical Transactions of the Royal Society B. 353 (1365): 113â30. doi:10.1098/rstb.1998.0195. PMC 1692181.
 "Biostratigraphy: William Smith". Archived from the original on July 24, 2008. Retrieved September 17, 2008.
 "Biogeography: Wallace and Wegener (1 of 2)". University of California Museum of Paleontology and University of California at Berkeley. Archived from the original on May 15, 2008. Retrieved September 17, 2008.
 "What is paleontology?". University of California Museum of Paleontology. Archived from the original on September 16, 2008. Retrieved September 17, 2008.
 Benton M.J.; Wills M.A.; Hitchin R. (2000). "Quality of the fossil record through time". Nature. 403 (6769): 534â37. Bibcode:2000Natur.403..534B. doi:10.1038/35000558. PMID 10676959. S2CID 4407172.
Non-technical summary Archived 2007-08-09 at the Wayback Machine
 Butterfield, N.J. (2003). "Exceptional Fossil Preservation and the Cambrian Explosion". Integrative and Comparative Biology. 43 (1): 166â77. doi:10.1093/icb/43.1.166. PMID 21680421.
 Butterfield, N.J. (2001). "Ecology and evolution of Cambrian plankton". The Ecology of the Cambrian Radiation. New York: Columbia University Press: 200â16. Retrieved September 27, 2007.[permanent dead link]
 Signor, P.W. (1982). "Sampling bias, gradual extinction patterns and catastrophes in the fossil record". Geological Implications of Impacts of Large Asteroids and Comets on the Earth. Geological Society of America Special Papers. Boulder, CO: Geological Society of America. 190: 291â96. doi:10.1130/SPE190-p291. ISBN 0-8137-2190-3. A 84â25651 10â42. Retrieved January 1, 2008.
 Fedonkin, M.A.; Gehling, J.G.; Grey, K.; Narbonne, G.M.; Vickers-Rich, P. (2007). The Rise of Animals: Evolution and Diversification of the Kingdom Animalia. JHU Press. pp. 213â16. ISBN 978-0-8018-8679-9.
 e.g. Seilacher, A. (1994). "How valid is Cruziana Stratigraphy?". International Journal of Earth Sciences. 83 (4): 752â58. Bibcode:1994GeoRu..83..752S. doi:10.1007/BF00251073. S2CID 129504434.
 Brocks, J.J.; Logan, G.A.; Buick, R. & Summons, R.E. (1999). "Archaean molecular fossils and the rise of eukaryotes". Science. 285 (5430): 1033â36. doi:10.1126/science.285.5430.1033. PMID 10446042. S2CID 11028394.
 Brochu, C.A & Sumrall, C.D. (July 2001). "Phylogenetic Nomenclature and Paleontology". Journal of Paleontology. 75 (4): 754â57. doi:10.1666/0022-3360(2001)075<0754:PNAP>2.0.CO;2. ISSN 0022-3360. JSTOR 1306999.
 Ereshefsky, M. (2001). The Poverty of the Linnaean Hierarchy: A Philosophical Study of Biological Taxonomy. Cambridge University Press. p. 5. ISBN 0-521-78170-1.
 Garwood, Russell J.; Sharma, Prashant P.; Dunlop, Jason A.; Giribet, Gonzalo (2014). "A Paleozoic Stem Group to Mite Harvestmen Revealed through Integration of Phylogenetics and Development". Current Biology. 24 (9): 1017â23. doi:10.1016/j.cub.2014.03.039. PMID 24726154. Retrieved April 17, 2014.
 Cohen, B.L.; Holmer, L.E. & Luter, C. (2003). "The brachiopod fold: a neglected body plan hypothesis" (PDF). Palaeontology. 46 (1): 59â65. doi:10.1111/1475-4983.00287. Archived from the original (PDF) on October 3, 2008. Retrieved August 7, 2008.
 Martin, M.W.; Grazhdankin, D.V.; Bowring, S.A.; Evans, D.A.D.; Fedonkin, M.A.; Kirschvink, J.L. (May 5, 2000). "Age of Neoproterozoic Bilaterian Body and Trace Fossils, White Sea, Russia: Implications for Metazoan Evolution". Science (abstract). 288 (5467): 841â45. Bibcode:2000Sci...288..841M. doi:10.1126/science.288.5467.841. PMID 10797002. S2CID 1019572.
 Pufahl, P.K.; Grimm, K.A.; Abed, A.M. & Sadaqah, R.M.Y. (October 2003). "Upper Cretaceous (Campanian) phosphorites in Jordan: implications for the formation of a south Tethyan phosphorite giant". Sedimentary Geology. 161 (3â4): 175â205. Bibcode:2003SedG..161..175P. doi:10.1016/S0037-0738(03)00070-8.
 "Geologic Time: Radiometric Time Scale". U.S. Geological Survey. Archived from the original on September 21, 2008. Retrieved September 20, 2008.
 LÃ¶fgren, A. (2004). "The conodont fauna in the Middle Ordovician Eoplacognathus pseudoplanus Zone of Baltoscandia". Geological Magazine. 141 (4): 505â24. Bibcode:2004GeoM..141..505L. doi:10.1017/S0016756804009227.
 Gehling, James; Jensen, SÃ¶ren; Droser, Mary; Myrow, Paul; Narbonne, Guy (March 2001). "Burrowing below the basal Cambrian GSSP, Fortune Head, Newfoundland". Geological Magazine. 138 (2): 213â18. Bibcode:2001GeoM..138..213G. doi:10.1017/S001675680100509X.
 Hug, L.A. & Roger, A.J. (2007). "The Impact of Fossils and Taxon Sampling on Ancient Molecular Dating Analyses". Molecular Biology and Evolution. 24 (8): 889â1897. doi:10.1093/molbev/msm115. PMID 17556757.
 Manten, A.A. (1966). "Some problematic shallow-marine structures". Marine Geol. 4 (3): 227â32. Bibcode:1966MGeol...4..227M. doi:10.1016/0025-3227(66)90023-5. hdl:1874/16526. Archived from the original on October 21, 2008. Retrieved June 18, 2007.
 * "Early Earth Likely Had Continents And Was Habitable". 2005-11-17. Archived from the original on 2008-10-14.
* Cavosie, A.J.; J.W. Valley, S.A., Wilde & E.I.M.F. (July 15, 2005). "Magmatic Î´18O in 4400â3900 Ma detrital zircons: A record of the alteration and recycling of crust in the Early Archean". Earth and Planetary Science Letters. 235 (3â4): 663â81. Bibcode:2005E&PSL.235..663C. doi:10.1016/j.epsl.2005.04.028.
 Dauphas, N.; Robert, F. & Marty, B. (December 2000). "The Late Asteroidal and Cometary Bombardment of Earth as Recorded in Water Deuterium to Protium Ratio". Icarus. 148 (2): 508â12. Bibcode:2000Icar..148..508D. doi:10.1006/icar.2000.6489. S2CID 85555707.
 Garwood, Russell J. (2012). "Patterns In Palaeontology: The first 3 billion years of evolution". Palaeontology Online. 2 (11): 1â14. Archived from the original on June 26, 2015. Retrieved June 25, 2015.
 Schopf, J. (2006). "Fossil evidence of Archaean life". Philos Trans R Soc Lond B Biol Sci. 361 (1470): 869â85. doi:10.1098/rstb.2006.1834. PMC 1578735. PMID 16754604.
 *Arrhenius, S. (1903). "The Propagation of Life in Space". Die Umschau. 7: 32. Bibcode:1980qel..book...32A. Reprinted in Goldsmith, D. (ed.). The Quest for Extraterrestrial Life. University Science Books. ISBN 0-19-855704-3.
* Hoyle, F. & Wickramasinghe, C. (1979). "On the Nature of Interstellar Grains". Astrophysics and Space Science. 66 (1): 77â90. Bibcode:1979Ap&SS..66...77H. doi:10.1007/BF00648361. S2CID 115165958.
* Crick, F.H.; Orgel, L.E. (1973). "Directed Panspermia". Icarus. 19 (3): 341â48. Bibcode:1973Icar...19..341C. doi:10.1016/0019-1035(73)90110-3.
 PeretÃ³, J. (2005). "Controversies on the origin of life" (PDF). Int. Microbiol. 8 (1): 23â31. PMID 15906258. Archived from the original (PDF) on August 24, 2015. Retrieved October 7, 2007.
 Krumbein, W.E.; Brehm, U.; Gerdes, G.; Gorbushina, A.A.; Levit, G. & Palinska, K.A. (2003). "Biofilm, Biodictyon, Biomat Microbialites, Oolites, Stromatolites, Geophysiology, Global Mechanism, Parahistology". In Krumbein, W.E.; Paterson, D.M. & Zavarzin, G.A. (eds.). Fossil and Recent Biofilms: A Natural History of Life on Earth (PDF). Kluwer Academic. pp. 1â28. ISBN 1-4020-1597-6. Archived from the original (PDF) on January 6, 2007. Retrieved July 9, 2008.
 Nisbet, E.G. & Fowler, C.M.R. (December 7, 1999). "Archaean metabolic evolution of microbial mats". Proceedings of the Royal Society B. 266 (1436): 2375. doi:10.1098/rspb.1999.0934. PMC 1690475.
 Gray M.W.; Burger G.; Lang B.F. (March 1999). "Mitochondrial evolution". Science. 283 (5407): 1476â81. Bibcode:1999Sci...283.1476G. doi:10.1126/science.283.5407.1476. PMC 3428767. PMID 10066161.
 El Albani, Abderrazak; Bengtson, Stefan; Canfield, Donald E.; Bekker, Andrey; Macchiarelli, Reberto; Mazurier, Arnaud; Hammarlund, Emma U.; Boulvais, Philippe; et al. (July 2010). "Large colonial organisms with coordinated growth in oxygenated environments 2.1 Gyr ago". Nature. 466 (7302): 100â04. Bibcode:2010Natur.466..100A. doi:10.1038/nature09166. PMID 20596019. S2CID 4331375.
 Butterfield, N.J. (September 2000). "Bangiomorpha pubescens n. gen., n. sp.: implications for the evolution of sex, multicellularity, and the Mesoproterozoic/Neoproterozoic radiation of eukaryotes". Paleobiology. 26 (3): 386â404. doi:10.1666/0094-8373(2000)026<0386:BPNGNS>2.0.CO;2. ISSN 0094-8373. Archived from the original on 2007-03-07. Retrieved 2008-09-02.
 Butterfield, N.J. (2005). "Probable Proterozoic fungi". Paleobiology. 31 (1): 165â82. doi:10.1666/0094-8373(2005)031<0165:PPF>2.0.CO;2. ISSN 0094-8373. Archived from the original on 2009-01-29. Retrieved 2008-09-02.
 Chen, J.-Y.; Oliveri, P.; Gao, F.; Dornbos, S.Q.; Li, C.-W.; Bottjer, D.J. & Davidson, E.H. (August 2002). "Precambrian Animal Life: Probable Developmental and Adult Cnidarian Forms from Southwest China" (PDF). Developmental Biology. 248 (1): 182â96. doi:10.1006/dbio.2002.0714. PMID 12142030. Archived from the original (PDF) on September 11, 2008. Retrieved September 3, 2008.
 Bengtson, S. (2004). Lipps, J.H.; Waggoner, B.M. (eds.). "Early Skeletal Fossils" (PDF). The Paleontological Society Papers. 10 NeoproterozoicâCambrian Biological Revolutions: 67â78. doi:10.1017/S1089332600002345. Archived from the original (PDF) on 2009-03-03. Retrieved July 18, 2008.
 Marshall, C.R. (2006). "Explaining the Cambrian "Explosion" of Animals". Annu. Rev. Earth Planet. Sci. 34: 355â84. Bibcode:2006AREPS..34..355M. doi:10.1146/annurev.earth.33.031504.103001. S2CID 85623607.
 Conway Morris, S. (August 2, 2003). "Once we were worms". New Scientist. 179 (2406): 34. Archived from the original on July 25, 2008. Retrieved September 5, 2008.
 Sansom I.J., Smith, M.M. & Smith, M.P. (2001). "The Ordovician radiation of vertebrates". In Ahlberg, P.E. (ed.). Major Events in Early Vertebrate Evolution. Taylor and Francis. pp. 156â71. ISBN 0-415-23370-4.
 Luo, Z.; Chen, P.; Li, G. & Chen, M. (March 2007). "A new eutriconodont mammal and evolutionary development in early mammals". Nature. 446 (7133): 288â93. Bibcode:2007Natur.446..288L. doi:10.1038/nature05627. PMID 17361176. S2CID 4329583.
 Russell Garwood & Gregory Edgecombe (2011). "Early terrestrial animals, evolution and uncertainty". Evolution: Education and Outreach. 4 (3): 489â501. doi:10.1007/s12052-011-0357-y.
 Selden, P.A. (2001). "Terrestrialization of Animals". In Briggs, D.E.G.; Crowther, P.R. (eds.). Palaeobiology II: A Synthesis. Blackwell. pp. 71â74. ISBN 0-632-05149-3.
 Kenrick, P. & Crane, P.R. (September 1997). "The origin and early evolution of plants on land" (PDF). Nature. 389 (6646): 33. Bibcode:1997Natur.389...33K. doi:10.1038/37918. S2CID 3866183. Archived from the original (PDF) on 2010-12-17. Retrieved 2010-11-11.
 Laurin, M. (2010). How Vertebrates Left the Water. Berkeley, California: University of California Press. ISBN 978-0-520-26647-6.
 MacNaughton, R.B.; Cole, J.M.; Dalrymple, R.W.; Braddy, S.J.; Briggs, D.E.G. & Lukie, T.D. (May 2002). "First steps on land: Arthropod trackways in Cambrian-Ordovician eolian sandstone, southeastern Ontario, Canada". Geology. 30 (5): 391â94. Bibcode:2002Geo....30..391M. doi:10.1130/0091-7613(2002)030<0391:FSOLAT>2.0.CO;2. ISSN 0091-7613.
 Collette, J.H.; Gass, K.C. & Hagadorn, J.W. (May 2012). "Protichnites eremita unshelled? Experimental model-based neoichnology and new evidence for a euthycarcinoid affinity for this ichnospecies". Journal of Paleontology. 86 (3): 442â54. doi:10.1666/11-056.1. S2CID 129234373.
 Gordon, M.S; Graham, J.B. & Wang, T. (SeptemberâOctober 2004). "Revisiting the Vertebrate Invasion of the Land". Physiological and Biochemical Zoology. 77 (5): 697â99. doi:10.1086/425182.
 Clack, J.A. (November 2005). "Getting a Leg Up on Land". Scientific American. Retrieved September 6, 2008.
 Padian, Kevin (2004). "Basal Avialae". In Weishampel, David B.; Dodson, Peter; OsmÃ³lska, Halszka (eds.). The Dinosauria (Second ed.). Berkeley: University of California Press. pp. 210â31. ISBN 0-520-24209-2.
 Sidor, C.A.; O'Keefe, F.R.; Damiani, R.; Steyer, J.S.; Smith, R.M.H.; Larsson, H.C.E.; Sereno, P.C.; Ide, O & Maga, A. (April 2005). "Permian tetrapods from the Sahara show climate-controlled endemism in Pangaea". Nature. 434 (7035): 886â89. Bibcode:2005Natur.434..886S. doi:10.1038/nature03393. PMID 15829962. S2CID 4416647.
 Benton M.J. (2005). When Life Nearly Died: The Greatest Mass Extinction of All Time. Thames & Hudson. ISBN 978-0-500-28573-2.
 Ward, P.D.; Botha, J.; Buick, R.; Kock, M.O.; et al. (2005). "Abrupt and gradual extinction among late Permian land vertebrates in the Karoo Basin, South Africa" (PDF). Science. 307 (5710): 709â14. Bibcode:2005Sci...307..709W. doi:10.1126/science.1107068. PMID 15661973. S2CID 46198018. Archived from the original (PDF) on 2012-08-13. Retrieved 2017-10-25.
 Benton, M.J. (March 1983). "Dinosaur Success in the Triassic: a Noncompetitive Ecological Model" (PDF). Quarterly Review of Biology. 58 (1): 29â55. doi:10.1086/413056. Archived from the original (PDF) on September 11, 2008. Retrieved September 8, 2008.
 Ruben, J.A. & Jones, T.D. (2000). "Selective Factors Associated with the Origin of Fur and Feathers". American Zoologist. 40 (4): 585â96. doi:10.1093/icb/40.4.585.
 Renne, Paul R.; Deino, Alan L.; Hilgen, Frederik J.; Kuiper, Klaudia F.; Mark, Darren F.; Mitchell, William S.; Morgan, Leah E.; Mundil, Roland; Smit, Jan (7 February 2013). "Time Scales of Critical Events Around the Cretaceous-Paleogene Boundary". Science. 339 (6120): 684â87. Bibcode:2013Sci...339..684R. doi:10.1126/science.1230492. PMID 23393261. S2CID 6112274.
 Alroy J. (March 1999). "The fossil record of North American mammals: evidence for a Paleocene evolutionary radiation". Systematic Biology. 48 (1): 107â18. doi:10.1080/106351599260472. PMID 12078635.
 Simmons, N.B.; Seymour, K.L.; Habersetzer, J. & Gunnell, G.F. (February 2008). "Primitive Early Eocene bat from Wyoming and the evolution of flight and echolocation" (PDF). Nature. 451 (7180): 818â21. Bibcode:2008Natur.451..818S. doi:10.1038/nature06549. hdl:2027.42/62816. PMID 18270539. S2CID 4356708.
 J.G.M. Thewissen; S.I. Madar & S.T. Hussain (1996). "Ambulocetus natans, an Eocene cetacean (Mammalia) from Pakistan". Courier Forschungsinstitut Senckenberg. 191: 1â86.
 Crane, P.R.; Friis, E.M. & Pedersen, K.R. (2000). "The Origin and Early Diversification of Angiosperms". In Gee, H. (ed.). Shaking the Tree: Readings from Nature in the History of Life. University of Chicago Press. pp. 233â50. ISBN 0-226-28496-4.
 Crepet, W.L. (November 2000). "Progress in understanding angiosperm history, success, and relationships: Darwin's abominably "perplexing phenomenon"". Proceedings of the National Academy of Sciences. 97 (24): 12939â41. Bibcode:2000PNAS...9712939C. doi:10.1073/pnas.97.24.12939. PMC 34068. PMID 11087846.
 Wilson, E.O. & HÃ¶lldobler, B. (September 2005). "Eusociality: Origin and consequences". Proceedings of the National Academy of Sciences. 102 (38): 13367â71. Bibcode:2005PNAS..10213367W. doi:10.1073/pnas.0505858102. PMC 1224642. PMID 16157878.
 Brunet M., Guy; Pilbeam, F.; Mackaye, H.T.D.; et al. (July 2002). "A new hominid from the Upper Miocene of Chad, Central Africa". Nature. 418 (6894): 145â51. Bibcode:2002Natur.418..145B. doi:10.1038/nature00879. PMID 12110880. S2CID 1316969.
 De Miguel, C. & Henneberg, M. (2001). "Variation in hominid brain size: How much is due to method?". Homo: Journal of Comparative Human Biology. 52 (1): 3â58. doi:10.1078/0018-442X-00019. PMID 11515396.
 Leakey, Richard (1994). The Origin of Humankind. Science Masters Series. New York: Basic Books. pp. 87â89. ISBN 0-465-05313-0.
 Benton, M.J. (2004). "6. Reptiles Of The Triassic". Vertebrate Palaeontology. Blackwell. ISBN 0-04-566002-6. Retrieved November 17, 2008.
 Van Valkenburgh, B. (1999). "Major patterns in the history of xarnivorous mammals". Annual Review of Earth and Planetary Sciences. 27: 463â93. Bibcode:1999AREPS..27..463V. doi:10.1146/annurev.earth.27.1.463.
 MacLeod, Norman (2001-01-06). "Extinction!". Archived from the original on April 4, 2008. Retrieved September 11, 2008.
 Martin, R.E. (1995). "Cyclic and secular variation in microfossil biomineralization: clues to the biogeochemical evolution of Phanerozoic oceans". Global and Planetary Change. 11 (1): 1. Bibcode:1995GPC....11....1M. doi:10.1016/0921-8181(94)00011-2.
 Martin, R.E. (1996). "Secular increase in nutrient levels through the Phanerozoic: Implications for productivity, biomass, and diversity of the marine biosphere". PALAIOS. 11 (3): 209â19. Bibcode:1996Palai..11..209M. doi:10.2307/3515230. JSTOR 3515230. S2CID 67810793.
 Gould, S.J. (1990). Wonderful Life: The Burgess Shale and the Nature of History. Hutchinson Radius. pp. 194â206. ISBN 0-09-174271-4.
 Rohde, R.A. & Muller, R.A. (March 2005). "Cycles in fossil diversity" (PDF). Nature. 434 (7030): 208â10. Bibcode:2005Natur.434..208R. doi:10.1038/nature03339. PMID 15758998. S2CID 32520208. Archived (PDF) from the original on October 3, 2008. Retrieved September 22, 2008.
 Rudwick, Martin J.S. (1985). The Meaning of Fossils (2nd ed.). The University of Chicago Press. pp. 24, 39, 200â01. ISBN 0-226-73103-0.
 Needham, Joseph (1986). Science and Civilization in China: Volume 3, Mathematics and the Sciences of the Heavens and the Earth. Caves Books Ltd. p. 614. ISBN 0-253-34547-2.
 Baucon, A. (2010). "Leonardo da Vinci, the founding father of ichnology". Palaios 25. Abstract available from the author's webpage[self-published source?]
 Baucon A., Bordy E., Brustur T., Buatois L., Cunningham T., De C., Duffin C., Felletti F., Gaillard C., Hu B., Hu L., Jensen S., Knaust D., Lockley M., Lowe P., Mayor A., Mayoral E., Mikulas R., Muttoni G., Neto de Carvalho C., Pemberton S., Pollard J., Rindsberg A., Santos A., Seike K., Song H., Turner S., Uchman A., Wang Y., Yi-ming G., Zhang L., Zhang W. (2012). "A history of ideas in ichnology". In: Bromley R.G., Knaust D. Trace Fossils as Indicators of Sedimentary Environments. Developments in Sedimentology, vol. 64. Tracemaker.com[self-published source?]
 Baucon, A. (2010). "Da Vinciâs Paleodictyon: the fractal beauty of traces". Acta Geologica Polonica, 60(1). Accessible from the author's homepage[self-published source?]
 McGowan, Christopher (2001). The Dragon Seekers. Persus Publishing. pp. 3â4. ISBN 0-7382-0282-7.
 Palmer, D. (2005). Earth Time: Exploring the Deep Past from Victorian England to the Grand Canyon. Wiley. ISBN 978-0470022214.
 Greene, Marjorie; David Depew (2004). The Philosophy of Biology: An Episodic History. Cambridge University Press. pp. 128â30. ISBN 0-521-64371-6.
 Bowler, Peter J.; Iwan Rhys Morus (2005). Making Modern Science. The University of Chicago Press. pp. 168â69. ISBN 0-226-06861-7.
 Rudwick, Martin J.S. (2008). Worlds Before Adam: The Reconstruction of Geohistory in the Age of Reform. The University of Chicago Press. p. 48. ISBN 978-0-226-73128-5.
 Buckland, W. & Gould, S.J. (1980). Geology and Mineralogy Considered With Reference to Natural Theology (History of Paleontology). Ayer Company Publishing. ISBN 978-0-405-12706-9.
 Shu, D.G.; Morris, S.C.; Han, J.; Zhang, Z F.; Yasui, K.; Janvier, P.; Chen, L.; Zhang, X.L.; Liu, J.N.; Li, Y.; Liu, H.-Q. (2003), "Head and backbone of the Early Cambrian vertebrate Haikouichthys", Nature, 421 (6922): 526â29, Bibcode:2003Natur.421..526S, doi:10.1038/nature01264, PMID 12556891, S2CID 4401274, archived from the original on 2015-11-24
 Everhart, Michael J. (2005). Oceans of Kansas: A Natural History of the Western Interior Sea. Indiana University Press. p. 17. ISBN 0-253-34547-2.
 Gee, H., ed. (2001). Rise of the Dragon: Readings from Nature on the Chinese Fossil Record. Chicago; London: University of Chicago Press. p. 276. ISBN 0-226-28491-3.
 Bowler, Peter J. (2003). Evolution: The History of an Idea. University of California Press. pp. 351â52, 325â39. ISBN 0-520-23693-9.
 Crick, F.H.C. (1955). "On degenerate templates and the adaptor hypothesis" (PDF). Archived from the original (PDF) on 2008-10-01. Retrieved 2008-10-04.
 Sarich, V.M. & Wilson, A.C. (December 1967). "Immunological time scale for hominid evolution". Science. 158 (3805): 1200â03. Bibcode:1967Sci...158.1200S. doi:10.1126/science.158.3805.1200. PMID 4964406. S2CID 7349579.
 Page, R.D.M. & Holmes, E.C. (1998). Molecular Evolution: A Phylogenetic Approach. Oxford: Blackwell Science. p. 2. ISBN 0-86542-889-1.
External links
Paleontology
at Wikipedia's sister projects
Definitions from Wiktionary
Media from Wikimedia Commons
News from Wikinews
Quotations from Wikiquote
Texts from Wikisource
Textbooks from Wikibooks
Travel guide from Wikivoyage
Resources from Wikiversity
Smithsonian's Paleobiology website
University of California Museum of Paleontology
The Paleontological Society
The Palaeontological Association
The Society of Vertebrate Paleontology
The Paleontology Portal
"Geology, Paleontology & Theories of the Earth", a collection of more than 100 digitised landmark and early books on Earth sciences at the Linda Hall Library
vte
Branches of biology
vte
Biology
vte
Geology
The Earth seen from Apollo 17 with transparent background.pngEarth sciences portalTree of life.svgEvolutionary biology portalAllosaurus Jardin des Plantes.pngPaleontology portal
Authority control Edit this at Wikidata
GND: 4044375-9HDS: 008280NARA: 10643769NDL: 00566688
Categories: PaleontologyEarth sciencesEvolutionary biologyFossilsHistorical geology
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons
Wikibooks
Wikiquote
Wikisource
Wikivoyage

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tagalog
Tiáº¿ng Viá»t
ä¸­æ
90 more
Edit links
This page was last edited on 19 October 2020, at 14:43 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Jigsaw puzzle
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For the Rolling Stones song, see Jigsaw Puzzle (song).

This article possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (January 2020) (Learn how and when to remove this template message)

People solving a jigsaw puzzle
Part of a series on
Puzzles
Jigsaw piece
Types[show]
Topics[show]
Lists[show]
vte
A jigsaw puzzle is a tiling puzzle that requires the assembly of often oddly shaped interlocking and mosaiced pieces. Typically, each individual piece has a portion of a picture; when assembled, the jigsaw puzzle produces a complete picture.

Jigsaw puzzles were originally created by painting a picture on a flat, rectangular piece of wood, and then cutting that picture into small pieces. Despite it being called a jigsaw, a jigsaw was never actually used to cut it. John Spilsbury, a London cartographer and engraver, is credited with commercializing jigsaw puzzles around 1760.[1] Jigsaw puzzles have since come to be made primarily of cardboard.

Typical images found on jigsaw puzzles include scenes from nature, buildings, and repetitive designs â castles and mountains are common, traditional subjects. However, any kind of picture can be used to make a jigsaw puzzle; artisanal puzzle-makers and companies using technologies to allow one-off or small print run puzzles allow a wide range of subject matter, from optical illusions, unusual art, or personal photographs. Beyond flat two-dimensional puzzles, three-dimensional puzzles have been moving to large-scale production and distribution, including spherical jigsaws and architectural recreations.

During recent years, a range of jigsaw puzzle accessories including boards, cases, frames, and roll-up mats have become available that are designed to assist jigsaw puzzle enthusiasts. Completed puzzles can also be attached to a backing with adhesive to be used as artwork.


Contents
1	History
2	Modern construction
3	Variations
4	Puzzle pieces
5	World records
5.1	Largest commercially available jigsaw puzzles
5.2	Calculating the number of border pieces before starting
5.3	Largest-sized jigsaw puzzles
5.4	Largest jigsaw puzzle â most pieces
6	Cultural references
6.1	Art and entertainment
6.2	Symbol for autism
7	See also
8	References
9	External links
History

John Spilsbury's "Europe divided into its kingdoms, etc." (1766). He created the jigsaw puzzle for educational purposes, and called them âDissected Mapsâ.[2][3]
The engraver and cartographer John Spilsbury, of London, is believed to have produced the first jigsaw puzzle around 1760, using a marquetry saw.[1] Early jigsaws, known as dissections, were produced by mounting maps on sheets of hardwood and cutting along national boundaries, creating a puzzle useful for the teaching of geography.[1] Such "dissected maps" were used to teach the children of King George III and Queen Charlotte by royal governess Lady Charlotte Finch.[4][5]

The name "jigsaw" came to be associated with the puzzle around 1880 when fretsaws became the tool of choice for cutting the shapes. Since fretsaws are distinct from jigsaws, the name appears to be a misnomer.[1] Cardboard jigsaw puzzles appeared during the late 1800s, but were slow to replace the wooden jigsaw due to the manufacturer's belief that cardboard puzzles would be perceived as being of low quality, and the fact that profit margins on wooden jigsaws were larger.[1]


Wooden jigsaw pieces, cut by hand
Jigsaw puzzles soared in popularity during the Great Depression, as they provided a cheap, long-lasting, recyclable form of entertainment.[1][6] It was around this time that jigsaws evolved to become more complex and more appealing to adults.[1] They were also given away in product promotions, and used in advertising, with customers completing an image of the product being promoted.[1][6]

Sales of wooden jigsaw puzzles fell after World War II as improved wages led to price increases, while at the same time improvements in manufacturing processes made paperboard jigsaws more attractive.[6]

According to the Alzheimer Society of Canada, doing jigsaw puzzles is one of many activities that can help keep the brain active and may contribute to reducing the risk of developing Alzheimer's disease.[7]

Demand for jigsaw puzzles saw a surge, comparable to that of the Great Depression, during the COVID-19 pandemic stay at home orders[8][9][8]

Modern construction

Paperboard jigsaw pieces
Most modern jigsaw puzzles are made out of paperboard since they are easier and cheaper to mass-produce than the original wooden models. An enlarged photograph or printed reproduction of a painting or other two-dimensional artwork is glued onto the cardboard before cutting. This board is then fed into a press. The press forces a set of hardened steel blades of the desired shape through the board until it is fully cut. This procedure is similar to making shaped cookies with a cookie cutter. The forces involved, however, are tremendously greater and a typical 1000-piece puzzle requires a press that can generate upwards of 700 tons of force to push the knives of the puzzle die through the board. A puzzle die is a flat board, often made from plywood, which has slots cut or burned in the same shape as the knives that are used. These knives are set into the slots and covered in a compressible material, typically foam rubber, which serves to eject the cut puzzle pieces.

Beginning in the 1930s, jigsaw puzzles were cut using large hydraulic presses which now cost in the hundreds of thousands of dollars. The cuts gave a very snug fit, but the cost limited jigsaw puzzle manufacture only to large corporations. Recent roller press design achieve the same effect, at a lower cost.[citation needed]

New technology has enabled laser-cutting of wooden or acrylic jigsaw puzzles. The advantage of cutting with a laser is that the puzzle can be custom cut into any size, any shape, with any size (or any number) of pieces. Many museums have laser cut acrylic puzzles made of some of their more important pieces of art so that children visiting the museum can see the original piece and then assemble a jigsaw puzzle of the image that is also in the same shape as the piece of art. Acrylic is used because the pieces are very durable, waterproof, and can withstand continued use without the image fading, or the pieces wearing out, or becoming frayed. Also, because the print and cut patterns are computer-based, lost pieces can be manufactured without remaking the entire puzzle.

By the early 1960s, Tower Press was the world's largest maker of jigsaw puzzles, acquired by Waddingtons in 1969.[10] Major jigsaw puzzle manufacturers currently include Ravensburger and Tower Press. Wooden and specialty jigsaw puzzle manufacturers include Artifact Puzzles. In addition to large-scale puzzle manufacturers, numerous puzzle makers work in an artisanal style, handcrafting and handcutting jigsaw puzzles.[11][12][13][14]

Variations

Jigsaw puzzle software allowing rotation of pieces

A three-dimensional puzzle composed of several two-dimensional puzzles stacked on top of one another

A puzzle without a picture
Jigsaw puzzles come in a variety of sizes. Among those targeted to adults, 300, 500, and 750 piece puzzles are considered "smaller". More sophisticated, but still common, jigsaw puzzles come in sizes of 1,000, 1,500, 2,000, 3,000, 4,000, 5,000, 6,000, 7,500, 8,000, 9,000, 13,200, 18,000, 24,000, 32,000 and 40,000 pieces.

Jigsaw puzzles that are geared towards children may have many fewer pieces, typically much larger. For very young children, a puzzle with as few as 4 to 9 "large"-size pieces (so not a choking hazard) are common. These are usually made of wood or plastic, to maintain durability, and are able to be cleaned without being damaged.

The most common layout for a thousand-piece puzzle is 38 pieces by 27 pieces, for a total count of 1,026 pieces. The majority of 500-piece puzzles are 27 pieces by 19 pieces. A few puzzles are made double-sided, so that they can be solved from either side. This adds a level of complexity, because it cannot be certain that the correct side of the piece is being viewed and assembled with the other pieces.

"Family puzzles" come in 100â550 pieces with three different-sized pieces from large to small. The pieces are placed from large to small, going in one direction or towards the middle of the puzzle. This allows a family of puzzlers of different skill levels and different-sized hands to work on the puzzle at the same time. Companies like Springbok, Cobble Hill, Ravensburger, and Suns Out make this type of specialty puzzle.

There are also three-dimensional jigsaw puzzles. Many of these are made of wood or styrofoam and require the puzzle to be solved in a certain order; some pieces will not fit in if others are already in place. Also common are puzzle boxes: simple three-dimensional jigsaw puzzles with a small drawer or box in the center for storage.

Another type of jigsaw puzzle, which is considered a 3-D puzzle, is a puzzle globe. Like a 2-D puzzle, a globe puzzle is often made of plastic and the assembled pieces form a single layer. But the final form is a three-dimensional shape. Most globe puzzles have designs representing spherical shapes such as the Earth, the Moon, and historical globes of the Earth.

There are also computer versions of jigsaw puzzles, which have the advantages of requiring zero cleanup and no risk of losing any pieces. Many computer-based jigsaw puzzles do not allow pieces to be rotated, so all pieces are displayed in their correct orientation. These puzzles are thus considerably easier than a physical jigsaw puzzle with the same number of pieces. A computer puzzle website can allow users to choose their own puzzle size, cut design, and image, or upload their own images to use as puzzles.[15] An online jigsaw version of Trolleholm Castle in Sweden may be worked and timed for speed of finishing.[16] The New Yorker Magazine subscription website preserves images of the magazine's cover illustrations as jigsaw puzzles, which are timed and offer several levels of difficulty.

In 2016 was introduced a computer version of puzzle globe, the immersive panorama jigsaw, which is based on the use of equirectangular images taken by 360-degree camera. Despite the physical spherical jigsaw, the player, who resides in the perfect center of the globe, assembles triangular-shaped interlocking pieces around him. When complete, this puzzle produces a full-degree panorama all around the player. An example of immersive jigsaw is Sitespot, which also enriches the gaming experience with the scene soundscape and allows pieces to be displayed rotated.

Jigsaw puzzles can vary greatly in price depending on the complexity, number of pieces, and brand. Children's puzzles can be as cheap as around $5.00, while larger puzzles can be closer to $50.00. The most expensive puzzle to date was sold for $27,000 in 2005 at a charitable auction for The Golden Retriever Foundation.[17]

Several word puzzle games use pieces similar to those used in jigsaw puzzles. Examples include Alfa-Lek, Jigsaw Words, Nab-It!, Puzzlage, Typ-Dom, Word Jigsaw, and Yottsugo.[18][citation needed]

Puzzle pieces

A "whimsy" piece in a wooden jigsaw puzzle

A 3D jigsaw puzzle
Many puzzles are termed "fully interlocking". This means that adjacent pieces are connected in such a way that if one piece is moved horizontally, the other pieces move with it, preserving the connection. Sometimes the connection is tight enough to pick up a solved part by holding one piece.

Some fully interlocking puzzles have pieces all of a similar shape, with rounded tabs out on opposite ends, with corresponding blanks cut into the intervening sides to receive the tabs of adjacent pieces. Other fully interlocking puzzles may have tabs and blanks variously arranged on each piece, but they usually have four sides, and the numbers of tabs and blanks thus add up to four. The uniform-shaped fully interlocking puzzles, sometimes called "Japanese Style", are the most difficult, because the differences in shapes between pieces can be very subtle.[citation needed]

Most jigsaw puzzles are square, rectangular, or round, with edge pieces that have one side that is either straight or smoothly curved to create this shape, plus four corner pieces if the puzzle is square or rectangular. Some jigsaw puzzles have edge pieces that are cut just like all the rest of the interlocking pieces, with no smooth edge, to make them more challenging. Other puzzles are designed so the shape of the whole puzzle forms a figure, such as an animal. The edge pieces may vary more in these cases.

The pieces of spherical jigsaw, like immersive panorama jigsaw, can be triangular shaped, according to the rules of tessellation of the geoid primitive.

The designer Yuu Asaka created âJigsaw Puzzle 29â which has not four corner pieces but five corner pieces, and is made from pale blue acrylic without a picture. [19] It was awarded the Jury Honorable Mention of 2018 Puzzle Design Competition. [20] But many puzzlers had solved it easily, he created âJigsaw Puzzle 19â which composed only with corner pieces as revenge. [21] It was made with transparent green acrylic pieces without a picture. [22]

World records
Largest commercially available jigsaw puzzles
Pieces	Name of puzzle	Company	Year	Size [cm]	Area [m2]
54,000	Travel by Art	Grafika	2020	864 Ã 204	17.65
52,110	(No title: collage of animals)	MartinPuzzle	2018	696 Ã 202	14.06
51,300	27 Wonders from Around the World	Kodak	2019	869 Ã 191	16.60
48,000	Around the World	Grafika	2017	768 Ã 204	15.67
42,000	La vuelta al Mundo	Educa	2017	749 Ã 157	11.76
40,320	Making Mickey Magic	Ravensburger	2018	680 Ã 192	13.06
40,320	Memorable Disney Moments	Ravensburger	2016	680 Ã 192	13.06
33,600	Wild Life	Educa	2014	570 Ã 157	8.95
32,000	New York City Window	Ravensburger	2014	544 Ã 192	10.45
32,000	Double Retrospect	Ravensburger	2010	544 Ã 192	10.45
24,000	Life, The greatest puzzle	Educa	2007	428 Ã 157	6.72
The world's largest commercially available jigsaw puzzle (Nov. 2018) is produced by Czech company MartinPuzzle and contains 52,110 pieces showing a collage of animals.

In 2016, the German company Ravensburger released their biggest puzzle. It shows 10 scenes from Disney works and has 40,320 pieces, measuring 680 cm by 192 cm when assembled.[23]

In 2018, Ravensburger released their second biggest puzzle. It shows Mickey Mouse making magic through the years and also has 40,320 pieces and also measures 680 cm by 192 cm when assembled.

Calculating the number of border pieces before starting
Jigsaw puzzlers often want to know in advance how many border pieces they are looking for to verify they have found all of them. Puzzle sizes are typically listed on commercially distributed puzzles, but usually just include the total number of pieces in the puzzle, and do not list the count of edge or interior pieces.

Puzzlers therefore calculate the number of border pieces. To calculate B (border pieces) from P (the total piece count), follow this method:

List the prime factors of P.
For example: For a 513-piece jigsaw, the prime factorization tree is 3Ã3Ã3Ã19=513.
Take the square root of P and round off.
The square root of 513 is about 22.6, so round to 23.
Look for numbers in the prime factor list within +/- 20 percent of the square root of P.
Calculate 20% of the square root of P.
20% of 23 = 4.6.
Develop the range, +/- 20%, from the square root of P.
The square root is about 23. 23 +/- 4.6 = 18.4 to 27.6
Compare the range with the factor list. Define this as E1.
The factor list shows 19 in the range.
Determine the horizontal / vertical dimensions.
Divide P (the total number of pieces) by E1 to determine the horizontal / vertical dimensions, E1xE2.
513 / 19 = 27. This is probably a 19x27 puzzle.
alternate approach: Take the remaining numbers from the prime factorization tree.
3x3x3 = 27
Add the four sides and subtract "4" to correct for the corner pieces, which would otherwise be counted in both the horizontal and vertical.
27 + 27 + 19 + 19 -4 = 88. These 88 border pieces include 4 corners, 17 pieces between corners on the short sides, and 25 between corners on the long sides.
Common puzzle dimensions:

1000 piece puzzle: 1026 pieces, 126 border pieces (38x27)[24]
Largest-sized jigsaw puzzles
The world's largest-sized jigsaw puzzle measured 5,428.8 m2 (58,435 sq ft) with 21,600 pieces, each measuring a Guinness World Records maximum size of 50 cm by 50 cm. It was assembled on 3 November 2002 by 777 people at the former Kai Tak Airport in Hong Kong.[25]

Largest jigsaw puzzle â most pieces

The Guinness record of CYM Group in 2011 with 551,232 pieces
The jigsaw with the greatest number of pieces had 551,232 pieces and measured 14.85 Ã 23.20 m (48 ft 8.64 in Ã 76 ft 1.38 in). It was assembled on 25 September 2011 at PhÃº Thá» Indoor Stadium in Ho Chi Minh City, Vietnam, by students of the University of Economics, Ho Chi Minh City. It is listed by the Guinness World Records for the "Largest Jigsaw Puzzle â most pieces", but as the intact jigsaw had been divided into 3,132 sections, each containing 176 pieces, which were reassembled and then connected, the claim is controversial.[26][27]

Cultural references
The logo of Wikipedia is a globe made out of jigsaw pieces. The incomplete sphere appears to have some pieces missing, symbolizing the room to add new knowledge.[citation needed]

In the logo of the Colombian Office of the Attorney General appears a jigsaw puzzle piece in foreground. They named it as "The Key Piece": "The jigsaw puzzle piece is the appropiate [sic?] symbol for visual representation of the Office, since it includes the concepts of searching, solution and response that the institution pursuits through its investigation activity."[28]

Art and entertainment
The central antagonist in the Saw film franchise is named Jigsaw.[29]

In the 1933 Laurel and Hardy short Me and My Pal, several characters attempt to complete a large jigsaw puzzle.[30]

Symbol for autism

An "autism awareness" ribbon, featuring red, blue, and yellow jigsaw pieces
Jigsaw puzzle pieces were first used as a symbol for autism in 1963 by the United Kingdom's National Autistic Society.[31] The organization chose jigsaw pieces for their logo to represent the "puzzling" nature of autism and the inability to "fit in" due to social differences, and also because jigsaw pieces were recognizable and otherwise unused. Puzzle pieces have since been incorporated into the logos and promotional materials of many organizations, including the Autism Society of America and Autism Speaks.

Proponents of the autism rights movement oppose the jigsaw puzzle iconography, stating that metaphors such as "puzzling" and "incomplete" are harmful to autistic people. Critics of the puzzle piece symbol instead advocate for a rainbow-colored infinity symbol representing diversity.[32] In 2017, the journal Autism concluded that the use of the jigsaw puzzle evoked negative public perception towards autistic individuals, and in February 2018 removed the puzzle piece from their cover.[33]

See also
Jigsaw puzzle accessories
Life: A User's Manual by Georges Perec, a novel told as a jigsaw puzzle
Lost in Translation (poem)
Tessellation
Three-dimensional edge-matching puzzle
References
 McAdam, Daniel. "History of Jigsaw Puzzles". American Jigsaw Puzzle Society. Archived from the original on 19 October 2000. Retrieved 13 October 2014.
 "The Time of the Jigsaws". BBC. 15 November 2016.
 "Top 10 facts about jigsaw puzzles". Daily Express. 15 November 2016.
 Historic Royal Palaces press release "Jigsaw cabinet" Archived 2015-06-13 at the Wayback Machine
 https://collections.vam.ac.uk/item/O1243701/puzzle-cabinet-unknown/ V&A collection; Museum number:B.1:1 & 2-2011; puzzle cabinet
 Williams, Anne, D. "Jigsaw Puzzles â A Brief History". www.mgcpuzzles.com. Retrieved 2 August 2014.
 Healthy Brain Archived 2010-12-12 at the Wayback Machine Alzheimer Society of Canada Accessed 30 March 2011
 Miller, Hannah (April 5, 2020). "Demand for jigsaw puzzles is surging as coronavirus keeps millions of Americans indoors". CNBC. Retrieved April 16, 2020.
 Doubek, James (April 13, 2020). "With People Stuck at Home, Jigsaw Puzzle Sales Soar". NPR. Retrieved April 16, 2020.
 Achievement. World Trade Magazines Ltd. 1962. p. 31. Retrieved 3 April 2013.
 Charlotte Arneson, "The Perfect Jigsaw for Every Type of Puzzler", Slate, April 10, 2020.
 Tracee M. Herbaugh, "Snapping Into Place: Jigsaw Puzzles Have Ardent Following", Associated Press via Minnesota Star-Tribune, Feb. 12, 2020.
 Andy Castillo, "Specialty puzzle uses laser-cut techniques to offer one-of-a-kind offerings", Greenfield Recorder, April 6, 2018.
 Jennifer A. Kingson, "Eye for Art and Artistry Amid Jigsawâs Jumble", New York Times, Dec. 7, 2010.
 "Online Jigsaw Puzzles". Jigzone. Retrieved 13 October 2014.
 "Trolleholm Castle, Sweden jigsaw puzzle". TheJigsawPuzzles.com. Retrieved 30 November 2016.
 "Most expensive jigsaw puzzle sold at auction". Guinness World Records. Retrieved 2016-03-06.
 "Puzzle â Board Game Category". boardgamegeek.com. Retrieved 31 March 2018.
 Ramsay, Chris (2019-03-07), Solving The HARDEST JIGSAW PUZZLE!! â LEVEL 10!, YouTube
 2018 Puzzle Design Competition Results, International Puzzle Collectors Association, 2018
 Valtiel (2019-08-21), This puzzle composed only with corners, Reddit
 Asaka, Yuu (2019), Yuu Asaka interview, Akita University of art
 "Largest commercial jigsaw puzzle â most pieces". Guinness World Records. 2010-09-01. Retrieved 2013-03-04.
 "How To Count Puzzles Pieces". Jigsaw Puzzle Hobby. Retrieved 2020-10-05.
 "Largest jigsaw puzzle". Guinness World Records. 2002-11-03. Retrieved 2013-03-04.
 "Largest jigsaw puzzle â most pieces". Guinness World Records. Retrieved 15 March 2017.
 "Vietnam puts together the world's largest jigsaw puzzle". Guinness World Records News. Retrieved 15 March 2017.
 "'Logo and anthem of the Office of the Attorney General of Colombia'". FiscalÃ­a General de la NaciÃ³n (Colombia). 2018-12-02.
 "'Saw' IMDB page". Internet Movie Database. 2004-10-29.
 "'Me and My Pal' IMDB page". Internet Movie Database. 2015-08-29.
 "NAS timeline (text only version)". National Autistic Society. 2013-02-15. Archived from the original on 2014-07-14. Retrieved 2014-07-13.
 Lisa D. (full last name unknown) (2012-05-02). "I am not a puzzle: From Reports from a Resident Alien". Unpuzzled. Archived from the original on 2014-07-30. Retrieved 2014-07-30.
 Diament, Michelle (February 2, 2018). "Autism Journal Abandons Puzzle Piece". Disability Scoop. Retrieved March 18, 2018.
External links
	Wikimedia Commons has media related to Jigsaw puzzle.
Jigsaw-puzzle.org at Wayback Machine (November 2000)
Categories: Jigsaw puzzlesTiling puzzlesMechanical puzzlesTraditional toysWooden toys
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Ø§ÙØ¹Ø±Ø¨ÙØ©
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
47 more
Edit links
This page was last edited on 5 October 2020, at 14:04 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki

Optical illusion
From Wikipedia, the free encyclopedia
  (Redirected from Optical illusions)
Jump to navigationJump to search
This article is about visual perception. For the album, see Optical Illusion (Time Requiem album).

The checker shadow illusion. Although square A appears a darker shade of gray than square B, in the image the two have exactly the same luminance.

Drawing a connecting bar between the two squares breaks the illusion and shows that they are the same shade.

Gregory's categorization of illusions [1]

In this animation, Mach bands exaggerate the contrast between edges of the slightly differing shades of gray as soon as they come in contact with one another.
An optical illusion (also called a visual illusion[2]) is an illusion caused by the visual system and characterized by a visual percept that arguably appears to differ from reality. Illusions come in a wide variety; their categorization is difficult because the underlying cause is often not clear[3] but a classification[1][4] proposed by Richard Gregory is useful as an orientation. According to that, there are three main classes: physical, physiological, and cognitive illusions, and in each class there are four kinds: Ambiguities, distortions, paradoxes, and fictions.[4] A classical example for a physical distortion would be the apparent bending of a stick half immerged in water; an example for a physiological paradox is the motion aftereffect (where, despite movement, position remains unchanged).[4] An example for a physiological fiction is an afterimage.[4] Three typical cognitive distortions are the Ponzo, Poggendorff, and MÃ¼ller-Lyer illusion.[4] Physical illusions are caused by the physical environment, e.g. by the optical properties of water.[4] Physiological illusions arise in the eye or the visual pathway, e.g. from the effects of excessive stimulation of a specific receptor type.[4] Cognitive visual illusions are the result of unconscious inferences and are perhaps those most widely known.[4]

Pathological visual illusions arise from pathological changes in the physiological visual perception mechanisms causing the aforementioned types of illusions; they are discussed e.g. under visual hallucinations.

Optical illusions, as well as multi-sensory illusions involving visual perception, can also be used in the monitoring and rehabilitation of some psychological disorders, including phantom limb syndrome[5] and schizophrenia.[6]


Contents
1	Physical visual illusions
2	Physiological visual illusions
3	Cognitive illusions
4	Explanation of cognitive illusions
4.1	Perceptual organization
4.2	Depth and motion perception
4.3	Color and brightness constancies
4.4	Object
4.5	Future perception
5	Pathological visual illusions (distortions)
6	Connections to psychological disorders
6.1	The rubber hand illusion (RHI)
6.2	Illusions and schizophrenia
7	List of illusions
8	In art
9	Cognitive processes hypothesis
10	Gallery
11	See also
12	Notes
13	References
14	Further reading
15	External links
Physical visual illusions
A familiar phenomenon and example for a physical visual illusion is when mountains appear to be much nearer in clear weather with low humidity (Foehn) than they are. This is because haze is a cue for depth perception, signalling the distance of far-away objects (Aerial perspective).

The classical example of a physical illusion is when a stick that is half immersed in water appears bent. This phenomenon has already been discussed by Ptolemy (ca. 150) [7] and was often a prototypical example for an illusion.

Physiological visual illusions
Physiological illusions, such as the afterimages[8] following bright lights, or adapting stimuli of excessively longer alternating patterns (contingent perceptual aftereffect), are presumed to be the effects on the eyes or brain of excessive stimulation or interaction with contextual or competing stimuli of a specific typeâbrightness, color, position, tile, size, movement, etc. The theory is that a stimulus follows its individual dedicated neural path in the early stages of visual processing and that intense or repetitive activity in that or interaction with active adjoining channels causes a physiological imbalance that alters perception.

The Hermann grid illusion and Mach bands are two illusions that are often explained using a biological approach. Lateral inhibition, where in receptive fields of the retina receptor signals from light and dark areas compete with one another, has been used to explain why we see bands of increased brightness at the edge of a color difference when viewing Mach bands. Once a receptor is active, it inhibits adjacent receptors. This inhibition creates contrast, highlighting edges. In the Hermann grid illusion, the gray spots that appear at the intersections at peripheral locations are often explained to occur because of lateral inhibition by the surround in larger receptive fields.[9] However, lateral inhibition as an explanation of the Hermann grid illusion has been disproved.[10] [11] [12] [13] [14] More recent empirical approaches to optical illusions have had some success in explaining optical phenomena with which theories based on lateral inhibition have struggled.[15]

Cognitive illusions

"The Organ Player" â Pareidolia phenomenon in Neptune's Grotto stalactite cave (Alghero, Sardinia)
Cognitive illusions are assumed to arise by interaction with assumptions about the world, leading to "unconscious inferences", an idea first suggested in the 19th century by the German physicist and physician Hermann Helmholtz.[16] Cognitive illusions are commonly divided into ambiguous illusions, distorting illusions, paradox illusions, or fiction illusions.

Ambiguous illusions are pictures or objects that elicit a perceptual "switch" between the alternative interpretations. The Necker cube is a well-known example; other instances are the Rubin vase and the "squircle", based on Kokichi Sugihara's ambiguous cylinder illusion.[17]
Distorting or geometrical-optical illusions are characterized by distortions of size, length, position or curvature. A striking example is the CafÃ© wall illusion. Other examples are the famous MÃ¼ller-Lyer illusion and Ponzo illusion.
Paradox illusions (or impossible object illusions) are generated by objects that are paradoxical or impossible, such as the Penrose triangle or impossible staircase seen, for example, in M. C. Escher's Ascending and Descending and Waterfall. The triangle is an illusion dependent on a cognitive misunderstanding that adjacent edges must join.
Fictions are when a figure is perceived even though it is not in the stimulus.
Explanation of cognitive illusions
Perceptual organization

Reversible figures and vase, or the figure-ground illusion

Rabbitâduck illusion
To make sense of the world it is necessary to organize incoming sensations into information which is meaningful. Gestalt psychologists believe one way this is done is by perceiving individual sensory stimuli as a meaningful whole.[18] Gestalt organization can be used to explain many illusions including the rabbitâduck illusion where the image as a whole switches back and forth from being a duck then being a rabbit and why in the figureâground illusion the figure and ground are reversible.

In this there is no "Drawn" White Triangle. Click caption for an explanation.
Kanizsa's triangle
In addition, gestalt theory can be used to explain the illusory contours in the Kanizsa's triangle. A floating white triangle, which does not exist, is seen. The brain has a need to see familiar simple objects and has a tendency to create a "whole" image from individual elements.[18] Gestalt means "form" or "shape" in German. However, another explanation of the Kanizsa's triangle is based in evolutionary psychology and the fact that in order to survive it was important to see form and edges. The use of perceptual organization to create meaning out of stimuli is the principle behind other well-known illusions including impossible objects. The brain makes sense of shapes and symbols putting them together like a jigsaw puzzle, formulating that which is not there to that which is believable.

The gestalt principles of perception govern the way different objects are grouped. Good form is where the perceptual system tries to fill in the blanks in order to see simple objects rather than complex objects. Continuity is where the perceptual system tries to disambiguate which segments fit together into continuous lines. Proximity is where objects that are close together are associated. Similarity is where objects that are similar are seen as associated. Some of these elements have been successfully incorporated into quantitative models involving optimal estimation or Bayesian inference. [19][20]

The double-anchoring theory, a popular but recent theory of lightness illusions, states that any region belongs to one or more frameworks, created by gestalt grouping principles, and within each frame is independently anchored to both the highest luminance and the surround luminance. A spot's lightness is determined by the average of the values computed in each framework.[21]

Depth and motion perception

The verticalâhorizontal illusion where the vertical line is thought to be longer than the horizontal
The Yellow lines are the same length. Click on the name at bottom of picture for an explanation.
Ponzo illusion
Illusions can be based on an individual's ability to see in three dimensions even though the image hitting the retina is only two dimensional. The Ponzo illusion is an example of an illusion which uses monocular cues of depth perception to fool the eye. But even with two-dimensional images, the brain exaggerates vertical distances when compared with horizontal distances, as in the vertical-horizontal illusion where the two lines are exactly the same length.

In the Ponzo illusion the converging parallel lines tell the brain that the image higher in the visual field is farther away, therefore, the brain perceives the image to be larger, although the two images hitting the retina are the same size. The optical illusion seen in a diorama/false perspective also exploits assumptions based on monocular cues of depth perception. The M.C. Escher painting Waterfall exploits rules of depth and proximity and our understanding of the physical world to create an illusion. Like depth perception, motion perception is responsible for a number of sensory illusions. Film animation is based on the illusion that the brain perceives a series of slightly varied images produced in rapid succession as a moving picture. Likewise, when we are moving, as we would be while riding in a vehicle, stable surrounding objects may appear to move. We may also perceive a large object, like an airplane, to move more slowly than smaller objects, like a car, although the larger object is actually moving faster. The phi phenomenon is yet another example of how the brain perceives motion, which is most often created by blinking lights in close succession.

The ambiguity of direction of motion due to lack of visual references for depth is shown in the spinning dancer illusion. The spinning dancer appears to be moving clockwise or counterclockwise depending on spontaneous activity in the brain where perception is subjective. Recent studies show on the fMRI that there are spontaneous fluctuations in cortical activity while watching this illusion, particularly the parietal lobe because it is involved in perceiving movement.[22]

Color and brightness constancies

Simultaneous contrast illusion. The background is a color gradient and progresses from dark gray to light gray. The horizontal bar appears to progress from light grey to dark grey, but is in fact just one color.
Perceptual constancies are sources of illusions. Color constancy and brightness constancy are responsible for the fact that a familiar object will appear the same color regardless of the amount of light or color of light reflecting from it. An illusion of color difference or luminosity difference can be created when the luminosity or color of the area surrounding an unfamiliar object is changed. The luminosity of the object will appear brighter against a black field (that reflects less light) compared to a white field, even though the object itself did not change in luminosity. Similarly, the eye will compensate for color contrast depending on the color cast of the surrounding area.

In addition to the gestalt principles of perception, water-color illusions contribute to the formation of optical illusions. Water-color illusions consist of object-hole effects and coloration. Object-hole effects occur when boundaries are prominent where there is a figure and background with a hole that is 3D volumetric in appearance. Coloration consists of an assimilation of color radiating from a thin-colored edge lining a darker chromatic contour. The water-color illusion describes how the human mind perceives the wholeness of an object such as top-down processing. Thus, contextual factors play into perceiving the brightness of an object.[23]

Object

"Shepard's tables" deconstructed. The two tabletops appear to be different, but they are the same size and shape.
Just as it perceives color and brightness constancies, the brain has the ability to understand familiar objects as having a consistent shape or size. For example, a door is perceived as a rectangle regardless of how the image may change on the retina as the door is opened and closed. Unfamiliar objects, however, do not always follow the rules of shape constancy and may change when the perspective is changed. The "Shepard's table" illusion[24] is an example of an illusion based on distortions in shape constancy.

Future perception
[dubious â discuss]


Optical illusion (black and white contrast) by Giovanni Guida
Researcher Mark Changizi of Rensselaer Polytechnic Institute in New York has a more imaginative take on optical illusions, saying that they are due to a neural lag which most humans experience while awake. When light hits the retina, about one-tenth of a second goes by before the brain translates the signal into a visual perception of the world. Scientists have known of the lag, yet they have debated how humans compensate, with some proposing that our motor system somehow modifies our movements to offset the delay.[25]

Changizi asserts that the human visual system has evolved to compensate for neural delays by generating images of what will occur one-tenth of a second into the future. This foresight enables humans to react to events in the present, enabling humans to perform reflexive acts like catching a fly ball and to maneuver smoothly through a crowd.[26] In an interview with ABC Changizi said, "Illusions occur when our brains attempt to perceive the future, and those perceptions don't match reality."[27] For example, an illusion called the Hering illusion looks like bicycle spokes around a central point, with vertical lines on either side of this central, so-called vanishing point.[28] The illusion tricks us into thinking we are looking at a perspective picture, and thus according to Changizi, switches on our future-seeing abilities. Since we aren't actually moving and the figure is static, we misperceive the straight lines as curved ones. Changizi said:

Evolution has seen to it that geometric drawings like this elicit in us premonitions of the near future. The converging lines toward a vanishing point (the spokes) are cues that trick our brains into thinking we are moving forwardâas we would in the real world, where the door frame (a pair of vertical lines) seems to bow out as we move through itâand we try to perceive what that world will look like in the next instant.[26]

Pathological visual illusions (distortions)
A pathological visual illusion is a distortion of a real external stimulus[29] and are often diffuse and persistent. Pathological visual illusions usually occur throughout the visual field, suggesting global excitability or sensitivity alterations.[30] Alternatively visual hallucination is the perception of an external visual stimulus where none exists.[29] Visual hallucinations are often from focal dysfunction and are usually transient.

Types of visual illusions include oscillopsia, halos around objects, illusory palinopsia (visual trailing, light streaking, prolonged indistinct afterimages), akinetopsia, visual snow, micropsia, macropsia, teleopsia, pelopsia, Alice in Wonderland syndrome, metamorphopsia, dyschromatopsia, intense glare, blue field entoptic phenomenon, and purkinje trees.

These symptoms may indicate an underlying disease state and necessitate seeing a medical practitioner. Etiologies associated with pathological visual illusions include multiple types of ocular disease, migraines, hallucinogen persisting perception disorder, head trauma, and prescription drugs. If a medical work-up does not reveal a cause of the pathological visual illusions, the idiopathic visual disturbances could be analogous to the altered excitability state seen in visual aura with no migraine headache. If the visual illusions are diffuse and persistent, they often affect the patient's quality of life. These symptoms are often refractory to treatment and may be caused by any of the aforementioned etiologies, but are often idiopathic. There is no standard treatment for these visual disturbances.

Connections to psychological disorders
The rubber hand illusion (RHI)

A visual representation of what an amputee with phantom limb syndrome senses.
The rubber hand illusion (RHI), a multi-sensory illusion involving both visual perception and touch, has been used to study how phantom limb syndrome affects amputees over time.[31] Amputees with the syndrome actually responded to RHI more strongly than controls, an effect that was often consistent for both the sides of the intact and the amputated arm.[31] However, in some studies, amputees actually had stronger responses to RHI on their intact arm, and more recent amputees responded to the illusion better than amputees who had been missing an arm for years or more.[31] Researchers believe this is a sign that the body schema, or an individual's sense of their own body and its parts, progressively adapts to the post-amputation state.[31] Essentially, the amputees were learning to no longer respond to sensations near what had once been their arm.[31] As a result, many have suggested the use of RHI as a tool for monitoring an amputee's progress in reducing their phantom limb sensations and adjusting to the new state of their body.[31]

Other research used RHI in the rehabilitation of amputees with prosthetic limbs.[32] After prolonged exposure to RHI, the amputees gradually stopped feeling a dissociation between the prosthetic (which resembled the rubber hand) and the rest of their body.[32] This was thought to be because they adjusted to responding to and moving a limb that did not feel as connected to the rest of their body or senses.[32]

RHI may also be used to diagnose certain disorders related to impaired proprioception or impaired sense of touch in non-amputees.[32]

Illusions and schizophrenia

Top-down processing involves using action plans to make perceptual interpretations and vice versa. (This is impaired in schizophrenia.)
Schizophrenia, a mental disorder often marked by hallucinations, also decreases a person's ability to perceive high-order optical illusions.[33] This is because schizophrenia impairs one's capacity to perform top-down processing and a higher-level integration of visual information beyond the primary visual cortex, V1.[33] Understanding how this specifically occurs in the brain may help in understanding how visual distortions, beyond imaginary hallucinations, affect schizophrenic patients.[33] Additionally, evaluating the differences between how schizophrenic patients and normal individuals see illusions may enable researchers to better identify where specific illusions are processed in the visual streams.[33]


An example of the hollow face illusion which makes concave masks appear to be jutting out (or convex).

An example of motion induced blindness: while fixating on the flashing dot, the stationary dots may disappear due to the brain prioritizing motion information.
One study on schizophrenic patients found that they were extremely unlikely to be fooled by a three dimensional optical illusion, the hollow face illusion, unlike neurotypical volunteers.[34] Based on fMRI data, researchers concluded that this resulted from a disconnect between their systems for bottom-up processing of visual cues and top-down interpretations of those cues in the parietal cortex.[34] In another study on the motion-induced blindness (MIB) illusion (pictured right), schizophrenic patients continued to perceive stationary visual targets even when observing distracting motion stimuli, unlike neurotypical controls, who experienced motion induced blindness.[35] The schizophrenic test subjects demonstrated impaired cognitive organization, meaning they were less able to coordinate their processing of motion cues and stationary image cues.[35]

List of illusions
There are a variety of different types of optical illusions. Many are included in the following list.

Main article: List of optical illusions
In art
Artists who have worked with optical illusions include M. C. Escher, Bridget Riley, Salvador DalÃ­, Giuseppe Arcimboldo, Patrick Bokanowski, Marcel Duchamp, Jasper Johns, Oscar ReutersvÃ¤rd, Victor Vasarely and Charles Allan Gilbert. Contemporary artists who have experimented with illusions include Jonty Hurwitz, Sandro del Prete, Octavio Ocampo, Dick Termes, Shigeo Fukuda, Patrick Hughes, IstvÃ¡n Orosz, Rob Gonsalves, Gianni A. Sarcone, Ben Heine and Akiyoshi Kitaoka. Optical illusion is also used in film by the technique of forced perspective.

Op art is a style of art that uses optical illusions to create an impression of movement, or hidden images and patterns. Trompe-l'Åil uses realistic imagery to create the optical illusion that depicted objects exist in three dimensions.

Cognitive processes hypothesis

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Optical illusion" â news Â· newspapers Â· books Â· scholar Â· JSTOR (March 2019) (Learn how and when to remove this template message)
The hypothesis claims that visual illusions occur because the neural circuitry in our visual system evolves, by neural learning, to a system that makes very efficient interpretations of usual 3D scenes based in the emergence of simplified models in our brain that speed up the interpretation process but give rise to optical illusions in unusual situations. In this sense, the cognitive processes hypothesis can be considered a framework for an understanding of optical illusions as the signature of the empirical statistical way vision has evolved to solve the inverse problem.[36]

Research indicates that 3D vision capabilities emerge and are learned jointly with the planning of movements.[37] That is, as depth cues are better perceived, individuals can develop more efficient patterns of movement and interaction within the 3D environment around them.[37] After a long process of learning, an internal representation of the world emerges that is well-adjusted to the perceived data coming from closer objects. The representation of distant objects near the horizon is less "adequate".[further explanation needed] In fact, it is not only the Moon that seems larger when we perceive it near the horizon. In a photo of a distant scene, all distant objects are perceived as smaller than when we observe them directly using our vision.

Gallery
Some images need to be viewed in full resolution to see their effect.
File:Illusion movie.ogv
Motion aftereffect: this video produces a distortion illusion when the viewer looks away after watching it.



Ebbinghaus illusion: the orange circle on the left appears smaller than that on the right, but they are in fact the same size.



CafÃ© wall illusion: the parallel horizontal lines in this image appear sloped.



Checker version: the diagonal checker squares at the larger grid points make the grid appear distorted.



Checker version with horizontal and vertical central symmetry



Lilac chaser: if the viewer focuses on the black cross in the center, the location of the disappearing dot appears green.



Motion illusion: contrasting colors create the illusion of motion.



Watercolor illusion: this shape's yellow and blue border create the illusion of the object being pale yellow rather than white[38]



Subjective cyan filter, left: subjectively constructed cyan square filter above blue circles, right: small cyan circles inhibit filter construction[39][40]



Pinna's illusory intertwining effect[41] and Pinna illusion (scholarpedia).[42](The picture shows squares spiralling in, although they are arranged in concentric circles.)



Optical illusion disc which is spun displaying the illusion of motion of a man bowing and a woman curtsying to each other in a circle at the outer edge of the disc, 1833



A hybrid image constructed from low-frequency components of a photograph of Marilyn Monroe (left inset) and high-frequency components of a photograph of Albert Einstein (right inset). The Einstein image is clearer in the full image.



An ancient Roman geometric mosaic. The cubic texture induces a Necker-cube-like optical illusion.


File:Optical illusion created by spinning disks.webm
A set of colorful spinning disks that create illusion. The disks appear to move backwards and forwards in different regions.



Pinna-Brelstaff illusion: the two circles seem to move when the viewer's head is moving forwards and backwards while looking at the black dot.[43]



The Spinning Dancer appears to move both clockwise and counter-clockwise



Forced perspective: the man is made to appear to be supporting the Leaning Tower of Pisa in the background.



Scintillating grid illusion: Dark dots seem to appear and disappear rapidly at random intersections, hence the label "scintillating".

See also
Auditory illusion
Barberpole illusion (Barber's pole)
Camouflage
Chronostasis (stopped-clock illusion)
Closed-eye hallucination/visualization
Contour rivalry
Emmert's law
Flashed face distortion effect
Fraser spiral illusion
Gravity hill
Human reactions to infrasound
Hidden faces
Infinity edge pool
Kinetic depth effect
Mirage
Multistable perception
Rabbitâduck illusion
Silencing
The dress
Troxler's fading
Visual space
Watercolour illusion
Notes
 Gregory, Richard (1991). "Putting illusions in their place". Perception. 20 (1): 1â4. doi:10.1068/p200001. PMID 1945728. S2CID 5521054.
 In the scientific literature the term "visual illusion" is preferred because the older term gives rise to the assumption that the optics of the eye were the general cause for illusions (which is only the case for so-called physical illusions). "Optical" in the term derives from the Greek optein = "seeing", so the term refers to an "illusion of seeing", not to optics as a branch of modern physics. A regular scientific source for illusions are the journals Perception and i-Perception
 Bach, Michael; Poloschek, C. M. (2006). "Optical Illusions" (PDF). Adv. Clin. Neurosci. Rehabil. 6 (2): 20â21.
 Gregory, Richard L. (1997). "Visual illusions classified" (PDF). Trends in Cognitive Sciences. 1 (5): 190â194. doi:10.1016/s1364-6613(97)01060-7. PMID 21223901. S2CID 42228451.
 DeCastro, Thiago Gomes; Gomes, William Barbosa (2017-05-25). "Rubber Hand Illusion: Evidence for a multisensory integration of proprioception". Avances en PsicologÃ­a Latinoamericana. 35 (2): 219. doi:10.12804/revistas.urosario.edu.co/apl/a.3430. ISSN 2145-4515.
 King, Daniel J.; Hodgekins, Joanne; Chouinard, Philippe A.; Chouinard, Virginie-Anne; Sperandio, Irene (2017-06-01). "A review of abnormalities in the perception of visual illusions in schizophrenia". Psychonomic Bulletin & Review. 24 (3): 734â751. doi:10.3758/s13423-016-1168-5. ISSN 1531-5320.
 Wade, Nicholas J. (1998). A natural history of vision. Cambridge, MA: MIT Press.
 "After Images". worqx.com. Archived from the original on 2015-04-22.
 Pinel, J. (2005) Biopsychology (6th ed.). Boston: Allyn & Bacon. ISBN 0-205-42651-4
 Lingelbach B, Block B, Hatzky B, Reisinger E (1985). "The Hermann grid illusion -- retinal or cortical?". Perception. 14 (1): A7.
 Geier J, BernÃ¡th L (2004). "Stopping the Hermann grid illusion by simple sine distortion". Perception. Malden Ma: Blackwell. pp. 33â53. ISBN 978-0631224211.
 Schiller, Peter H.; Carvey, Christina E. (2005). "The Hermann grid illusion revisited". Perception. 34 (11): 1375â1397. doi:10.1068/p5447. PMID 16355743. S2CID 15740144. Archived from the original on 2011-12-12. Retrieved 2011-10-03.
 Geier J, BernÃ¡th L, HudÃ¡k M, SÃ©ra L (2008). "Straightness as the main factor of the Hermann grid illusion". Perception. 37 (5): 651â665. doi:10.1068/p5622. PMID 18605141. S2CID 21028439.
 Bach, Michael (2008). "Die Hermann-Gitter-TÃ¤uschung: LehrbucherklÃ¤rung widerlegt (The Hermann grid illusion: the classic textbook interpretation is obsolete)". Ophthalmologe. 106 (10): 913â917. doi:10.1007/s00347-008-1845-5. PMID 18830602.
 Howe, Catherine Q.; Yang, Zhiyong; Purves, Dale (2005). "The Poggendorff illusion explained by natural scene geometry". PNAS. 102 (21): 7707â7712. Bibcode:2005PNAS..102.7707H. doi:10.1073/pnas.0502893102. PMC 1093311. PMID 15888555.
 David Eagleman (April 2012). Incogito: The Secret Lives of the Brain. Vintage Books. pp. 33â. ISBN 978-0-307-38992-3. Archived from the original on 12 October 2013. Retrieved 14 August 2013.
 Gili Malinsky (22 July 2019). "An optical illusion that seems to be both a circle and a square is baffling the internet â here's how it works". Insider.
 Myers, D. (2003). Psychology in Modules, (7th ed.) New York: Worth. ISBN 0-7167-5850-4
 Yoon Mo Jung and Jackie (Jianhong) Shen (2008), J. Visual Comm. Image Representation, 19(1):42-55, First-order modeling and stability analysis of illusory contours.
 Yoon Mo Jung and Jackie (Jianhong) Shen (2014), arXiv:1406.1265, Illusory shapes via phase transition Archived 2017-11-24 at the Wayback Machine.
 Bressan, P (2006). "The Place of White in a World of Grays: A Double-Anchoring Theory of Lightness Perception". Psychological Review. 113 (3): 526â553. doi:10.1037/0033-295x.113.3.526. PMID 16802880.
 Bernal, B., Guillen, M., & Marquez, J. (2014). The spinning dancer illusion and spontaneous brain fluctuations: An fMRI study. Neurocase (Psychology Press), 20(6), 627-639.
 Tanca, M.; Grossberg, S.; Pinna, B. (2010). "Probing Perceptual Antinomies with the Watercolor Illusion and Explaining How the Brain Resolves Them" (PDF). Seeing & Perceiving. 23 (4): 295â333. CiteSeerX 10.1.1.174.7709. doi:10.1163/187847510x532685. PMID 21466146. Archived (PDF) from the original on 2017-09-21.
 Bach, Michael (4 January 2010) [16 August 2004]. "Shepard's "Turning the Tables"". michaelbach.de. Michael Bach. Archived from the original on 27 December 2009. Retrieved 27 January 2010.
 Bryner, Jeanna. "Scientist: Humans Can See Into Future". foxnews.com. Retrieved 13 July 2018.
 Key to All-Optical Illusions Discovered Archived 2008-09-05 at the Wayback Machine, Jeanna Bryner, Senior Writer, LiveScience.com 6/2/08. His research on this topic is detailed in the May/June 2008 issue of the journal Cognitive Science.
 NIERENBERG, CARI (2008-02-07). "Optical Illusions: When Your Brain Can't Believe Your Eyes". ABC News. Retrieved 13 July 2018.
 Barile, Margherita. "Hering Illusion". mathworld. Wolfram. Retrieved 13 July 2018.
 Pelak, Victoria. "Approach to the patient with visual hallucinations". www.uptodate.com. Archived from the original on 2014-08-26. Retrieved 2014-08-25.
 Gersztenkorn, D; Lee, AG (Jul 2, 2014). "Palinopsia revamped: A systematic review of the literature". Survey of Ophthalmology. 60 (1): 1â35. doi:10.1016/j.survophthal.2014.06.003. PMID 25113609.
 DeCastro, Thiago Gomes; Gomes, William Barbosa (2017-05-25). "Rubber Hand Illusion: Evidence for a multisensory integration of proprioception". Avances en PsicologÃ­a Latinoamericana. 35 (2): 219. doi:10.12804/revistas.urosario.edu.co/apl/a.3430. ISSN 2145-4515.
 Christ, Oliver; Reiner, Miriam (2014-07-01). "Perspectives and possible applications of the rubber hand and virtual hand illusion in non-invasive rehabilitation: Technological improvements and their consequences". Neuroscience & Biobehavioral Reviews. Applied Neuroscience: Models, methods, theories, reviews. A Society of Applied Neuroscience (SAN) special issue. 44: 33â44. doi:10.1016/j.neubiorev.2014.02.013. ISSN 0149-7634
 King, Daniel J.; Hodgekins, Joanne; Chouinard, Philippe A.; Chouinard, Virginie-Anne; Sperandio, Irene (2017-06-01). "A review of abnormalities in the perception of visual illusions in schizophrenia". Psychonomic Bulletin & Review. 24 (3): 734â751. doi:10.3758/s13423-016-1168-5. ISSN 1531-5320.
 Dima, Danai; Roiser, Jonathan P.; Dietrich, Detlef E.; Bonnemann, Catharina; Lanfermann, Heinrich; Emrich, Hinderk M.; Dillo, Wolfgang (2009-07-15). "Understanding why patients with schizophrenia do not perceive the hollow-mask illusion using dynamic causal modelling". NeuroImage. 46 (4): 1180â1186. doi:10.1016/j.neuroimage.2009.03.033. ISSN 1053-8119. PMID 19327402. S2CID 10008080.
 Tschacher, Wolfgang; Schuler, Daniela; Junghan, Ulrich (2006-01-31). "Reduced perception of the motion-induced blindness illusion in schizophrenia". Schizophrenia Research. 81 (2): 261â267. doi:10.1016/j.schres.2005.08.012. ISSN 0920-9964. PMID 16243490. S2CID 10752733.
 Gregory, Richard L. (1997). "Knowledge in perception and illusion" (PDF). Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences. 352 (1358): 1121â7. doi:10.1098/rstb.1997.0095. PMC 1692018. PMID 9304679. Archived (PDF) from the original on 2005-04-04.
 Sweet, Barbara; Kaiser, Mary (August 2011). "Depth Perception, Cueing, and Control" (PDF). AIAA Modeling and Simulation Technologies Conference. NASA Ames Research Center. doi:10.2514/6.2011-6424. ISBN 978-1-62410-154-0 â via American Institute of Aeronautics and Astronautics.
 Bangio Pinna; Gavin Brelstaff; Lothar Spillman (2001). "Surface color from boundaries: a new watercolor illusion". Vision Research. 41 (20): 2669â2676. doi:10.1016/s0042-6989(01)00105-5. PMID 11520512. S2CID 16534759.
 Hoffmann, Donald D. (1998). Visual Intelligence. How we create what we see. Norton., p.174
 Stephen Grossberg; Baingio Pinna (2012). "Neural Dynamics of Gestalt Principles of Perceptual Organization: From Grouping to Shape and Meaning" (PDF). Gestalt Theory. 34 (3+4): 399â482. Archived from the original (PDF) on 2013-10-04. Retrieved 2013-07-14.
 Pinna, B., Gregory, R.L. (2002). "Shifts of Edges and Deformations of Patterns". Perception. 31 (12): 1503â1508. doi:10.1068/p3112pp. PMID 12916675. S2CID 220053062.
 Pinna, Baingio (2009). "Pinna illusion". Scholarpedia. 4 (2): 6656. Bibcode:2009SchpJ...4.6656P. doi:10.4249/scholarpedia.6656.
 Baingio Pinna; Gavin J. Brelstaff (2000). "A new visual illusion of relative motion" (PDF). Vision Research. 40 (16): 2091â2096. doi:10.1016/S0042-6989(00)00072-9. PMID 10878270. S2CID 11034983. Archived (PDF) from the original on 2013-10-05.
References
Bach, Michael; Poloschek, C. M. (2006). "Optical Illusions" (PDF). Adv. Clin. Neurosci. Rehabil. 6 (2): 20â21.
Changizi, Mark A.; Hsieh, Andrew; Nijhawan, Romi; Kanai, Ryota; Shimojo, Shinsuke (2008). "Perceiving the Present and a Systematization of Illusions" (PDF). Cognitive Science. 32 (3): 459â503. doi:10.1080/03640210802035191. PMID 21635343.
Eagleman, D. M. (2001). "Visual Illusions and Neurobiology" (PDF). Nature Reviews Neuroscience. 2 (12): 920â6. doi:10.1038/35104092. PMID 11733799. S2CID 205023280.
Gregory, Richard (1991). "Putting illusions in their place". Perception. 20 (1): 1â4. doi:10.1068/p200001. PMID 1945728. S2CID 5521054.
Gregory, Richard (1997). "Knowledge in perception and illusion" (PDF). Phil. Trans. R. Soc. Lond. B. 352 (1358): 1121â1128. doi:10.1098/rstb.1997.0095. PMC 1692018. PMID 9304679.
Purves, D.; Lotto, R.B.; Nundy, S. (2002). "Why We See What We Do". American Scientist. 90 (3): 236â242. doi:10.1511/2002.9.784.
Purves, D.; Williams, M. S.; Nundy, S.; Lotto, R. B. (2004). "Perceiving the intensity of light". Psychological Review. 111 (1): 142â158. CiteSeerX 10.1.1.1008.6441. doi:10.1037/0033-295x.111.1.142. PMID 14756591.
Renier, L.; Laloyaux, C.; Collignon, O.; Tranduy, D.; Vanlierde, A.; Bruyer, R.; De Volder, A. G. (2005). "The Ponzo illusion using auditory substitution of vision in sighted and early blind subjects". Perception. 34 (7): 857â867. doi:10.1068/p5219. PMID 16124271. S2CID 17265107.
Renier, L.; Bruyer, R.; De Volder, A. G. (2006). "Vertical-horizontal illusion present for sighted but not early blind humans using auditory substitution of vision". Perception & Psychophysics. 68 (4): 535â542. doi:10.3758/bf03208756. PMID 16933419.
Yang, Z.; Purves, D. (2003). "A statistical explanation of visual space". Nature Neuroscience. 6 (6): 632â640. doi:10.1038/nn1059. PMID 12754512. S2CID 610068.
Dixon, E.; Shapiro, A.; Lu, Z. (2014). "Scale-Invariance in brightness illusions implicates object-level visual processing". Scientific Reports. 4: 3900. Bibcode:2014NatSR...4E3900D. doi:10.1038/srep03900. PMC 3905277. PMID 24473496.
Further reading
Purves, Dale; et al. (2008). "Visual illusions:An Empirical Explanation". Scholarpedia. 3 (6): 3706. Bibcode:2008SchpJ...3.3706P. doi:10.4249/scholarpedia.3706.
David Cycleback. 2018. Understanding Human Minds and Their Limits. Publisher Bookboon.com ISBN 978-87-403-2286-6
External links
	Wikimedia Commons has media related to Optical illusion.

This article's use of external links may not follow Wikipedia's policies or guidelines. Please improve this article by removing excessive or inappropriate external links, and converting useful links where appropriate into footnote references. (October 2017) (Learn how and when to remove this template message)
Optical illusions and Transformation
Optical illusions and perception paradoxes by Archimedes Lab
Optical Illusions Categorized by Just-Riddles.net
Project LITE Atlas of Visual Phenomena
Autokinetic optical illusions, on Smithsonian Magazine
Akiyoshi's illusion pages Professor Akiyoshi KITAOKA's anomalous motion illusions
Spiral Or Not? by Enrique Zeleny, Wolfram Demonstrations Project
Still images that move by Op Artist Gianni A. Sarcone
Optical illusion Clock
Best Illusion of The Year Contest (scientific contest)
139 Optical Illusions & Visual Phenomena
Perception museums and perceptual illusions (resource page)
vte
Op art
vte
Optical illusions (list)
Authority control Edit this at Wikidata
LCCN: sh85095148NDL: 01105716
Categories: Optical illusionsOptical phenomenaConsciousness studies
Navigation menu
Not logged inTalkContributionsCreate accountLog in
ArticleTalk
ReadEditView historySearch
Search Wikipedia
Main page
Contents
Current events
Random article
About Wikipedia
Contact us
Donate
Contribute
Help
Learn to edit
Community portal
Recent changes
Upload file
Tools
What links here
Related changes
Special pages
Permanent link
Page information
Cite this page
Wikidata item
Print/export
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
EspaÃ±ol
FranÃ§ais
íêµ­ì´
à¤¹à¤¿à¤¨à¥à¤¦à¥
Italiano
Ð ÑÑÑÐºÐ¸Ð¹
Tiáº¿ng Viá»t
ä¸­æ
46 more
Edit links
This page was last edited on 26 October 2020, at 03:02 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. WikipediaÂ® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaMobile viewDevelopersStatisticsCookie statementWikimedia FoundationPowered by MediaWiki